[{"content":"剑指 Offer 52. 两个链表的第一个公共节点 链接：剑指 Offer 52. 两个链表的第一个公共节点\nlabuladong 题解思路\nJZ52 两个链表的第一个公共节点 链接：JZ52 两个链表的第一个公共节点\n题目描述 输入两个链表，找出它们的第一个公共节点。\n如下面的两个链表**：**\n在节点 c1 开始相交。\n示例 1：\n输入：intersectVal = 8, listA = [4,1,8,4,5], listB = [5,0,1,8,4,5], skipA = 2, skipB = 3 输出：Reference of the node with value = 8 输入解释：相交节点的值为 8 （注意，如果两个列表相交则不能为 0）。从各自的表头开始算起，链表 A 为 [4,1,8,4,5]，链表 B 为 [5,0,1,8,4,5]。在 A 中，相交节点前有 2 个节点；在 B 中，相交节点前有 3 个节点。 示例 2：\n输入：intersectVal = 2, listA = [0,9,1,2,4], listB = [3,2,4], skipA = 3, skipB = 1 输出：Reference of the node with value = 2 输入解释：相交节点的值为 2 （注意，如果两个列表相交则不能为 0）。从各自的表头开始算起，链表 A 为 [0,9,1,2,4]，链表 B 为 [3,2,4]。在 A 中，相交节点前有 3 个节点；在 B 中，相交节点前有 1 个节点。 示例 3：\n输入：intersectVal = 0, listA = [2,6,4], listB = [1,5], skipA = 3, skipB = 2 输出：null 输入解释：从各自的表头开始算起，链表 A 为 [2,6,4]，链表 B 为 [1,5]。由于这两个链表不相交，所以 intersectVal 必须为 0，而 skipA 和 skipB 可以是任意值。 解释：这两个链表不相交，因此返回 null。 注意：\n如果两个链表没有交点，返回 null. 在返回结果后，两个链表仍须保持原有的结构。 可假定整个链表结构中没有循环。 程序尽量满足 O(n) 时间复杂度，且仅用 O(1) 内存。 本题与主站 160 题相同：https://leetcode-cn.com/problems/intersection-of-two-linked-lists/ 解题思路 两个链表公共节点，注意是链表节点相等，而不是节点值相等\n两个链表不一样长度比较不了，所以需要将两个链表改变成长度一样在比较。\n不能从前面截取一样长度，需要从后面往前截取，所以就需要知道两个链表各自的长度了，通过 cur 指针遍历可以求出链表长度\n然后让长的链表先遍历到短链表的位置，然后开始比较，如果两个节点相等就直接返回，不相等继续比较，如果链表遍历结束还没找到就返回 null\n正确题解 /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { * val = x; * next = null; * } * } */ class Solution { ListNode getIntersectionNode(ListNode headA, ListNode headB) { ListNode head1 = headA; ListNode head2 = headB; //求出两链表的长度 int len1 = 0; int len2 = 0; while(head1 != null) { len1++; head1 = head1.next; } while(head2 != null) { len2++; head2 = head2.next; } head1 = headA; head2 = headB; if(len1 \u0026lt; len2) { //让链表head1为头结点，并且长度大于链表head2 //链表1长度大 int tmp = len1; len1 = len2; len2 = tmp; //swap(head1,head2) ListNode temp = null; temp = head1; head1 = head2; head2 = temp; } int diff = len1 - len2; //让链表1走到与链表2长度相等的节点位置 while(diff \u0026gt; 0) { head1 = head1.next; diff--; } while(head1 != null) { if(head1 == head2) { return head1; //找到交点 } head1 = head1.next; head2 = head2.next; } return null; } } 其他题解 ","permalink":"https://lidengxm.github.io/posts/algorithm/52%E4%B8%A4%E4%B8%AA%E9%93%BE%E8%A1%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%85%AC%E5%85%B1%E8%8A%82%E7%82%B9/","summary":"剑指 Offer 52. 两个链表的第一个公共节点 链接：剑指 Offer 52. 两个链表的第一个公共节点 labuladong 题解思路 JZ52 两个链表的第一个公共节点 链接：JZ52 两个链表的第一个公共节点 题目描述 输入两个链表，找出它们的第一个公共节点。 如下面的两个链表**：** 在节点 c1 开始相交。 示例 1： 输入：intersectVal = 8, listA","title":"52两个链表的第一个公共节点"},{"content":"剑指 Offer 25. 合并两个排序的链表 剑指 Offer 25. 合并两个排序的链表\nJZ25 合并两个有序的链表 JZ25 合并两个有序的链表\n题目描述 输入两个递增排序的链表，合并这两个链表并使新链表中的节点仍然是递增排序的。\n示例 1：\n输入：1-\u0026gt;2-\u0026gt;4, 1-\u0026gt;3-\u0026gt;4 输出：1-\u0026gt;1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;4 限制：\n0 \u0026lt;= 链表长度 \u0026lt;= 1000 注意：本题与主站 21 题相同：https://leetcode-cn.com/problems/merge-two-sorted-lists/\n解题思路 链表的长度是可变的，所以其实不需要求出两个链表的长度，新的有序链表直接定义就好了\n比较两个链表的头指针，哪个节点的值小就先接到有序链表之后 循环条件是两个链表不都为空 当循环条件结束时，可能是一个链表为空另一个链表不为空，所以要进行判断 正确题解 /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public ListNode mergeTwoLists(ListNode l1, ListNode l2) { if(l1 == null) return l2; if(l2 == null) return l1; ListNode dummy = new ListNode(-1); ListNode p = dummy; ListNode h1 = l1; //两个链表的指针 ListNode h2 = l2; while(h1 != null \u0026amp;\u0026amp; h2 != null) { if(h1.val \u0026gt; h2.val) { //比较两个指针，将较小的节点接到p指针 p.next = h2; h2 = h2.next; } else { p.next = h1; h1 = h1.next; } p = p.next; //p指针不断前进 } if(h1 != null) p.next = h1; if(h2 != null) p.next = h2; return dummy.next; } } 其他题解 ","permalink":"https://lidengxm.github.io/posts/algorithm/25%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8/","summary":"剑指 Offer 25. 合并两个排序的链表 剑指 Offer 25. 合并两个排序的链表 JZ25 合并两个有序的链表 JZ25 合并两个有序的链表 题目描述 输入两个递增排序的链表，合并这两个链表并使新链表中的节点仍然是递增排序的。 示例 1： 输入：1-\u0026gt;2-\u0026gt;4, 1-\u0026gt;3-\u0026gt;4 输出：1-\u0026gt;1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026g","title":"25合并两个排序的链表"},{"content":"剑指 Offer 24. 反转链表 剑指 Offer 24. 反转链表\nJZ24 反转链表 JZ24 反转链表\n题目描述 定义一个函数，输入一个链表的头节点，反转该链表并输出反转后链表的头节点。\n示例:\n输入: 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5-\u0026gt;NULL 输出: 5-\u0026gt;4-\u0026gt;3-\u0026gt;2-\u0026gt;1-\u0026gt;NULL 限制：\n0 \u0026lt;= 节点个数 \u0026lt;= 5000 注意：本题与主站 206 题相同：https://leetcode-cn.com/problems/reverse-linked-list/\n解题思路 迭代（双指针）\n考虑遍历链表，并在访问各节点时修改 next 引用指向\n递归\n考虑使用递归法遍历链表，当越过尾节点后终止递归，在回溯时修改各节点的 next 引用指向。\n终止条件：当 cur 为空，则返回尾节点 pre （即反转链表的头节点）； 递归后继节点，记录返回值（即反转链表的头节点）为 res ； 修改当前节点 cur 引用指向前驱节点 pre ； 返回反转链表的头节点 res ； 正确题解（迭代） //迭代 class Solution { public ListNode reverseList(ListNode head) { if(head == null) return head; ListNode pre = null; ListNode cur = head; while(cur != null) { //暂存后继节点cue.next ListNode tmp = cur.next; //修改next引用指向 cur.next = pre; //pre暂存cur pre = cur; cur = tmp; } return pre; } } 其他题解（递归） /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode() {} * ListNode(int val) { this.val = val; } * ListNode(int val, ListNode next) { this.val = val; this.next = next; } * } */ //递归 class Solution { public ListNode reverseList(ListNode head) { //递归反转链表 return reverse(null,head); } public ListNode reverse(ListNode pre, ListNode cur) { //递归终止条件 if(cur == null) { return pre; } //递归后继节点 ListNode res = reverse(cur,cur.next); //修改next指向 cur.next = pre; return res; } } ","permalink":"https://lidengxm.github.io/posts/algorithm/24%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/","summary":"剑指 Offer 24. 反转链表 剑指 Offer 24. 反转链表 JZ24 反转链表 JZ24 反转链表 题目描述 定义一个函数，输入一个链表的头节点，反转该链表并输出反转后链表的头节点。 示例: 输入: 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5-\u0026gt;NULL 输出: 5-\u0026gt;4-\u0026gt;3-\u0026gt;2-\u0026gt;1-\u0026gt;NULL 限制： 0 \u0026lt;= 节点个数 \u0026lt;= 5000 注意：本题与主站 206 题相同：https://leetcode-cn.com/problems/revers","title":"24反转链表"},{"content":"剑指 Offer 06. 从尾到头打印链表 剑指 Offer 06. 从尾到头打印链表\nJZ6 从尾到头打印链表 JZ6 从尾到头打印链表\n输入一个链表的头节点，从尾到头反过来返回每个节点的值（用数组返回）。\n示例 1：\n输入：head = [1,3,2] 输出：[2,3,1] 限制：\n0 \u0026lt;= 链表长度 \u0026lt;= 10000 解题思路 直接反转不好搞，可以通过借助额外空间，比如栈，将链表元素顺序入栈，然后反方向出栈并返回数组\n或者使用递归\n如果使用递归，需要先求出链表的长度\nListNode node = head; int count = 0; while(node != null) { ++count; node = node.next; } 注意空数组的返回，必须显示表示\nif(head == null) return new int[0]; 正确题解 /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public int[] reversePrint(ListNode head) { if(head == null) { return new int[0]; } //借助栈来保存节点 Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); ListNode node = head; //正序遍历链表存储到栈中 while(node != null) { stack.push(node.val); node = node.next; } int[] print = new int[stack.size()]; int index = 0; while(!stack.isEmpty()) { print[index++] = stack.pop(); } return print; } } 其他题解 不分配额外空间 求出链表长度，创建相同大小的数组，将链表从头打印，遍历赋值给数组尾\n//不分配额外空间 class Solution { public int[] reversePrint(ListNode head) { //先求出链表长度 ListNode node = head; //记录链表长度 int count = 0; while(node != null) { ++count; node = node.next; } int[] print = new int[count]; //反转链表输出到数组 node = head; for(int i = count - 1; i \u0026gt;= 0; i--) { print[i] = node.val; node = node.next; } return print; } } 借助栈（ACM 模式） import java.util.Stack; class ListNode { int val; ListNode next; ListNode(int val) { this.val = val; } } public class ReverseLinkedListUsingStack { public int[] reversePrint(ListNode head) { Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); ListNode node = head; while (node != null) { stack.push(node.val); node = node.next; } int[] result = new int[stack.size()]; int index = 0; while (!stack.isEmpty()) { result[index] = stack.pop(); index++; } return result; } public static void main(String[] args) { ListNode head = new ListNode(1); head.next = new ListNode(3); head.next.next = new ListNode(2); ReverseLinkedListUsingStack solution = new ReverseLinkedListUsingStack(); int[] result = solution.reversePrint(head); for (int val : result) { System.out.print(val + \u0026#34; \u0026#34;); } } } 递归 class ListNode { int val; ListNode next; ListNode(int val) { this.val = val; } } public class ReverseLinkedList { public int[] reversePrint(ListNode head) { int length = getLength(head); int[] result = new int[length]; reversePrintHelper(head, result, length - 1); return result; } private int getLength(ListNode node) { int length = 0; while (node != null) { length++; node = node.next; } return length; } private void reversePrintHelper(ListNode node, int[] result, int index) { if (node == null) { return; } result[index] = node.val; reversePrintHelper(node.next, result, index - 1); } public static void main(String[] args) { ListNode head = new ListNode(1); head.next = new ListNode(3); head.next.next = new ListNode(2); ReverseLinkedList solution = new ReverseLinkedList(); int[] result = solution.reversePrint(head); for (int val : result) { System.out.print(val + \u0026#34; \u0026#34;); } } } ","permalink":"https://lidengxm.github.io/posts/algorithm/06%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8/","summary":"剑指 Offer 06. 从尾到头打印链表 剑指 Offer 06. 从尾到头打印链表 JZ6 从尾到头打印链表 JZ6 从尾到头打印链表 输入一个链表的头节点，从尾到头反过来返回每个节点的值（用数组返回）。 示例 1： 输入：head = [1,3,2] 输出：[2,3,1] 限制： 0 \u0026lt;= 链表长度 \u0026lt;= 10000 解题思路 直接反转不好搞，可以通过借助额外空间，比如栈，将链表","title":"06从尾到头打印链表"},{"content":"IO 分类 字节流可以处理一切文件，而字符流只能处理文本\n认识 IO IO，即 in 和 out，也就是输入和输出，指应用程序和外部设备之间的数据传递，常见的外部设备包括文件、管道、网络连接。\nJava 中是通过流处理 IO 的，那么什么是流？\n流（Stream），是一个抽象的概念，是指一连串的数据（字符或字节），是以先进先出的方式发送信息的通道。\n当程序需要读取数据的时候，就会开启一个通向数据源的流，这个数据源可以是文件，内存，或是网络连接。类似的，当程序需要写入数据的时候，就会开启一个通向目的地的流。这时候你就可以想象数据好像在这其中“流”动一样。\n一般来说关于流的特性有下面几点：\n先进先出：最先写入输出流的数据最先被输入流读取到。 顺序存取：可以一个接一个地往流中写入一串字节，读出时也将按写入顺序读取一串字节，不能随机访问中间的数据。（RandomAccessFile 除外） 只读或只写：每个流只能是输入流或输出流的一种，不能同时具备两个功能，输入流只能进行读操作，对输出流只能进行写操作。在一个数据传输通道中，如果既要写入数据，又要读取数据，则要分别提供两个流 传输方式划分 传输方式有两种，字节和字符，那首先得搞明白字节和字符有什么区别，对吧？\n字节（byte）是计算机中用来表示存储容量的一个计量单位，通常情况下，一个字节有 8 位（bit）。 字符（char）可以是计算机中使用的字母、数字、和符号，比如说 A 1 $ 这些。 通常来说，一个字母或者一个字符占用一个字节，一个汉字占用两个字节。\n具体还要看字符编码，比如说在 UTF-8 编码下，一个英文字母（不分大小写）为一个字节，一个中文汉字为三个字节；在 Unicode 编码中，一个英文字母为一个字节，一个中文汉字为两个字节。\n那字节流和字符流的场景有哪些？\n字节流用来处理二进制文件，比如说图片啊、MP3 啊、视频啊。 字符流用来处理文本文件，文本文件可以看作是一种特殊的二进制文件，只不过经过了编码，便于人们阅读。 换句话说就是，字节流可以处理一切文件，而字符流只能处理文本。\n虽然 IO 类很多，但核心的就是 4 个抽象类：InputStream、OutputStream、Reader、Writer。\nInputStream 类\nint read() ：读取数据 int read(byte[] b[], int off, int len)：从第 off 位置开始读，读取 Len 长度的字节，然后放入数组 b 中 long skip(long n)：跳过指定个数的字节 int available()：返回可读的字节数 void close()：关闭流，释放资源 OutputStream 类\nvoid write(int b)：写入一个字节，虽然参数是一个 int 类型，但只有低 8 位才会写入，高 24 位会舍弃 void write(byte[] b, int off, int len)：将数组 b 中的从 off 位置开始，长度为 len 的字节写入 void flush()：强制刷新，将缓冲区的数据写入 void close()：关闭流 Reader 类\nint read()：读取单个字符 int read(char cbuf[], int off, int len)：从第 off 位置开始读，读取 len 长度的字符，然后放入数组 b 中 long skip(long n)：跳过指定个数的字符 int ready()：是否可以读了 void close()：关闭流，释放资源 Writer 类\nvoid write(int c)：写入一个字符 void write(char[], int off, int len)：将数组 cbuf 中的从 off 位置开始，长度为 Len 的字符写入 void flush()：强制刷新 void close()：关闭流 字节流和字符流的区别：\n字节流一般用来处理图像、视频、音频、PPT、Word 等类型的文件。字符流一般用于处理纯文本类型的文件，如 TXT 文件等，但不能处理图像视频等非文本文件。用一句话说就是：字节流可以处理一切文件，而字符流只能处理纯文本文件。 字节流本身没有缓冲区，缓冲字节流相对于字节流，效率提升非常高。而字符流本身就带有缓冲区，缓冲字符流相对于字符流效率提升就不是那么大了。 操作对象划分 IO，不就是输入输出（Input/Output）嘛：\nInput：将外部的数据读入内存，比如说把文件从硬盘读取到内存，从网络读取数据到内存等等 Output：将内存中的数据写入到外部，比如说把数据从内存写入到文件，把数据从内存输出到网络等等。 所有的程序，在执行的时候，都是在内存上进行的，一旦关机，内存中的数据就没了，那如果想要持久化，就需要把内存中的数据输出到外部，比如说文件。\n文件操作算是 IO 中最典型的操作了，也是最频繁的操作。那其实你可以换个角度来思考，比如说按照 IO 的操作对象来思考，IO 就可以分类为：文件、数组、管道、基本数据类型、缓冲、打印、对象序列化/反序列化，以及转换等。\n文件 文件流就是直接操作文件的流，可以细分为字节流（FileInputStream 和 FileOuputStream）和字符流（FileReader 和 FileWrite）\nFileInputStream 的例子：\n// 声明一个 int 类型的变量 b，用于存储读取到的字节 int b; // 创建一个 FileInputStream 对象，用于读取文件 fis.txt 中的数据 FileInputStream fis1 = new FileInputStream(\u0026#34;fis.txt\u0026#34;); // 循环读取文件中的数据 while ((b = fis1.read()) != -1) { // 将读取到的字节转换为对应的 ASCII 字符，并输出到控制台 System.out.println((char)b); } // 关闭 FileInputStream 对象，释放资源 fis1.close(); FileOuputStream 的例子：\n// 创建一个 FileOutputStream 对象，用于写入数据到文件 fos.txt 中 FileOutputStream fos = new FileOutputStream(\u0026#34;fos.txt\u0026#34;); // 向文件中写入数据，这里写入的是字符串 \u0026#34;沉默王二\u0026#34; 对应的字节数组 fos.write(\u0026#34;沉默王二\u0026#34;.getBytes()); // 关闭 FileOutputStream 对象，释放资源 fos.close(); FileReader 的例子：\n// 声明一个 int 类型的变量 b，用于存储读取到的字符 int b = 0; // 创建一个 FileReader 对象，用于读取文件 read.txt 中的数据 FileReader fileReader = new FileReader(\u0026#34;read.txt\u0026#34;); // 循环读取文件中的数据 while ((b = fileReader.read()) != -1) { // 将读取到的字符强制转换为 char 类型，并输出到控制台 System.out.println((char)b); } // 关闭 FileReader 对象，释放资源 fileReader.close(); FileWriter 的例子：\n// 创建一个 FileWriter 对象，用于写入数据到文件 fw.txt 中 FileWriter fileWriter = new FileWriter(\u0026#34;fw.txt\u0026#34;); // 将字符串 \u0026#34;沉默王二\u0026#34; 转换为字符数组 char[] chars = \u0026#34;沉默王二\u0026#34;.toCharArray(); // 向文件中写入数据，这里写入的是 chars 数组中的所有字符 fileWriter.write(chars, 0, chars.length); // 关闭 FileWriter 对象，释放资源 fileWriter.close(); 文件流还可以用于创建、删除、重命名文件等操作。FileOutputStream 和 FileWriter 构造函数的第二个参数可以指定是否追加数据到文件末尾。\nnew File().createNewFile()：创建文件 new File().delete()：删除文件 new File().renameTo()：重命名文件 数组 通常来说，针对文件的读写操作，使用文件流配合缓冲流就够用了，但为了提升效率，频繁地读写文件并不是太好，那么就出现了数组流，有时候也称为内存流。\nByteArrayInputStream 的例子：\n// 创建一个 ByteArrayInputStream 对象，用于从字节数组中读取数据 InputStream is = new BufferedInputStream( new ByteArrayInputStream( \u0026#34;沉默王二\u0026#34;.getBytes(StandardCharsets.UTF_8))); // 定义一个字节数组用于存储读取到的数据 byte[] flush = new byte[1024]; // 定义一个变量用于存储每次读取到的字节数 int len = 0; // 循环读取字节数组中的数据，并输出到控制台 while (-1 != (len = is.read(flush))) { // 将读取到的字节转换为对应的字符串，并输出到控制台 System.out.println(new String(flush, 0, len));//沉默王二 } // 关闭输入流，释放资源 is.close(); ByteArrayOutputStream 的例子：\n// 创建一个 ByteArrayOutputStream 对象，用于写入数据到内存缓冲区中 ByteArrayOutputStream bos = new ByteArrayOutputStream(); // 定义一个字节数组用于存储要写入内存缓冲区中的数据 byte[] info = \u0026#34;沉默王二\u0026#34;.getBytes(); // 向内存缓冲区中写入数据，这里写入的是 info 数组中的所有字节 bos.write(info, 0, info.length); // 将内存缓冲区中的数据转换为字节数组 byte[] dest = bos.toByteArray(); // 关闭 ByteArrayOutputStream 对象，释放资源 bos.close(); 数组流可以用于在内存中读写数据，比如将数据存储在字节数组中进行压缩、加密、序列化等操作。它的优点是不需要创建临时文件，可以提高程序的效率。但是，数组流也有缺点，它只能存储有限的数据量，如果存储的数据量过大，会导致内存溢出\n管道 Java 中的管道和 Unix/Linux 中的管道不同，在 Unix/Linux 中，不同的进程之间可以通过管道来通信，但 Java 中，通信的双方必须在同一个进程中，也就是在同一个 JVM 中，管道为线程之间的通信提供了通信能力。\n一个线程通过 PipedOutputStream 写入的数据可以被另外一个线程通过相关联的 PipedInputStream 读取出来。\n// 创建一个 PipedOutputStream 对象和一个 PipedInputStream 对象 final PipedOutputStream pipedOutputStream = new PipedOutputStream(); final PipedInputStream pipedInputStream = new PipedInputStream(pipedOutputStream); // 创建一个线程，向 PipedOutputStream 中写入数据 Thread thread1 = new Thread(new Runnable() { @Override public void run() { try { // 将字符串 \u0026#34;沉默王二\u0026#34; 转换为字节数组，并写入到 PipedOutputStream 中 pipedOutputStream.write(\u0026#34;沉默王二\u0026#34;.getBytes(StandardCharsets.UTF_8)); // 关闭 PipedOutputStream，释放资源 pipedOutputStream.close(); } catch (IOException e) { e.printStackTrace(); } } }); // 创建一个线程，从 PipedInputStream 中读取数据并输出到控制台 Thread thread2 = new Thread(new Runnable() { @Override public void run() { try { // 定义一个字节数组用于存储读取到的数据 byte[] flush = new byte[1024]; // 定义一个变量用于存储每次读取到的字节数 int len = 0; // 循环读取字节数组中的数据，并输出到控制台 while (-1 != (len = pipedInputStream.read(flush))) { // 将读取到的字节转换为对应的字符串，并输出到控制台 System.out.println(new String(flush, 0, len)); } // 关闭 PipedInputStream，释放资源 pipedInputStream.close(); } catch (IOException e) { e.printStackTrace(); } } }); // 启动线程1和线程2 thread1.start(); thread2.start(); 使用管道流可以实现不同线程之间的数据传输，可以用于线程间的通信、数据的传递等。但是，管道流也有一些局限性，比如只能在同一个 JVM 中的线程之间使用，不能跨越不同的 JVM 进程。\n基本数据类型 基本数据类型输入输出流是一个字节流，该流不仅可以读写字节和字符，还可以读写基本数据类型。\nDataInputStream 提供了一系列可以读基本数据类型的方法：\n// 创建一个 DataInputStream 对象，用于从文件中读取数据 DataInputStream dis = new DataInputStream(new FileInputStream(\u0026#34;das.txt\u0026#34;)); // 读取一个字节，将其转换为 byte 类型 byte b = dis.readByte(); // 读取两个字节，将其转换为 short 类型 short s = dis.readShort(); // 读取四个字节，将其转换为 int 类型 int i = dis.readInt(); // 读取八个字节，将其转换为 long 类型 long l = dis.readLong(); // 读取四个字节，将其转换为 float 类型 float f = dis.readFloat(); // 读取八个字节，将其转换为 double 类型 double d = dis.readDouble(); // 读取一个字节，将其转换为 boolean 类型 boolean bb = dis.readBoolean(); // 读取两个字节，将其转换为 char 类型 char ch = dis.readChar(); // 关闭 DataInputStream，释放资源 dis.close(); DataOutputStream 提供了一系列可以写基本数据类型的方法：\n// 创建一个 DataOutputStream 对象，用于将数据写入到文件中 DataOutputStream das = new DataOutputStream(new FileOutputStream(\u0026#34;das.txt\u0026#34;)); // 将一个 byte 类型的数据写入到文件中 das.writeByte(10); // 将一个 short 类型的数据写入到文件中 das.writeShort(100); // 将一个 int 类型的数据写入到文件中 das.writeInt(1000); // 将一个 long 类型的数据写入到文件中 das.writeLong(10000L); // 将一个 float 类型的数据写入到文件中 das.writeFloat(12.34F); // 将一个 double 类型的数据写入到文件中 das.writeDouble(12.56); // 将一个 boolean 类型的数据写入到文件中 das.writeBoolean(true); // 将一个 char 类型的数据写入到文件中 das.writeChar(\u0026#39;A\u0026#39;); // 关闭 DataOutputStream，释放资源 das.close(); 除了 DataInputStream 和 DataOuputStream，Java IO 还提供了其他一些读写基本数据类型和字符串的流类，包括 ObjectInputStream 和 ObjectOutputStream（用于读写对象）。\npublic static void main(String[] args) { try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\u0026#34;person.dat\u0026#34;))) { Person p = new Person(\u0026#34;张三\u0026#34;, 20); oos.writeObject(p); } catch (IOException e) { e.printStackTrace(); } try (ObjectInputStream ois = new ObjectInputStream(new FileInputStream(\u0026#34;person.dat\u0026#34;))) { Person p = (Person) ois.readObject(); System.out.println(p); } catch (IOException | ClassNotFoundException e) { e.printStackTrace(); } } 以上代码创建了一个 Person 对象，将其写入文件中，然后从文件中读取该对象，并打印在控制台上\n缓冲 CPU 很快，它比内存快 100 倍，比磁盘快百万倍。那也就意味着，程序和内存交互会很快，和硬盘交互相对就很慢，这样就会导致性能问题。\n为了减少程序和硬盘的交互，提升程序的效率，就引入了缓冲流，也就是类名前缀带有 Buffer 的那些，比如说 BufferedInputStream、BufferedOutputStream、BufferedReader、BufferedWriter。\n缓冲流在内存中设置了一个缓冲区，只有缓冲区存储了足够多的带操作的数据后，才会和内存或者硬盘进行交互。简单来说，就是一次多读/写点，少读/写几次，这样程序的性能就会提高。\n以下是一个使用 BufferedInputStream 读取文件的示例代码：\n// 创建一个 BufferedInputStream 对象，用于从文件中读取数据 BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\u0026#34;data.txt\u0026#34;)); // 创建一个字节数组，作为缓存区 byte[] buffer = new byte[1024]; // 读取文件中的数据，并将其存储到缓存区中 int bytesRead; while ((bytesRead = bis.read(buffer)) != -1) { // 对缓存区中的数据进行处理 // 这里只是简单地将读取到的字节数组转换为字符串并打印出来 System.out.println(new String(buffer, 0, bytesRead)); } // 关闭 BufferedInputStream，释放资源 bis.close(); 上述代码中，首先创建了一个 BufferedInputStream 对象，用于从文件中读取数据。然后创建了一个字节数组作为缓存区，每次读取数据时将数据存储到缓存区中。读取数据的过程是通过 while 循环实现的，每次读取数据后对缓存区中的数据进行处理。最后关闭 BufferedInputStream，释放资源。\n以下是一个使用 BufferedOutputStream 写入文件的示例代码：\n// 创建一个 BufferedOutputStream 对象，用于将数据写入到文件中 BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\u0026#34;data.txt\u0026#34;)); // 创建一个字节数组，作为缓存区 byte[] buffer = new byte[1024]; // 将数据写入到文件中 String data = \u0026#34;沉默王二是个大傻子!\u0026#34;; buffer = data.getBytes(); bos.write(buffer); // 刷新缓存区，将缓存区中的数据写入到文件中 bos.flush(); // 关闭 BufferedOutputStream，释放资源 bos.close(); 上述代码中，首先创建了一个 BufferedOutputStream 对象，用于将数据写入到文件中。然后创建了一个字节数组作为缓存区，将数据写入到缓存区中。写入数据的过程是通过 write() 方法实现的，将字节数组作为参数传递给 write() 方法即可。\n最后，通过 flush() 方法将缓存区中的数据写入到文件中。在写入数据时，由于使用了 BufferedOutputStream，数据会先被写入到缓存区中，只有在缓存区被填满或者调用了 flush() 方法时才会将缓存区中的数据写入到文件中。\n以下是一个使用 BufferedReader 读取文件的示例代码：\n// 创建一个 BufferedReader 对象，用于从文件中读取数据 BufferedReader br = new BufferedReader(new FileReader(\u0026#34;data.txt\u0026#34;)); // 读取文件中的数据，并将其存储到字符串中 String line; while ((line = br.readLine()) != null) { // 对读取到的数据进行处理 // 这里只是简单地将读取到的每一行字符串打印出来 System.out.println(line); } // 关闭 BufferedReader，释放资源 br.close(); 上述代码中，首先创建了一个 BufferedReader 对象，用于从文件中读取数据。然后使用 readLine() 方法读取文件中的数据，每次读取一行数据并将其存储到一个字符串中。读取数据的过程是通过 while 循环实现的。\n以下是一个使用 BufferedWriter 写入文件的示例代码：\n// 创建一个 BufferedWriter 对象，用于将数据写入到文件中 BufferedWriter bw = new BufferedWriter(new FileWriter(\u0026#34;data.txt\u0026#34;)); // 将数据写入到文件中 String data = \u0026#34;沉默王二，真帅气\u0026#34;; bw.write(data); // 刷新缓存区，将缓存区中的数据写入到文件中 bw.flush(); // 关闭 BufferedWriter，释放资源 bw.close(); 上述代码中，首先创建了一个 BufferedWriter 对象，用于将数据写入到文件中。然后使用 write() 方法将数据写入到缓存区中，写入数据的过程和使用 FileWriter 类似。需要注意的是，使用 BufferedWriter 写入数据时，数据会先被写入到缓存区中，只有在缓存区被填满或者调用了 flush() 方法时才会将缓存区中的数据写入到文件中。\n最后，通过 flush() 方法将缓存区中的数据写入到文件中，并通过 close() 方法关闭 BufferedWriter，释放资源。\n使用缓冲流可以提高读写效率，减少了频繁的读写磁盘或网络的次数，从而提高了程序的性能。但是，在使用缓冲流时需要注意缓冲区的大小和清空缓冲区的时机，以避免数据丢失或不完整的问题\n打印 Java 的打印流是一组用于打印输出数据的类，包括 PrintStream 和 PrintWriter 两个类。\n恐怕 Java 程序员一生当中最常用的就是打印流了：System.out 其实返回的就是一个 PrintStream 对象，可以用来打印各式各样的对象。\nSystem.out.println(\u0026#34;沉默王二是真的二！\u0026#34;); PrintStream 最终输出的是字节数据，而 PrintWriter 则是扩展了 Writer 接口，所以它的 print()/println() 方法最终输出的是字符数据。使用上几乎和 PrintStream 一模一样。\nStringWriter buffer = new StringWriter(); try (PrintWriter pw = new PrintWriter(buffer)) { pw.println(\u0026#34;沉默王二\u0026#34;); } System.out.println(buffer.toString()); 对象序列化/反序列化 序列化本质上是将一个 Java 对象转成字节数组，然后可以将其保存到文件中，或者通过网络传输到远程。\n// 创建一个 ByteArrayOutputStream 对象 buffer，用于存储数据 ByteArrayOutputStream buffer = new ByteArrayOutputStream(); // 使用 try-with-resources 语句创建一个 ObjectOutputStream 对象 output，并将其与 buffer 关联 try (ObjectOutputStream output = new ObjectOutputStream(buffer)) { // 使用 writeUTF() 方法将字符串 \u0026#34;沉默王二\u0026#34; 写入到缓冲区中 output.writeUTF(\u0026#34;沉默王二\u0026#34;); } // 使用 toByteArray() 方法将缓冲区中的数据转换成字节数组，并输出到控制台 System.out.println(Arrays.toString(buffer.toByteArray())); 对应的反序列化就是将字节数组转换成 Java 对象的过程：\ntry (ObjectInputStream input = new ObjectInputStream(new FileInputStream( new File(\u0026#34;Person.txt\u0026#34;)))) { String s = input.readUTF(); } 这段代码主要使用了 Java 的 ByteArrayOutputStream 和 ObjectOutputStream 类，将字符串 \u0026ldquo;沉默王二\u0026rdquo; 写入到一个字节数组缓冲区中，并将缓冲区中的数据转换成字节数组输出到控制台。\n具体的执行过程如下：\n创建一个 ByteArrayOutputStream 对象 buffer，用于存储数据。 使用 try-with-resources 语句创建一个 ObjectOutputStream 对象 output，并将其与 buffer 关联。 使用 writeUTF() 方法将字符串 \u0026ldquo;沉默王二\u0026rdquo; 写入到缓冲区中。 当 try-with-resources 语句执行完毕时，会自动调用 output 的 close() 方法关闭输出流，释放资源。 使用 toByteArray() 方法将缓冲区中的数据转换成字节数组。 使用 Arrays.toString() 方法将字节数组转换成字符串，并输出到控制台 转换 InputStreamReader 是从字节流到字符流的桥连接，它使用指定的字符集读取字节并将他们解码为字符\n// 创建一个 InputStreamReader 对象 isr，使用 FileInputStream 对象读取文件 demo.txt 的内容并将其转换为字符流 InputStreamReader isr = new InputStreamReader(new FileInputStream(\u0026#34;demo.txt\u0026#34;)); // 创建一个字符数组 cha，用于存储读取的字符数据，其中 1024 表示数组的长度 char[] cha = new char[1024]; // 使用 read() 方法读取 isr 中的数据，并将读取的字符数据存储到 cha 数组中，返回值 len 表示读取的字符数 int len = isr.read(cha); // 将 cha 数组中从下标 0 开始、长度为 len 的部分转换成字符串，并输出到控制台 System.out.println(new String(cha, 0, len)); // 关闭 InputStreamReader 对象 isr，释放资源 isr.close(); 这段代码主要使用了 Java 的 InputStreamReader 和 FileInputStream 类，从文件 demo.txt 中读取数据并将其转换为字符流，然后将读取的字符数据存储到一个字符数组中，并输出转换成字符串后的结果到控制台。\nOutputStreamWriter 将一个字符流的输出对象变为字节流的输出对象，是字符流通向字节流的桥梁\n// 创建一个 File 对象 f，表示文件 test.txt File f = new File(\u0026#34;test.txt\u0026#34;); // 创建一个 OutputStreamWriter 对象 out，使用 FileOutputStream 对象将数据写入到文件 f 中，并将字节流转换成字符流 Writer out = new OutputStreamWriter(new FileOutputStream(f)); // 使用 write() 方法将字符串 \u0026#34;沉默王二!!\u0026#34; 写入到文件 f 中 out.write(\u0026#34;沉默王二!!\u0026#34;); // 关闭 Writer 对象 out，释放资源 out.close(); 使用转换流可以方便地在字节流和字符流之间进行转换。在进行文本文件读写时，通常使用字符流进行操作，而在进行网络传输或与设备进行通信时，通常使用字节流进行操作。\n另外，在使用转换流时需要注意字符编码的问题。如果不指定字符编码，则使用默认的字符编码，可能会出现乱码问题。因此，建议在使用转换流时，始终指定正确的字符编码，以避免出现乱码问题\n文件流-IO 流的起始和终点 在 IO 操作中，文件的操作相对来说是比较复杂的，但也是使用频率最高的部分，我们几乎所有的项目中几乎都躺着一个叫做 FileUtil 或者 FileUtils 的工具类。\njava.io.File 类是专门对文件进行操作的类，注意只能对文件本身进行操作，不能对文件内容进行操作，想要操作内容，必须借助输入输出流。 File 类是文件和目录的抽象表示，主要用于文件和目录的创建、查找和删除等操作。 怎么理解上面两句话？\n第一句是说 File 跟流无关，File 类不能对文件进行读和写，也就是输入和输出！\n第二句是说 File 可以表示D:\\\\文件目录1与D:\\\\文件目录1\\\\文件.txt，前者是文件夹（Directory，或者叫目录）后者是文件(file)，File 类就是用来操作它俩的\nFile 类的构造方法 比较常用的构造方法有三个：\nFile(String pathname)：通过给定的路径来创建新的 File 实例 File(String parent,String child)：从父路径（字符串）和子路径创建新的 File 实例 File(File parent, String child) ：从父路径（File）和子路径名字符串创建新的 File 实例 举例：\n// 文件路径名 String path = \u0026#34;/Users/username/123.txt\u0026#34;; File file1 = new File(path); // 文件路径名 String path2 = \u0026#34;/Users/username/1/2.txt\u0026#34;; File file2 = new File(path2); -------------相当于/Users/username/1/2.txt // 通过父路径和子路径字符串 String parent = \u0026#34;/Users/username/aaa\u0026#34;; String child = \u0026#34;bbb.txt\u0026#34;; File file3 = new File(parent, child); --------相当于/Users/username/aaa/bbb.txt // 通过父级File对象和子路径字符串 File parentDir = new File(\u0026#34;/Users/username/aaa\u0026#34;); String child = \u0026#34;bbb.txt\u0026#34;; File file4 = new File(parentDir, child); --------相当于/Users/username/aaa/bbb.txt 注意：macOS 路径使用正斜杠（/）作为路径分隔符，而 Windows 路径使用反斜杠（\\）作为路径分隔符。所以在遇到路径分隔符的时候，不要直接去写/或者\\。\nJava 中提供了一个跨平台的方法来获取路径分隔符，即使用 File.separator，这个属性会根据操作系统自动返回正确的路径分隔符。\nFile 类构造函数的注意点：\n一个 File 对象代表硬盘中实际存在的一个文件或者目录。 File 类的构造方法不会检验这个文件或目录是否真实存在，因此无论该路径下是否存在文件或者目录，都不影响 File 对象的创建 File 类常用方法 常用方法分为：\n获取功能 获取绝对路径和相对路径 判断功能 创建删除功能 获取功能的方法\ngetAbsolutePath() ：返回此 File 的绝对路径。 getPath() ：结果和 getAbsolutePath 一致。 getName() ：返回文件名或目录名。 length() ：返回文件长度，以字节为单位。 测试：\nFile f = new File(\u0026#34;D:\\\\Coding workspace\\\\utils\\\\ILock.java\u0026#34;); System.out.println(\u0026#34;文件绝对路径:\u0026#34;+f.getAbsolutePath()); System.out.println(\u0026#34;文件构造路径:\u0026#34;+f.getPath()); System.out.println(\u0026#34;文件名称:\u0026#34;+f.getName()); System.out.println(\u0026#34;文件长度:\u0026#34;+f.length()+\u0026#34;字节\u0026#34;); File f2 = new File(\u0026#34;D:\\\\Coding workspace\\\\utils\u0026#34;); System.out.println(\u0026#34;目录绝对路径:\u0026#34;+f2.getAbsolutePath()); System.out.println(\u0026#34;目录构造路径:\u0026#34;+f2.getPath()); System.out.println(\u0026#34;目录名称:\u0026#34;+f2.getName()); System.out.println(\u0026#34;目录长度:\u0026#34;+f2.length()); 输出结果：\n文件绝对路径:D:\\Coding workspace\\utils\\ILock.java 文件构造路径:D:\\Coding workspace\\utils\\ILock.java 文件名称:ILock.java 文件长度:265字节 目录绝对路径:D:\\Coding workspace\\utils 目录构造路径:D:\\Coding workspace\\utils 目录名称:utils 目录长度:4096 绝对路径和相对路径\n绝对路径是从文件系统的根目录开始的完整路径，它描述了一个文件或目录在文件系统中的确切位置。在 Windows 系统中，绝对路径通常以盘符（如 C:）开始，例如 \u0026ldquo;C:\\Program Files\\Java\\jdk1.8.0_291\\bin\\java.exe\u0026quot;。\n相对路径是相对于当前工作目录的路径，它描述了一个文件或目录与当前工作目录之间的位置关系。在 Java 中，相对路径通常是相对于当前 Java 程序所在的目录，例如 \u0026ldquo;config/config.properties\u0026quot;。如果当前工作目录是 \u0026ldquo;/Users/username/project\u0026quot;，那么相对路径 \u0026ldquo;config/config.properties\u0026rdquo; 就表示 \u0026ldquo;/Users/username/project/config/config.properties\u0026quot;。\n举例：\n// 绝对路径示例 File absoluteFile = new File(\u0026#34;D:\\\\Coding workspace\\\\utils\\\\ILock.java\u0026#34;); System.out.println(\u0026#34;绝对路径：\u0026#34; + absoluteFile.getAbsolutePath()); // 相对路径示例 File relativeFile = new File(\u0026#34;utils\\\\ILock.java\u0026#34;); System.out.println(\u0026#34;相对路径：\u0026#34; + relativeFile.getPath()); 判断功能的方法\nexists()：判断文件或目录是否存在 isDirectory()：判断是否为目录 isFile()：判断是否为文件 演示：\nFile file = new File(\u0026#34;D:\\\\Coding workspace\\\\utils\\\\ILock.java\u0026#34;); // 判断文件或目录是否存在 if (file.exists()) { System.out.println(\u0026#34;文件或目录存在\u0026#34;); } else { System.out.println(\u0026#34;文件或目录不存在\u0026#34;); } // 判断是否是目录 if (file.isDirectory()) { System.out.println(\u0026#34;是目录\u0026#34;); } else { System.out.println(\u0026#34;不是目录\u0026#34;); } // 判断是否是文件 if (file.isFile()) { System.out.println(\u0026#34;是文件\u0026#34;); } else { System.out.println(\u0026#34;不是文件\u0026#34;); } 输出：目录存在、不是目录、是文件\n创建、删除功能的方法\ncreateNewFile() ：文件不存在，创建一个新的空文件并返回true，文件存在，不创建文件并返回false。 delete() ：删除文件或目录。如果是目录，只有目录为空才能删除。 mkdir() ：只能创建一级目录，如果父目录不存在，则创建失败。返回 true 表示创建成功，返回 false 表示创建失败。 mkdirs() ：可以创建多级目录，如果父目录不存在，则会一并创建。返回 true 表示创建成功，返回 false 表示创建失败或目录已经存在。 开发中一般使用mkdirs()\n示例：\n// 创建文件 File file = new File(\u0026#34;D:\\\\1.txt\u0026#34;); if (file.createNewFile()) { System.out.println(\u0026#34;创建文件成功：\u0026#34; + file.getAbsolutePath()); } else { System.out.println(\u0026#34;创建文件失败：\u0026#34; + file.getAbsolutePath()); } // 删除文件 if (file.delete()) { System.out.println(\u0026#34;删除文件成功：\u0026#34; + file.getAbsolutePath()); } else { System.out.println(\u0026#34;删除文件失败：\u0026#34; + file.getAbsolutePath()); } // 创建多级目录 File directory = new File(\u0026#34;D:\\\\111\\\\222\u0026#34;); if (directory.mkdirs()) { System.out.println(\u0026#34;创建目录成功：\u0026#34; + directory.getAbsolutePath()); } else { System.out.println(\u0026#34;创建目录失败：\u0026#34; + directory.getAbsolutePath()); } 目录的遍历\nString[] list() ：返回一个 String 数组，表示该 File 目录中的所有子文件或目录。 File[] listFiles() ：返回一个 File 数组，表示该 File 目录中的所有的子文件或目录。 举例：\nFile directory = new File(\u0026#34;D:\\\\blog\u0026#34;); // 列出目录下的文件名 String[] files = directory.list(); System.out.println(\u0026#34;目录下的文件名：\u0026#34;); for (String file : files) { System.out.println(file); } // 列出目录下的文件和子目录 File[] filesAndDirs = directory.listFiles(); System.out.println(\u0026#34;目录下的文件和子目录：\u0026#34;); for (File fileOrDir : filesAndDirs) { if (fileOrDir.isFile()) { System.out.println(\u0026#34;文件：\u0026#34; + fileOrDir.getName()); } else if (fileOrDir.isDirectory()) { System.out.println(\u0026#34;目录：\u0026#34; + fileOrDir.getName()); } } 注意：listFiles 再获取指定目录下的文件或者子目录时必须满足下面两个条件：\n指定的目录必须存在 指定的必须是目录，否则会引发 NullPointerException 异常 Apache FileUtils 类 FileUtils 类是 Apache Commons IO 库中的一个类，提供了一些更为方便的方法来操作文件或目录\n复制文件目录\nFile srcFile = new File(\u0026#34;path/to/src/file\u0026#34;); File destFile = new File(\u0026#34;path/to/dest/file\u0026#34;); // 复制文件 FileUtils.copyFile(srcFile, destFile); // 复制目录 FileUtils.copyDirectory(srcFile, destFile); 删除文件或目录\nFile file = new File(\u0026#34;path/to/file\u0026#34;); // 删除文件或目录 FileUtils.delete(file); 需要注意的是，如果要删除一个非空目录，需要先删除目录中的所有文件和子目录\n移动文件或目录\nFile srcFile = new File(\u0026#34;path/to/src/file\u0026#34;); File destFile = new File(\u0026#34;path/to/dest/file\u0026#34;); // 移动文件或目录 FileUtils.moveFile(srcFile, destFile); 查询文件或目录的信息\nFile file = new File(\u0026#34;path/to/file\u0026#34;); // 获取文件或目录的修改时间 Date modifyTime = FileUtils.lastModified(file); // 获取文件或目录的大小 long size = FileUtils.sizeOf(file); // 获取文件或目录的扩展名 String extension = FileUtils.getExtension(file.getName()); Hutool FileUtil 类 FileUtil 类是 Hutool 工具包中的文件操作工具类，提供了一系列简单易用的文件操作方法，可以帮助 Java 开发者快速完成文件相关的操作任务。\nFileUtil 类包含以下几类操作工具：\n文件操作：包括文件目录的新建、删除、复制、移动、改名等 文件判断：判断文件或目录是否非空，是否为目录，是否为文件等等。 绝对路径：针对 ClassPath 中的文件转换为绝对路径文件。 文件名：主文件名，扩展名的获取 读操作：包括 getReader、readXXX 操作 写操作：包括 getWriter、writeXXX 操作 下面是 FileUtil 类中一些常用的方法：\ncopyFile：复制文件，该方法可以将指定的源文件复制到指定的目标文件中 move：移动文件或目录，该方法可以将指定的源文件或目录移动到指定的目标文件或目录中 del：删除文件或目录，该方法可以删除指定的文件或目录，如果指定的文件或目录不存在，则会抛出异常。 rename：重命名文件或目录，该方法可以将指定的文件或目录重命名为指定的新名称 readLines：从文件中读取每一行数据 字节流-IO 流的基石 一切文件（文本、视频、图片）的数据都是以二进制的形式存储的，传输时也是。所以，字节流可以传输任意类型的文件数据。（而字符流只能传输文本类型的数据）\n字节流分为：\n字节输出流（OutputStream） 字节输入流（InputStream） 字节输出流（OutputStream） java.io.OutputStream 是字节输出流的超类（父类），我们来看一下它定义的一些共性方法：\n1、 close() ：关闭此输出流并释放与此流相关联的系统资源。\n2、 flush() ：刷新此输出流并强制缓冲区的字节被写入到目的地。\n3、 write(byte[] b)：将 b.length 个字节从指定的字节数组写入此输出流。\n4、 write(byte[] b, int off, int len) ：从指定的字节数组写入 len 字节到此输出流，从偏移量 off 开始。 也就是说从 off 个字节数开始一直到 len 个字节结束\nFileOutputStream 类 FileOutputStream 类的 OutputStream 类的一个子类，用于将数据写入文件中\nFileOutputStream 的构造方法 使用文件名创建 FileOutputStream 对象 String fileName = \u0026#34;example.txt\u0026#34;; FileOutputStream fos = new FileOutputStream(fileName); 上面使用使用文件名 \u0026ldquo;example.txt\u0026rdquo; 创建一个 FileOutputStream 对象，将数据写入到该文件中。如果文件不存在，则创建一个新文件；如果文件已经存在，则覆盖原有文件\n使用文件对象创建 FileOutputStream 对象 File file = new File(\u0026#34;example.txt\u0026#34;); FileOutputStream fos = new FileOutputStream(file); FileOutputStream 的使用示例： FileOutputStream fos = null; try { fos = new FileOutputStream(\u0026#34;example.txt\u0026#34;); //写入文件到example.txt文件中 fos.write(\u0026#34;沉默王二\u0026#34;.getBytes()); } catch (IOException e) { e.printStackTrace(); } finally { if (fos != null) { try { fos.close(); } catch (IOException e) { e.printStackTrace(); } } } FileOutputStream 写入字节数据 FileOutputStream 写入字节数据主要通过 write 方法\nwrite(int b) write(byte[] b) write(byte[] b,int off,int len) //从`off`索引开始，`len`个字节 写入字节：write(int b)方法，每次可以写入一个字节 // 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(\u0026#34;fos.txt\u0026#34;); // 写出数据 fos.write(97); // 第1个字节 fos.write(98); // 第2个字节 fos.write(99); // 第3个字节 // 关闭资源 fos.close(); a 的 ASCLL 值为 97，字符 b 的 ASCII 值为 98，字符 b 的 ASCII 值为 99。\n以上代码可以切换数字成对应的 ASCLL 小写字母\n使用 write(int b) 方法写出一个字节时，参数 b 表示要写出的字节的整数值。**由于一个字节只有 8 位，因此参数 b 的取值范围应该在 0 到 255 之间，超出这个范围的值将会被截断。**例如，如果参数 b 的值为 -1，那么它会被截断为 255，如果参数 b 的值为 256，那么它会被截断为 0。\n在将参数 b 写入输出流中时，write(int b) 方法只会将参数 b 的低 8 位写入，而忽略高 24 位。这是因为在 Java 中，整型类型（包括 byte、short、int、long）在内存中以二进制补码形式表示。当将一个整型值传递给 write(int b) 方法时，方法会将该值转换为 byte 类型，只保留二进制补码的低 8 位，而忽略高 24 位。\n例如，如果要写出的整数为 0x12345678，它的二进制补码表示为 0001 0010 0011 0100 0101 0110 0111 1000。当使用 write(int b) 方法写出该整数时，只会将二进制补码的低 8 位 0111 1000 写出，而忽略高 24 位 0001 0010 0011 0100 0101 0110。这就是参数 b 的高 24 位被忽略的原因。\n0111 1000 是一个 8 位的二进制数，它对应的十进制数是 120，对应的 ASCII 码字符是小写字母 \u0026ldquo;x\u0026rdquo;。在 ASCII 码表中，小写字母 \u0026ldquo;x\u0026rdquo; 的十进制 ASCII 码值为 120。因此，如果使用 write(int b) 方法写出一个字节值为 0x78（十进制为 120），那么写出的结果就是小写字母 \u0026ldquo;x\u0026rdquo;\nFileOutputStream 写入字节数组write(byte[] b)\n代码示例：\n// 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(\u0026#34;fos.txt\u0026#34;); // 字符串转换为字节数组 byte[] b = \u0026#34;沉默王二有点帅\u0026#34;.getBytes(); // 写入字节数组数据 fos.write(b); // 关闭资源 fos.close(); FileOutputStream 写入指定长度字节数组write(byte[] b, int off, int len)\n代码示例：\n// 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(\u0026#34;fos.txt\u0026#34;); // 字符串转换为字节数组 byte[] b = \u0026#34;abcde\u0026#34;.getBytes(); // 从索引2开始，2个字节。索引2是c，两个字节，也就是cd。 fos.write(b,2,2); // 关闭资源 fos.close(); FileOutputStream 实现数据追加、换行 上面的代码示例中，每次运行程序都会创建新的输出流对象，于是文件中的数据也会被清空。如果想保留目标文件中的数据，还能继续追加新数据，该怎么办呢？以及如何实现换行呢？\nFileOutputStream 可以解决\nFileOutputStream的另外两个构造方法，如下：\n使用文件名和追加标志创建 FileOutputStream 对象 String fileName = \u0026#34;example.txt\u0026#34;; boolean append = true; FileOutputStream fos = new FileOutputStream(fileName, append); 以上代码使用文件名 \u0026ldquo;example.txt\u0026rdquo; 和追加标志创建一个 FileOutputStream 对象，将数据追加到该文件的末尾。如果文件不存在，则创建一个新文件；如果文件已经存在，则在文件末尾追加数据\n使用文件对象和追加标志创建 FileOutputStream 对象 File file = new File(\u0026#34;example.txt\u0026#34;); boolean append = true; FileOutputStream fos = new FileOutputStream(file, append); 以上代码使用文件对象和追加标志创建一个 FileOutputStream 对象，将数据追加到该文件的末尾。\n这两个构造方法，第二个参数中都需要传入一个 boolean 类型的值，true 表示追加数据，false 表示不追加也就是清空原有数据。\n实现数据追加代码如下：\n// 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(\u0026#34;fos.txt\u0026#34;,true); // 字符串转换为字节数组 byte[] b = \u0026#34;abcde\u0026#34;.getBytes(); // 写出从索引2开始，2个字节。索引2是c，两个字节，也就是cd。 fos.write(b); // 关闭资源 fos.close(); 多次运行代码，会发现数据在不断追加\n在 Java 中，字符串中的回车符可以用 \u0026ldquo;\\r\u0026rdquo; 来表示，换行符可以用 \u0026ldquo;\\n\u0026rdquo; 来表示。\n字节输入流（InputStream） java.io.InputStream 是字节输入流的超类（父类），我们来看一下它的一些共性方法：\n1、close() ：关闭此输入流并释放与此流相关的系统资源。\n2、int read()： 从输入流读取数据的下一个字节。\n3、read(byte[] b)： 该方法返回的 int 值代表的是读取了多少个字节，读到几个返回几个，读取不到返回-1\nFileinputStream 类 InputStream 有很多子类，我们从最简单的一个子类 FileInputStream 开始。看名字就知道是文件输入流，用于将数据从文件中读取数据\nFileInputStream 的构造方法 FileInputStream(String name)：创建一个 FileInputStream 对象，并打开指定名称的文件进行读取。文件名由 name 参数指定。如果文件不存在，将会抛出 FileNotFoundException 异常。 FileInputStream(File file)：创建一个 FileInputStream 对象，并打开指定的 File 对象表示的文件进行读取。 代码示例：\n// 创建一个 FileInputStream 对象 FileInputStream fis = new FileInputStream(\u0026#34;test.txt\u0026#34;); // 读取文件内容 int data; while ((data = fis.read()) != -1) { System.out.print((char) data); } // 关闭输入流 fis.close(); FileInputStream 读取字节数据 读取字节：read()方法会读取一个字节并返回其整数表示。如果已经到达文件的末尾，则返回 -1。如果在读取时发生错误，则会抛出 IOException 异常。 代码示例：\n// 创建一个 FileInputStream 对象 FileInputStream fis = new FileInputStream(\u0026#34;test.txt\u0026#34;); // 读取文件内容 int data; while ((data = fis.read()) != -1) { System.out.print((char) data); } // 关闭输入流 fis.close(); 使用字节数组读取：read(byte[] b) 方法会从输入流中最多读取 b.length 个字节，并将它们存储到缓冲区数组 b 中。 代码示例：\n// 创建一个 FileInputStream 对象 FileInputStream fis = new FileInputStream(\u0026#34;test.txt\u0026#34;); // 读取文件内容到缓冲区 byte[] buffer = new byte[1024]; int count; while ((count = fis.read(buffer)) != -1) { System.out.println(new String(buffer, 0, count)); } // 关闭输入流 fis.close(); 字节流 FileInputstream 复制图片 原理很简单，就是把图片信息读入到字节输入流中，再通过字节输出流写入到文件中。\n代码示例：\n// 创建一个 FileInputStream 对象以读取原始图片文件 FileInputStream fis = new FileInputStream(\u0026#34;original.jpg\u0026#34;); // 创建一个 FileOutputStream 对象以写入复制后的图片文件 FileOutputStream fos = new FileOutputStream(\u0026#34;copy.jpg\u0026#34;); // 创建一个缓冲区数组以存储读取的数据 byte[] buffer = new byte[1024]; int count; // 读取原始图片文件并将数据写入复制后的图片文件 while ((count = fis.read(buffer)) != -1) { fos.write(buffer, 0, count); } // 关闭输入流和输出流 fis.close(); fos.close(); 上面的代码创建了一个 FileInputStream 对象以读取原始图片文件，并创建了一个 FileOutputStream 对象以写入复制后的图片文件。然后，**使用 while 循环逐个读取原始图片文件中的字节，并将其写入复制后的图片文件中。**最后，关闭输入流和输出流释放资源\n小结 InputStream 是字节输入流的抽象类，它定义了读取字节数据的方法，如 read()、read(byte[] b)、read(byte[] b, int off, int len) 等。OutputStream 是字节输出流的抽象类，它定义了写入字节数据的方法，如 write(int b)、write(byte[] b)、write(byte[] b, int off, int len) 等。这两个抽象类是字节流的基础。\nFileInputStream 是从文件中读取字节数据的流，它继承自 InputStream。FileOutputStream 是将字节数据写入文件的流，它继承自 OutputStream。这两个类是字节流最常用的实现类之一。\n字符流-Reader 和 Writer 的故事 字符流主要包括：\n字符输入流（Reader） 字符输出流（Writer） 字符流 Reader 和 Writer 的故事要从他们的类图开始：\n字符流是一种用于读取和写入字符数据的输入输出流。与字节流不同，字符流以字符为单位读取和写入数据，而不是以字节为单位。常用来处理文本信息\n字节流直接读取中文，可能会遇到乱码问题\n//FileInputStream为操作文件的字符输入流 FileInputStream inputStream = new FileInputStream(\u0026#34;a.txt\u0026#34;);//内容为“沉默王二是傻 X” int len; while ((len=inputStream.read())!=-1){ System.out.print((char)len); } 运行结果：\n运行结果： æ²é»çäºæ¯å» X 之所以出现乱码是因为在字节流中，一个字符通常由多个字节组成，而不同的字符编码使用的字节数不同。如果我们使用了错误的字符编码，或者在读取和写入数据时没有正确处理字符编码的转换，就会导致读取出来的中文字符出现乱码。\n例如，当我们使用默认的字符编码（见上例）读取一个包含中文字符的文本文件时，就会出现乱码。因为默认的字符编码通常是 ASCII 编码，它只能表示英文字符，而不能正确地解析中文字符。\n那使用字节流该如何正确地读出中文呢？见下例。\ntry (FileInputStream inputStream = new FileInputStream(\u0026#34;a.txt\u0026#34;)) { byte[] bytes = new byte[1024]; int len; while ((len = inputStream.read(bytes)) != -1) { System.out.print(new String(bytes, 0, len)); } } 我们拿 String 类进行了解码，查看new String(byte bytes[], int offset, int length)的源码就可以发现，该构造方法有解码功能\npublic String(byte bytes[], int offset, int length) { checkBounds(bytes, offset, length); this.value = StringCoding.decode(bytes, offset, length); } 追看 StringCoding.decode() 方法调用的 defaultCharset() 方法，会发现默认编码是UTF-8，代码如下\npublic static Charset defaultCharset() { if (defaultCharset == null) { synchronized (Charset.class) { if (cs != null) defaultCharset = cs; else defaultCharset = forName(\u0026#34;UTF-8\u0026#34;); } } return defaultCharset; } static char[] decode(byte[] ba, int off, int len) { String csn = Charset.defaultCharset().name(); try { // use charset name decode() variant which provides caching. return decode(csn, ba, off, len); } catch (UnsupportedEncodingException x) { warnUnsupportedCharset(csn); } } Java 中，常用的字符编码有 ASCII、ISO-8859-1、UTF-8、UTF-16 等。其中，ASCII 和 ISO-8859-1 只能表示部分字符，而 UTF-8 和 UTF-16 可以表示所有的 Unicode 字符，包括中文字符。\n当我们使用 new String(byte bytes[], int offset, int length) 将字节流转换为字符串时，Java 会根据 UTF-8 的规则将每 3 个字节解码为一个中文字符，从而正确地解码出中文。\n尽管字节流也有办法解决乱码问题，但不够直接，于是就有了字符流，专门用于处理文本文件（音频、图片、视频等为非文本文件）\n从另一角度来说：字符流 = 字节流 + 编码表\n字符输入流（Reader） java.io.Reader是字符输入流的超类（父类），它定义了字符输入流的一些共性方法：\n1、close()：关闭此流并释放与此流相关的系统资源。 2、read()：从输入流读取一个字符。 3、read(char[] cbuf)：从输入流中读取一些字符，并将它们存储到字符数组 cbuf中 FileReader 是 Reader 的子类，用于从文件中读取字符数据。它的主要特点如下：\n可以通过构造方法指定要读取的文件路径。 每次可以读取一个或多个字符。 可以读取 Unicode 字符集中的字符，通过指定字符编码来实现字符集的转换。 FileReader 构造方法 FileReader(File file)：创建一个新的 FileReader，参数为File 对象。 FileReader(String fileName)：创建一个新的 FileReader，参数为文件名 代码示例：\n// 使用File对象创建流对象 File file = new File(\u0026#34;a.txt\u0026#34;); FileReader fr = new FileReader(file); // 使用文件名称创建流对象 FileReader fr = new FileReader(\u0026#34;b.txt\u0026#34;); FileReader 读取字符数据 读取字符：read方法，每次可以读取一个字符，返回读取的字符（转为 int 类型），当读取到文件末尾时，返回-1。 代码示例如下：\n// 使用文件名称创建流对象 FileReader fr = new FileReader(\u0026#34;abc.txt\u0026#34;); // 定义变量，保存数据 int b; // 循环读取 while ((b = fr.read())!=-1) { System.out.println((char)b); } // 关闭资源 fr.close(); 读取指定长度的字符：read(char[] cbuf, int off, int len)，并将其存储到字符数组中。其中，cbuf 表示存储读取结果的字符数组，off 表示存储结果的起始位置，len 表示要读取的字符数。 代码示例如下：\nFile textFile = new File(\u0026#34;docs/约定.md\u0026#34;); // 给一个 FileReader 的示例 // try-with-resources FileReader try(FileReader reader = new FileReader(textFile);) { // read(char[] cbuf) char[] buffer = new char[1024]; int len; while ((len = reader.read(buffer, 0, buffer.length)) != -1) { System.out.print(new String(buffer, 0, len)); } } 上面代码用 FileReader 从文件中读取字符数据，并将其存储到一个大小为 1024 的字符数组中。每次读取 len 个字符，然后使用 String 构造方法将其转换为字符串并输出。\nFileReader 实现了 AutoCloseable 接口，因此可以使用 try-with-resources 语句自动关闭资源，避免了手动关闭资源的繁琐操作。\n字符输出流（Writer） java.io.Writer 是字符输出流类的超类（父类），可以将指定的字符信息写入到目的地，来看它定义的一些共性方法：\n1、write(int c) 写入单个字符。 2、write(char[] cbuf) 写入字符数组。 3、write(char[] cbuf, int off, int len) 写入字符数组的一部分，off 为开始索引，len 为字符个数。 4、write(String str) 写入字符串。 5、write(String str, int off, int len) 写入字符串的某一部分，off 指定要写入的子串在 str 中的起始位置，len 指定要写入的子串的长度。 6、flush() 刷新该流的缓冲。 7、close() 关闭此流，但要先刷新它。 java.io.FileWriter 类是 Writer 的子类，用来将字符写入到文件\nFileWriter 构造方法 FileWriter(File file)： 创建一个新的 FileWriter，参数为要读取的 File 对象。 FileWriter(String fileName)： 创建一个新的 FileWriter，参数为要读取的文件的名称。 代码示例如下：\n// 第一种：使用File对象创建流对象 File file = new File(\u0026#34;a.txt\u0026#34;); FileWriter fw = new FileWriter(file); // 第二种：使用文件名称创建流对象 FileWriter fw = new FileWriter(\u0026#34;b.txt\u0026#34;); FileWriter 写入数据 写入字符：write(int b) 方法，每次可以写出一个字符 代码示例如下：\nFileWriter fw = null; try { fw = new FileWriter(\u0026#34;output.txt\u0026#34;); fw.write(72); // 写入字符\u0026#39;H\u0026#39;的ASCII码 fw.write(101); // 写入字符\u0026#39;e\u0026#39;的ASCII码 fw.write(108); // 写入字符\u0026#39;l\u0026#39;的ASCII码 fw.write(108); // 写入字符\u0026#39;l\u0026#39;的ASCII码 fw.write(111); // 写入字符\u0026#39;o\u0026#39;的ASCII码 } catch (IOException e) { e.printStackTrace(); } finally { try { if (fw != null) { fw.close(); } } catch (IOException e) { e.printStackTrace(); } } 这个示例代码中，首先创建一个 FileWriter 对象 fw，并指定要写入的文件路径 \u0026ldquo;output.txt\u0026rdquo;。然后使用 fw.write() 方法将字节写入文件中，这里分别写入字符\u0026rsquo;H\u0026rsquo;、\u0026rsquo;e\u0026rsquo;、\u0026rsquo;l\u0026rsquo;、\u0026rsquo;l\u0026rsquo;、\u0026lsquo;o\u0026rsquo;的 ASCII 码。最后在 finally 块中关闭 FileWriter 对象，释放资源。\n需要注意的是，使用 write(int b) 方法写入的是一个字节，而不是一个字符。如果需要写入字符，可以使用 write(char cbuf[]) 或 write(String str) 方法\n写入字符数组：write(char[] cbuf) 方法，将指定字符数组写入输出流。 示例代码：\nFileWriter fw = null; try { fw = new FileWriter(\u0026#34;output.txt\u0026#34;); char[] chars = {\u0026#39;H\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;}; fw.write(chars); // 将字符数组写入文件 } catch (IOException e) { e.printStackTrace(); } finally { try { if (fw != null) { fw.close(); } } catch (IOException e) { e.printStackTrace(); } } 写入指定字符数组：write(char[] cbuf, int off, int len) 方法，将指定字符数组的一部分写入输出流。 代码示例：\nfw = new FileWriter(\u0026#34;output.txt\u0026#34;); char[] chars = {\u0026#39;H\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39; \u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;!\u0026#39;}; fw.write(chars, 0, 5); // 将字符数组的前 5 个字符写入文件 写入字符串：write(String str) 方法，将指定字符串写入输出流。代码示例如下： fw = new FileWriter(\u0026#34;output.txt\u0026#34;); String str = \u0026#34;沉默王二\u0026#34;; fw.write(str); // 将字符串写入文件 写入指定字符串：write(String str, int off, int len) 方法，将指定字符串的一部分写入输出流。代码示例如下（try-with-resources 形式）： String str = \u0026#34;沉默王二真的帅啊！\u0026#34;; try (FileWriter fw = new FileWriter(\u0026#34;output.txt\u0026#34;)) { fw.write(str, 0, 5); // 将字符串的前 5 个字符写入文件 } catch (IOException e) { e.printStackTrace(); } 注意：如果不关闭资源，数据只是保存在缓冲区，并未保存到文件中。\n关闭流 close 和刷新 flush FileWriter 内置了缓冲区 ByteBuffer，所以如果不关闭输出流，就无法把字符写入到文件中\n但是关闭了流对象，就无法继续写数据了。如果我们既想写入数据，又想继续使用流，就需要 flush 方法了。\nflush ：刷新缓冲区，流对象可以继续使用。\nclose ：先刷新缓冲区，然后通知系统释放资源。流对象不可以再被使用了。\n先刷新 flush 可以继续写数据，关闭了流之后就不能再写数据了 即使 flush 写入了数据，最后还是要 close 关闭流 FileWriter 的续写和换行 续写和换行：操作类似于 FileOutputStream，直接上代码：\n// 使用文件名称创建流对象，可以续写数据 FileWriter fw = new FileWriter(\u0026#34;fw.txt\u0026#34;,true); // 写出字符串 fw.write(\u0026#34;沉默王二\u0026#34;); // 写出换行 fw.write(\u0026#34;\\r\\n\u0026#34;); // 写出字符串 fw.write(\u0026#34;是傻 X\u0026#34;); // 关闭资源 fw.close(); 输出：\n沉默王二 是傻 X 文本文件复制 代码示例：\nimport java.io.FileReader; import java.io.FileWriter; import java.io.IOException; public class CopyFile { public static void main(String[] args) throws IOException { //创建输入流对象 FileReader fr=new FileReader(\u0026#34;aa.txt\u0026#34;);//文件不存在会抛出java.io.FileNotFoundException //创建输出流对象 FileWriter fw=new FileWriter(\u0026#34;copyaa.txt\u0026#34;); /*创建输出流做的工作： * 1、调用系统资源创建了一个文件 * 2、创建输出流对象 * 3、把输出流对象指向文件 * */ //文本文件复制，一次读一个字符 copyMethod1(fr, fw); //文本文件复制，一次读一个字符数组 copyMethod2(fr, fw); fr.close(); fw.close(); } public static void copyMethod1(FileReader fr, FileWriter fw) throws IOException { int ch; while((ch=fr.read())!=-1) {//读数据 fw.write(ch);//写数据 } fw.flush(); } public static void copyMethod2(FileReader fr, FileWriter fw) throws IOException { char chs[]=new char[1024]; int len=0; while((len=fr.read(chs))!=-1) {//读数据 fw.write(chs,0,len);//写数据 } fw.flush(); } } 小结 Writer 和 Reader 是 Java I/O 中用于字符输入输出的抽象类，它们提供了一系列方法用于读取和写入字符数据。它们的区别在于 Writer 用于将字符数据写入到输出流中，而 Reader 用于从输入流中读取字符数据。\nWriter 和 Reader 的常用子类有 FileWriter、FileReader，可以将字符流写入和读取到文件中。\n在使用 Writer 和 Reader 进行字符输入输出时，需要注意字符编码的问题。\n缓冲流-IO 流读写效率提高 Java 的缓冲流是对字节流和字符流的一种封装，通过在内存中开辟缓冲区来提高 I/O 操作的效率。Java 通过 BufferedInputStream 和 BufferedOutputStream 来实现字节流的缓冲，通过 BufferedReader 和 BufferedWriter 来实现字符流的缓冲。\n**缓冲流的工作原理是将数据先写入缓冲区中，当缓冲区满时再一次性写入文件或输出流，或者当缓冲区为空时一次性从文件或输入流中读取一定量的数据。**这样可以减少系统的 I/O 操作次数，提高系统的 I/O 效率，从而提高程序的运行效率。\n字节缓冲流 BufferedInputStream 和 BufferedOutputStream 属于字节缓冲流，强化了字节流 InputStream 和 OutputStream。\n构造方法 BufferedInputStream(InputStream in) ：创建一个新的缓冲输入流，注意参数类型为InputStream。 BufferedOutputStream(OutputStream out)： 创建一个新的缓冲输出流，注意参数类型为OutputStream。 示例：\n// 创建字节缓冲输入流，先声明字节流 FileInputStream fps = new FileInputStream(b.txt); BufferedInputStream bis = new BufferedInputStream(fps); // 创建字节缓冲输入流（一步到位） BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\u0026#34;b.txt\u0026#34;)); // 创建字节缓冲输出流（一步到位） BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\u0026#34;b.txt\u0026#34;)); 缓冲流的高效 通过复制一个 370M+ 的大文件，来测试缓冲流的效率。为了做对比，我们先用基本流来实现一下，代码如下：\n// 记录开始时间 long start = System.currentTimeMillis(); // 创建流对象 try (FileInputStream fis = new FileInputStream(\u0026#34;py.mp4\u0026#34;);//exe文件够大 FileOutputStream fos = new FileOutputStream(\u0026#34;copyPy.mp4\u0026#34;)){ // 读写数据 int b; while ((b = fis.read()) != -1) { fos.write(b); } } // 记录结束时间 long end = System.currentTimeMillis(); System.out.println(\u0026#34;普通流复制时间:\u0026#34;+(end - start)+\u0026#34; 毫秒\u0026#34;); 切换到缓冲流试一下，代码如下：\n// 记录开始时间 long start = System.currentTimeMillis(); // 创建流对象 try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\u0026#34;py.mp4\u0026#34;)); BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\u0026#34;copyPy.mp4\u0026#34;));){ // 读写数据 int b; while ((b = bis.read()) != -1) { bos.write(b); } } // 记录结束时间 long end = System.currentTimeMillis(); System.out.println(\u0026#34;缓冲流复制时间:\u0026#34;+(end - start)+\u0026#34; 毫秒\u0026#34;); 如何更快呢，可以换数组的方式来读写\n// 记录开始时间 long start = System.currentTimeMillis(); // 创建流对象 try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\u0026#34;py.mp4\u0026#34;)); BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\u0026#34;copyPy.mp4\u0026#34;));){ // 读写数据 int len; byte[] bytes = new byte[8*1024]; while ((len = bis.read(bytes)) != -1) { bos.write(bytes, 0 , len); } } // 记录结束时间 long end = System.currentTimeMillis(); System.out.println(\u0026#34;缓冲流使用数组复制时间:\u0026#34;+(end - start)+\u0026#34; 毫秒\u0026#34;); 字符缓冲流 BufferedReader 类继承自 Reader 类，提供了一些便捷的方法，例如 readLine() 方法可以一次读取一行数据，而不是一个字符一个字符地读取。\nBufferedWriter 类继承自 Writer 类，提供了一些便捷的方法，例如 newLine() 方法可以写入一个系统特定的行分隔符。\n构造方法 BufferedReader(Reader in) ：创建一个新的缓冲输入流，注意参数类型为Reader。 BufferedWriter(Writer out)： 创建一个新的缓冲输出流，注意参数类型为Writer。 代码示例：\n// 创建字符缓冲输入流 BufferedReader br = new BufferedReader(new FileReader(\u0026#34;b.txt\u0026#34;)); // 创建字符缓冲输出流 BufferedWriter bw = new BufferedWriter(new FileWriter(\u0026#34;b.txt\u0026#34;)); 字符缓冲流特有方法 字符缓冲流的基本方法与普通字符流调用方式一致，这里不再赘述，我们来看字符缓冲流特有的方法。\nBufferedReader：String readLine(): 读一行数据，读取到最后返回 null BufferedWriter：newLine(): 换行，由系统定义换行符。 readLine 方法示例：\n// 创建流对象 BufferedReader br = new BufferedReader(new FileReader(\u0026#34;a.txt\u0026#34;)); // 定义字符串,保存读取的一行文字 String line = null; // 循环读取,读取到最后返回null while ((line = br.readLine())!=null) { System.out.print(line); System.out.println(\u0026#34;------\u0026#34;); } // 释放资源 br.close(); newLine 方法示例：\n// 创建流对象 BfferedWriter bw = new BufferedWriter(new FileWriter(\u0026#34;b.txt\u0026#34;)); // 写出数据 bw.write(\u0026#34;沉\u0026#34;); // 写出换行 bw.newLine(); bw.write(\u0026#34;默\u0026#34;); bw.newLine(); bw.write(\u0026#34;王\u0026#34;); bw.newLine(); bw.write(\u0026#34;二\u0026#34;); bw.newLine(); // 释放资源 bw.close(); 转换流-字节流和字符流的桥梁 **转换流可以将一个字节流包装成字符流，或者将一个字符流包装成字节流。**这种转换通常用于处理文本数据，如读取文本文件或将数据从网络传输到应用程序。\n转换流主要有两种类型：\nInputStreamReader\nOutputStreamWriter\nInputStreamReader 将一个字节输入流转换为一个字符输入流，而 OutputStreamWriter 将一个字节输出流转换为一个字符输出流。它们使用指定的字符集将字节流和字符流之间进行转换。常用的字符集包括 UTF-8、GBK、ISO-8859-1 等。\n编码和解码 计算机中，数据通常以二进制形式存储和传输。\n编码就是将原始数据（比如说文本、图像、视频、音频等）转换为二进制形式。 解码就是将二进制数据转换为原始数据，是一个反向的过程。 常见的编码和解码方式有很多，举几个例子：\n**ASCII 编码和解码：**在计算机中，常常使用 ASCII 码来表示字符，如键盘上的字母、数字和符号等。例如，字母 A 对应的 ASCII 码是 65，字符 + 对应的 ASCII 码是 43。 **Unicode 编码和解码：**Unicode 是一种字符集，支持多种语言和字符集。在计算机中，Unicode 可以使用 UTF-8、UTF-16 等编码方式将字符转换为二进制数据进行存储和传输。 **Base64 编码和解码：**Base64 是一种将二进制数据转换为 ASCII 码的编码方式。它将 3 个字节的二进制数据转换为 4 个 ASCII 字符，以便在网络传输中使用。例如，将字符串 \u0026ldquo;Hello, world!\u0026rdquo; 进行 Base64 编码后，得到的结果是 \u0026ldquo;SGVsbG8sIHdvcmxkIQ==\u0026quot;。 **图像编码和解码：**在图像处理中，常常使用 JPEG、PNG、GIF 等编码方式将图像转换为二进制数据进行存储和传输。在解码时，可以将二进制数据转换为图像，以便显示或处理。 **视频编码和解码：**在视频处理中，常常使用 H.264、AVC、MPEG-4 等编码方式将视频转换为二进制数据进行存储和传输。在解码时，可以将二进制数据转换为视频，以便播放或处理。 简单一点就是：\n编码：字符（能看懂的）=\u0026gt;字节（看不懂的） 解码：字节（看不懂的）=\u0026gt;字符（能看懂的） 代码示例：\nString str = \u0026#34;沉默王二\u0026#34;; String charsetName = \u0026#34;UTF-8\u0026#34;; // 编码 byte[] bytes = str.getBytes(Charset.forName(charsetName)); System.out.println(\u0026#34;编码: \u0026#34; + bytes); // 解码 String decodedStr = new String(bytes, Charset.forName(charsetName)); System.out.println(\u0026#34;解码: \u0026#34; + decodedStr); 输出：\n编码: [B@53bd815b 解码: 沉默王二 在这个示例中，首先定义了一个字符串变量 str 和一个字符集名称 charsetName。然后，使用 Charset.forName() 方法获取指定字符集的 Charset 对象。接着，使用字符串的 getBytes() 方法将字符串编码为指定字符集的字节数组。最后，使用 new String() 方法将字节数组解码为字符串。\n需要注意的是，在编码和解码过程中，要保证使用相同的字符集，以便正确地转换数据\n字符集 Charset：字符集，是一组字符的集合，每个字符都有一个唯一的编码值，也称为码点。\n常见的字符集包括 ASCII、Unicode 和 GBK，而 Unicode 字符集包含了多种编码方式，比如说 UTF-8、UTF-16。\nASCII 字符集 ASCII（American Standard Code for Information Interchange，美国信息交换标准代码）字符集是一种最早的字符集，包含 128 个字符，其中包括控制字符、数字、英文字母以及一些标点符号。ASCII 字符集中的每个字符都有一个唯一的 7 位二进制编码（由 0 和 1 组成），可以表示为十进制数或十六进制数。\nASCII 编码方式是一种固定长度的编码方式，每个字符都使用 7 位二进制编码来表示。ASCII 编码只能表示英文字母、数字和少量的符号，不能表示其他语言的文字和符号，因此在全球范围内的应用受到了很大的限制\nUnicode 字符集 Unicode 包含了世界上几乎所有的字符，用于表示人类语言、符号和表情等各种信息。Unicode 字符集中的每个字符都有一个唯一的码点（code point），用于表示该字符在字符集中的位置，可以用十六进制数表示。\n为了在计算机中存储和传输 Unicode 字符集中的字符，需要使用一种编码方式。UTF-8、UTF-16 和 UTF-32 都是 Unicode 字符集的编码方式，用于将 Unicode 字符集中的字符转换成字节序列，以便于存储和传输。它们的差别在于使用的字节长度不同。\nUTF-8 是一种可变长度的编码方式，对于 ASCII 字符（码点范围为 0x00~0x7F），使用一个字节表示，对于其他 Unicode 字符，使用两个、三个或四个字节表示。UTF-8 编码方式被广泛应用于互联网和计算机领域，因为它可以有效地压缩数据，适用于网络传输和存储。 UTF-16 是一种固定长度的编码方式，对于基本多语言平面（Basic Multilingual Plane，Unicode 字符集中的一个码位范围，包含了世界上大部分常用的字符，总共包含了超过 65,000 个码位）中的字符（码点范围为 0x0000~0xFFFF），使用两个字节表示，对于其他 Unicode 字符，使用四个字节表示。 UTF-32 是一种固定长度的编码方式，对于所有 Unicode 字符，使用四个字节表示。 GBK 字符集 GBK 包含了 GB2312 字符集中的字符，同时还扩展了许多其他汉字字符和符号，共收录了 21,913 个字符。GBK 采用双字节编码方式，每个汉字占用 2 个字节，其中高字节和低字节都使用了 8 位，因此 GBK 编码共有 2^16=65536 种可能的编码，其中大部分被用于表示汉字字符。\nGBK 编码是一种变长的编码方式，对于 ASCII 字符（码位范围为 0x00 到 0x7F），使用一个字节表示，对于其他字符，使用两个字节表示。GBK 编码中的每个字节都可以采用 0x81 到 0xFE 之间的任意一个值，因此可以表示 2^15=32768 个字符。为了避免与 ASCII 码冲突，GBK 编码的第一个字节采用了 0x81 到 0xFE 之间除了 0x7F 的所有值，第二个字节采用了 0x40 到 0x7E 和 0x80 到 0xFE 之间的所有值，共 94 个值。\nGB2312 的全名是《信息交换用汉字编码字符集基本集》，也被称为“国标码”。采用了双字节编码方式，每个汉字占用 2 个字节，其中高字节和低字节都使用了 8 位，因此 GB2312 编码共有 2^16=65536 种可能的编码，其中大部分被用于表示汉字字符。GB2312 编码中的每个字节都可以采用 0xA1 到 0xF7 之间的任意一个值，因此可以表示 126 个字符。\nGB2312 是一个较为简单的字符集，只包含了常用的汉字和符号，因此对于一些较为罕见的汉字和生僻字，GB2312 不能满足需求，现在已经逐渐被 GBK、GB18030 等字符集所取代。\nGB18030 是最新的中文码表。收录汉字 70244 个，采用多字节编码，每个字可以由 1 个、2 个或 4 个字节组成。支持中国国内少数民族的文字，同时支持繁体汉字以及日韩汉字等。\n乱码 当使用不同的编码方式读取或者写文件时，就会出现乱码问题\nString s = \u0026#34;沉默王二！\u0026#34;; try { // 将字符串按GBK编码方式保存到文件中 OutputStreamWriter out = new OutputStreamWriter( new FileOutputStream(\u0026#34;logs/test_utf8.txt\u0026#34;), \u0026#34;GBK\u0026#34;); out.write(s); out.close(); FileReader fileReader = new FileReader(\u0026#34;logs/test_utf8.txt\u0026#34;); int read; while ((read = fileReader.read()) != -1) { System.out.print((char)read); } fileReader.close(); } catch (IOException e) { e.printStackTrace(); } 上面的示例代码中，首先定义了一个包含中文字符的字符串，然后将该字符串按 GBK 编码方式保存到文件中，接着将文件按默认编码方式（UTF-8）读取，并显示内容。此时就会出现乱码问题，显示为“��Ĭ������”。\n这是因为文件中的 GBK 编码的字符在使用 UTF-8 编码方式解析时无法正确解析，从而导致出现乱码问题。\n那如何才能解决乱码问题呢？\n这就引出我们今天的主角了——转换流。\nInputStreamReader java.io.InputStreamReader 是 Reader 类的子类。它的作用是将字节流（InputStream）转换为字符流（Reader），同时支持指定的字符集编码方式，从而实现字符流与字节流之间的转换\n构造方法 InputStreamReader(InputStream in): 创建一个使用默认字符集的字符流。 InputStreamReader(InputStream in, String charsetName): 创建一个指定字符集的字符 代码示例：\nInputStreamReader isr = new InputStreamReader(new FileInputStream(\u0026#34;in.txt\u0026#34;)); InputStreamReader isr2 = new InputStreamReader(new FileInputStream(\u0026#34;in.txt\u0026#34;) , \u0026#34;GBK\u0026#34;); 解决编码问题 一个使用 InputStreamReader 解决乱码问题的示例代码：\nString s = \u0026#34;沉默王二！\u0026#34;; try { // 将字符串按GBK编码方式保存到文件中 OutputStreamWriter outUtf8 = new OutputStreamWriter( new FileOutputStream(\u0026#34;logs/test_utf8.txt\u0026#34;), \u0026#34;GBK\u0026#34;); outUtf8.write(s); outUtf8.close(); // 将字节流转换为字符流，使用GBK编码方式 InputStreamReader isr = new InputStreamReader(new FileInputStream(\u0026#34;logs/test_utf8.txt\u0026#34;), \u0026#34;GBK\u0026#34;); // 读取字符流 int c; while ((c = isr.read()) != -1) { System.out.print((char) c); } isr.close(); } catch (IOException e) { e.printStackTrace(); } 由于使用了 InputStreamReader 对字节流进行了编码方式的转换，因此在读取字符流时就可以正确地解析出中文字符，避免了乱码问题\nOutStreamWriter java.io.OutputStreamWriter 是 Writer 的子类，字面看容易误以为是转为字符流，其实是将字符流转换为字节流，是字符流到字节流的桥梁。\nOutputStreamWriter(OutputStream in): 创建一个使用默认字符集的字符流。 OutputStreamWriter(OutputStream in, String charsetName)：创建一个指定字符集的字符流 代码示例：\nOutputStreamWriter isr = new OutputStreamWriter(new FileOutputStream(\u0026#34;a.txt\u0026#34;)); OutputStreamWriter isr2 = new OutputStreamWriter(new FileOutputStream(\u0026#34;b.txt\u0026#34;) , \u0026#34;GBK\u0026#34;); 通常为了提高读写效率，我们会在转换流上再加一层缓冲流，代码示例：\ntry { // 从文件读取字节流，使用UTF-8编码方式 FileInputStream fis = new FileInputStream(\u0026#34;test.txt\u0026#34;); // 将字节流转换为字符流，使用UTF-8编码方式 InputStreamReader isr = new InputStreamReader(fis, \u0026#34;UTF-8\u0026#34;); // 使用缓冲流包装字符流，提高读取效率 BufferedReader br = new BufferedReader(isr); // 创建输出流，使用UTF-8编码方式 FileOutputStream fos = new FileOutputStream(\u0026#34;output.txt\u0026#34;); // 将输出流包装为转换流，使用UTF-8编码方式 OutputStreamWriter osw = new OutputStreamWriter(fos, \u0026#34;UTF-8\u0026#34;); // 使用缓冲流包装转换流，提高写入效率 BufferedWriter bw = new BufferedWriter(osw); // 读取输入文件的每一行，写入到输出文件中 String line; while ((line = br.readLine()) != null) { bw.write(line); bw.newLine(); // 每行结束后写入一个换行符 } // 关闭流 br.close(); bw.close(); } catch (IOException e) { e.printStackTrace(); } 在上面的示例代码中，首先使用 FileInputStream 从文件中读取字节流，使用 UTF-8 编码方式进行读取。然后，使用 InputStreamReader 将字节流转换为字符流，使用 UTF-8 编码方式进行转换。接着，使用 BufferedReader 包装字符流，提高读取效率。然后，创建 FileOutputStream 用于输出文件，使用 UTF-8 编码方式进行创建。接着，使用 OutputStreamWriter 将输出流转换为字符流，使用 UTF-8 编码方式进行转换。最后，使用 BufferedWriter 包装转换流，提高写入效率\n小结 InputStreamReader 和 OutputStreamWriter 是将字节流转换为字符流或者将字符流转换为字节流。通常用于解决字节流和字符流之间的转换问题，可以将字节流以指定的字符集编码方式转换为字符流，或者将字符流以指定的字符集编码方式转换为字节流。\nInputStreamReader 类的常用方法包括：\nread()：从输入流中读取一个字符的数据。 read(char[] cbuf, int off, int len)：从输入流中读取 len 个字符的数据到指定的字符数组 cbuf 中，从 off 位置开始存放。 ready()：返回此流是否已准备好读取。 close()：关闭输入流。 OutputStreamWriter 类的常用方法包括：\nwrite(int c)：向输出流中写入一个字符的数据。 write(char[] cbuf, int off, int len)：向输出流中写入指定字符数组 cbuf 中的 len 个字符，从 off 位置开始。 flush()：将缓冲区的数据写入输出流中。 close()：关闭输出流。 在使用转换流时，需要指定正确的字符集编码方式，否则可能会导致数据读取或写入出现乱码\n序列流-Java 对象的序列化和反序列化 Java 的序列流（ObjectInputStream 和 ObjectOutputStream）是一种可以将 Java 对象序列化和反序列化的流。\n序列化是指将一个对象转换为一个字节序列（包含对象的数据、对象的类型和对象中存储的属性等信息），以便在网络上传输或保存到文件中，或者在程序之间传递。在 Java 中，序列化通过实现 java.io.Serializable 接口来实现，只有实现了 Serializable 接口的对象才能被序列化。\n反序列化是指将一个字节序列转换为一个对象，以便在程序中使用。\nObjectOutputStream java.io.ObjectOutputStream 继承自 OutputStream 类，因此可以将序列化后的字节序列写入到文件、网络等输出流中。\n来看 ObjectOutputStream 的构造方法： ObjectOutputStream(OutputStream out)\n该构造方法接收一个 OutputStream 对象作为参数，用于将序列化后的字节序列输出到指定的输出流中。例如：\nFileOutputStream fos = new FileOutputStream(\u0026#34;file.txt\u0026#34;); ObjectOutputStream oos = new ObjectOutputStream(fos); 一个对象要想序列化，必须满足两个条件:\n该类必须实现[java.io.Serializable 接口，否则会抛出NotSerializableException` 。 该类的所有字段都必须是可序列化的。如果一个字段不需要序列化，则需要使用transient 关键字进行修饰。 使用示例如下：\npublic class Employee implements Serializable { public String name; public String address; public transient int age; // transient瞬态修饰成员,不会被序列化 } 下面，来聊聊 writeObject (Object obj) 方法，该方法是 ObjectOutputStream 类中用于将对象序列化成字节序列并输出到输出流中的方法，可以处理对象之间的引用关系、继承关系、静态字段和 transient 字段。\npublic class ObjectOutputStreamDemo { public static void main(String[] args) { Person person = new Person(\u0026#34;沉默王二\u0026#34;, 20); try { FileOutputStream fos = new FileOutputStream(\u0026#34;logs/person.dat\u0026#34;); ObjectOutputStream oos = new ObjectOutputStream(fos); oos.writeObject(person); oos.close(); } catch (IOException e) { e.printStackTrace(); } } } class Person implements Serializable { private String name; private int age; public Person(String name, int age) { this.name = name; this.age = age; } public String getName() { return name; } public int getAge() { return age; } } 上面的代码中，首先创建了一个 Person 对象，然后使用 FileOutputStream 和 ObjectOutputStream 将 Person 对象序列化并输出到 person.dat 文件中。在 Person 类中，实现了 Serializable 接口，表示该类可以进行对象序列化\nObjectInputStream ObjectInputStream 可以读取 ObjectOutputStream 写入的字节流，并将其反序列化为相应的对象（包含对象的数据、对象的类型和对象中存储的属性等信息）。\n说简单点就是，序列化之前是什么样子，反序列化后就是什么样子。\n来看一下构造方法：ObjectInputStream(InputStream in) ： 创建一个指定 InputStream 的 ObjectInputStream。\n其中，ObjectInputStream 的 readObject 方法用来读取指定文件中的对象，示例如下：\nString filename = \u0026#34;logs/person.dat\u0026#34;; // 待反序列化的文件名 try (FileInputStream fileIn = new FileInputStream(filename); ObjectInputStream in = new ObjectInputStream(fileIn)) { // 从指定的文件输入流中读取对象并反序列化 Object obj = in.readObject(); // 将反序列化后的对象强制转换为指定类型 Person p = (Person) obj; // 打印反序列化后的对象信息 System.out.println(\u0026#34;Deserialized Object: \u0026#34; + p); } catch (IOException | ClassNotFoundException e) { e.printStackTrace(); } 我们首先指定了待反序列化的文件名（前面通过 ObjectOutputStream 序列化后的文件），然后创建了一个 FileInputStream 对象和一个 ObjectInputStream 对象。接着我们调用 ObjectInputStream 的 readObject 方法来读取指定文件中的对象，并将其强制转换为 Person 类型。最后我们打印了反序列化后的对象信息\n","permalink":"https://lidengxm.github.io/posts/java/io%E6%B5%81%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/","summary":"IO 分类 字节流可以处理一切文件，而字符流只能处理文本 认识 IO IO，即 in 和 out，也就是输入和输出，指应用程序和外部设备之间的数据传递，常见的外部设备包括文件、管道、网络连接。 Java 中是通过流处理 IO 的，那么什么是流？ 流（Stream），是一个抽象的概念，是指一连串的数据（字符或字节），是以","title":"IO流知识图谱"},{"content":"HashMap 基本用法 这篇文章将通过源码的方式，详细透彻地讲清楚 Java 的 HashMap，包括 hash 方法的原理、HashMap 的扩容机制、HashMap 的加载因子为什么是 0.75 而不是 0.6、0.8，以及 HashMap 为什么是线程不安全的，基本上 HashMap 的常见面试题 open in new window，都会在这一篇文章里讲明白。\nHashMap 是 Java 中常用的数据结构之一，用于存储键值对。在 HashMap 中，每个键都映射到一个唯一的值，可以通过键来快速访问对应的值。\n来一段代码演示 HashMap 的增删改查\nHashMap\u0026lt;String,Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); //添加元素 map.put(\u0026#34;hak\u0026#34;,20); map.put(\u0026#34;xiaomign\u0026#34;,25); //移除元素 map.remove(\u0026#34;hak\u0026#34;); //修改键的值 map.put(\u0026#34;xiaomign\u0026#34;,30); //查找键的值 map.get(\u0026#34;xiaomign\u0026#34;); 在实际应用中，HashMap 可以用于缓存、索引等场景。例如，可以将用户 ID 作为键，用户信息作为值，将用户信息缓存到 HashMap 中，以便快速查找。又如，可以将关键字作为键，文档 ID 列表作为值，将文档索引缓存到 HashMap 中，以便快速搜索文档。\nHashMap 的实现原理是基于哈希表的，它的底层是一个数组，数组的每个位置可能是一个链表或红黑树，也可能只是一个键值对（后面会讲）。当添加一个键值对时，HashMap 会根据键的哈希值计算出该键对应的数组下标（索引），然后将键值对插入到对应的位置。\n当通过键查找值时，HashMap 也会根据键的哈希值计算出数组下标，并查找对应的值\nhash 方法原理 看一下 hash 方法的源码（JDK 8 中的 HashMap）：\nstatic final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } 这段代码是将 key 的 hashCode 值进行处理，得到最终的哈希值。\n参数 key：需要计算哈希码的键值。 key == null ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16)：这是一个三目运算符，如果键值为 null，则哈希码为 0（依旧是说如果键为 null，则存放在第一个位置）；否则，通过调用 hashCode()方法获取键的哈希码，并将其与右移 16 位的哈希码进行异或运算。 ^ 运算符：异或运算符是 Java 中的一种位运算符，它用于将两个数的二进制位进行比较，如果相同则为 0，不同则为 1。 h \u0026raquo;\u0026gt; 16：将哈希码向右移动 16 位，相当于将原来的哈希码分成了两个 16 位的部分。 最终返回的是 key 经过异或运算后得到的哈希码值。 HashMap 的底层是通过数组的形式实现的，初始大小是 16（这个后面会讲），先记住。\n理论上，哈希值（哈希码）是一个 int 类型，范围从-2147483648 到 2147483648。前后加起来大概 40 亿的映射空间，只要哈希值映射得比较均匀松散，一般是不会出现哈希碰撞（哈希冲突会降低 HashMap 的效率）。\n但问题是一个 40 亿长度的数组，内存是放不下的。HashMap 扩容之前的数组初始大小只有 16，所以这个哈希值是不能直接拿来用的，用之前要和数组的长度做取模运算（前文提到的 (n - 1) \u0026amp; hash），用得到的余数来访问数组下标才行\n取模运算 取模运算（（“Modulo Operation”））和取余运算（（“Remainder Operation ”））是两种不同的运算方式，它们在计算机中的实现也不同。\n在数学中，取模运算和取余运算是等价的，都是计算一个数除以另一个数的余数。例如，10 mod 3 和 10 % 3 都等于 1，因为 10 除以 3 的余数是 1。\n在计算机中，取模运算和取余运算的差别在于，当被除数为负数时，取模运算的结果符号与被除数相同，取余运算的结果符号与除数相同。\n例如，-10 mod 3 的结果是 -1，而 -10 % 3 的结果是 2，因为 -10 除以 3 的余数是 -1，所以 -10 取模 3 的结果应该是 -1；而 -10 对 3 取余的结果是 2，因为：-10 ÷ 3 = -3 余 -1，由于除数为正数 3，余数的符号应与被除数的符号相同，因此余数应为正数 2，而不是 -1。\n在 Java 中，取模运算使用 % 运算符，取余运算使用 Math.floorMod() 方法。例如，计算 -10 mod 3 和 -10 % 3 的结果：\nint a = -10 % 3; // a = -1 int b = Math.floorMod(-10, 3); // b = 2 需要注意的是，在数学中，取模运算和取余运算都有定义域的限制，即除数不能为 0。在计算机中，除数为 0 会抛出异常或返回 NaN（Not a Number）。\nHashMap 的取模运算有两处。\n一处是往 HashMap 中 put 的时候（会调用私有的 putVal 方法）：\nfinal V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { // 数组 HashMap.Node\u0026lt;K,V\u0026gt;[] tab; // 元素 HashMap.Node\u0026lt;K,V\u0026gt; p; // n 为数组的长度 i 为下标 int n, i; // 数组为空的时候 if ((tab = table) == null || (n = tab.length) == 0) // 第一次扩容后的数组长度 n = (tab = resize()).length; // 计算节点的插入位置，如果该位置为空，则新建一个节点插入 if ((p = tab[i = (n - 1) \u0026amp; hash]) == null) tab[i] = newNode(hash, key, value, null); } 其中 (n - 1) \u0026amp; hash 为取模运算，为什么没用 %，我们随后解释。\n一处是从 HashMap 中 get 的时候（会调用 getNode 方法）：\nfinal Node\u0026lt;K,V\u0026gt; getNode(int hash, Object key) { // 获取当前的数组和长度，以及当前节点链表的第一个节点（根据索引直接从数组中找） Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; first, e; int n; K k; if ((tab = table) != null \u0026amp;\u0026amp; (n = tab.length) \u0026gt; 0 \u0026amp;\u0026amp; (first = tab[(n - 1) \u0026amp; hash]) != null) { // 如果第一个节点就是要查找的节点，则直接返回 if (first.hash == hash \u0026amp;\u0026amp; ((k = first.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) return first; // 如果第一个节点不是要查找的节点，则遍历节点链表查找 if ((e = first.next) != null) { do { if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } // 如果节点链表中没有找到对应的节点，则返回 null return null; } 看到没，取模运算 (n - 1) \u0026amp; hash 再次出现，说简单点，就是把键的哈希码经过 hash() 方法计算后，再和（数组长度-1）做了一个“与”运算\n取模运算是为了计算数组的下标\nput 的时候计算下标，把键值对放到对应的桶上。 get 的时候通过下标，把键值对从对应的桶上取出来 为什么取模运算之前要调用 hash 方法呢？ 某哈希值为 11111111 11111111 11110000 1110 1010，将它右移 16 位（h \u0026raquo;\u0026gt; 16），刚好是 00000000 00000000 11111111 11111111，再进行异或操作（h ^ (h \u0026raquo;\u0026gt; 16)），结果是 11111111 11111111 00001111 00010101\n异或（^）运算是基于二进制的位运算，采用符号 XOR 或者^来表示，运算规则是：如果是同值取 0、异值取 1\n由于混合了原来哈希值的高位和低位，所以低位的随机性加大了（掺杂了部分高位的特征，高位的信息也得到了保留）。\n结果再与数组长度-1（00000000 00000000 00000000 00001111）做取模运算，得到的下标就是 00000000 00000000 00000000 00000101，也就是 5。\nhash 方法是用来做哈希值优化的，把哈希值右移 16 位，也就正好是自己长度的一半，之后与原哈希值做异或运算，这样就混合了原哈希值中的高位和低位，增大了随机性。\n说白了，hash 方法就是为了增加随机性，让数据元素更加均衡的分布，减少 hash 碰撞。\n小结 hash 方法的主要作用是将 key 的 hashCode 值进行处理，得到最终的哈希值。由于 key 的 hashCode 值是不确定的，可能会出现哈希冲突，因此需要将哈希值通过一定的算法映射到 HashMap 的实际存储位置上。\nhash 方法的原理是，**先获取 key 对象的 hashCode 值，然后将其高位与低位进行异或操作，得到一个新的哈希值。**为什么要进行异或操作呢？因为对于 hashCode 的高位和低位，它们的分布是比较均匀的，如果只是简单地将它们加起来或者进行位运算，容易出现哈希冲突，而异或操作可以避免这个问题。\n然后将新的哈希值取模（mod），得到一个实际的存储位置。这个取模操作的目的是将哈希值映射到桶（Bucket）的索引上，桶是 HashMap 中的一个数组，每个桶中会存储着一个链表（或者红黑树），装载哈希值相同的键值对（没有相同哈希值的话就只存储一个键值对）。\n总的来说，HashMap 的 hash 方法就是将 key 对象的 hashCode 值进行处理，得到最终的哈希值，并通过一定的算法映射到实际的存储位置上。这个过程决定了 HashMap 内部键值对的查找效率\nHashMap 的扩容机制 数组一旦初始化后大小就无法改变了，所以就有了 ArrayList这种“动态数组”，可以自动扩容。\nHashMap 的底层用的也是数组。向 HashMap 里不停地添加元素，当数组无法装载更多元素时，就需要对数组进行扩容，以便装入更多的元素；除此之外，容量的提升也会相应地提高查询效率，因为“桶（坑）”更多了嘛，原来需要通过链表存储的（查询的时候需要遍历），扩容后可能就有自己专属的“坑位”了（直接就能查出来）。\nresize 方法 HashMap 的扩容是通过 resize 方法来实现的，JDK 8 中融入了红黑树（链表长度超过 8 的时候，会将链表转化为红黑树来提高查询效率），对于新手来说，可能比较难理解。\n为了减轻大家的学习压力，就还使用 JDK 7 的源码，搞清楚了 JDK 7 的，再看 JDK 8 的就会轻松很多。\n来看 Java7 的 resize 方法源码，我加了注释：\n// newCapacity为新的容量 void resize(int newCapacity) { // 小数组，临时过度下 Entry[] oldTable = table; // 扩容前的容量 int oldCapacity = oldTable.length; // MAXIMUM_CAPACITY 为最大容量，2 的 30 次方 = 1\u0026lt;\u0026lt;30 if (oldCapacity == MAXIMUM_CAPACITY) { // 容量调整为 Integer 的最大值 0x7fffffff（十六进制）=2 的 31 次方-1 threshold = Integer.MAX_VALUE; return; } // 初始化一个新的数组（大容量） Entry[] newTable = new Entry[newCapacity]; // 把小数组的元素转移到大数组中 transfer(newTable, initHashSeedAsNeeded(newCapacity)); // 引用新的大数组 table = newTable; // 重新计算阈值 threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1); } 该方法接收一个新的容量 newCapacity，然后将 HashMap 的容量扩大到 newCapacity。\n首先，方法获取当前 HashMap 的旧数组 oldTable 和旧容量 oldCapacity。如果旧容量已经达到 HashMap 支持的最大容量 MAXIMUM_CAPACITY（ 2 的 30 次方），就将新的阈值 threshold 调整为 Integer.MAX_VALUE（2 的 31 次方 - 1），这是因为 HashMap 的容量不能超过 MAXIMUM_CAPACITY。\n因为 2,147,483,647（Integer.MAX_VALUE） - 1,073,741,824（MAXIMUM_CAPACITY） = 1,073,741,823，刚好相差一倍（HashMap 每次扩容都是之前的一倍）。\n接着，**方法创建一个新的数组 newTable，并将旧数组 oldTable 中的元素转移到新数组 newTable 中。转移过程是通过调用 transfer 方法来实现的。**该方法遍历旧数组中的每个桶，并将每个桶中的键值对重新计算哈希值后，将其插入到新数组对应的桶中。\n转移完成后，方**法将 HashMap 内部的数组引用 table 指向新数组 newTable，并重新计算阈值 threshold。**新的阈值是新容量 newCapacity 乘以负载因子 loadFactor 的结果，但如果计算结果超过了 HashMap 支持的最大容量 MAXIMUM_CAPACITY，则将阈值设置为 MAXIMUM_CAPACITY + 1，这是因为 HashMap 的元素数量不能超过 MAXIMUM_CAPACITY\n新容量 newCapacity 那 JDK7 中 newCapacity 是如何计算的呢？\nint newCapacity = oldCapacity * 2; if (newCapacity \u0026lt; 0 || newCapacity \u0026gt;= MAXIMUM_CAPACITY) { newCapacity = MAXIMUM_CAPACITY; } else if (newCapacity \u0026lt; DEFAULT_INITIAL_CAPACITY) { newCapacity = DEFAULT_INITIAL_CAPACITY; } 新容量 newCapacity 被初始化为原容量 oldCapacity 的两倍。\n然后，如果 newCapacity 超过了 HashMap 的容量限制 MAXIMUM_CAPACITY（2^30），就将 newCapacity 设置为 MAXIMUM_CAPACITY。**如果 newCapacity 小于默认初始容量 DEFAULT_INITIAL_CAPACITY（16），就将 newCapacity 设置为 DEFAULT_INITIAL_CAPACITY。**这样可以避免新容量太小或太大导致哈希冲突过多或者浪费空间。\nJava 8 的时候，newCapacity 的计算方式发生了一些细微的变化。\nint newCapacity = oldCapacity \u0026lt;\u0026lt; 1; //如果新容量和旧容量都大于默认容量16 if (newCapacity \u0026gt;= DEFAULT_INITIAL_CAPACITY \u0026amp;\u0026amp; oldCapacity \u0026gt;= DEFAULT_INITIAL_CAPACITY) { //判断新容量是否大于最大容量 if (newCapacity \u0026gt; MAXIMUM_CAPACITY) newCapacity = MAXIMUM_CAPACITY; } else { //新容量和旧容量不大于默认容量16，判断新容量是否小于默认容量 if (newCapacity \u0026lt; DEFAULT_INITIAL_CAPACITY) //小于默认容量就等于默认容量 newCapacity = DEFAULT_INITIAL_CAPACITY; } 新容量 newCapacity 被初始化为原容量左移两位，二进制格式左移两位也就是变成原数的平方\na=39 b = a \u0026lt;\u0026lt; 2 左移运算符 \u0026laquo; ，即转换成二进制再左移两位\n比如 39，转换成二进制就是0010 0111，左移两位就是1001 1100，低位补 0，再转换成十进制就是 156，刚好变成了原来的二倍\ntransfer 方法 该方法用来转移，将旧的小数组元素拷贝到新的大数组中，还有一个参数 rehash，boolean 类型的，判断是否要重新计算 hash 值\nvoid transfer(Entry[] newTable, boolean rehash) { // 新的容量 int newCapacity = newTable.length; // 遍历小数组 for (Entry\u0026lt;K,V\u0026gt; e : table) { while(null != e) { // 拉链法，相同 key 上的不同值 Entry\u0026lt;K,V\u0026gt; next = e.next; // 是否需要重新计算 hash if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } // 根据大数组的容量，和键的 hash 计算元素在数组中的下标 int i = indexFor(e.hash, newCapacity); // 同一位置上的新元素被放在链表的头部 e.next = newTable[i]; // 放在新的数组上 newTable[i] = e; // 链表上的下一个元素 e = next; } } } 该方法接受一个新的 Entry 数组 newTable 和一个布尔值 rehash 作为参数，其中 newTable 表示新的哈希表，rehash 表示是否需要重新计算键的哈希值。\n在方法中，首先获取新哈希表（数组）的长度 newCapacity，然后遍历旧哈希表中的每个 Entry。对于每个 Entry，使用拉链法将相同 key 值的不同 value 值存储在同一个链表中。如果 rehash 为 true，则需要重新计算键的哈希值，并将新的哈希值存储在 Entry 的 hash 属性中。\n接着，根据新哈希表的长度和键的哈希值，计算 Entry 在新数组中的位置 i，然后将该 Entry 添加到新数组的 i 位置上。由于新元素需要被放在链表的头部，因此将新元素的下一个元素设置为当前数组位置上的元素。\n最后，遍历完旧哈希表中的所有元素后，转移工作完成，新的哈希表 newTable 已经包含了旧哈希表中的所有元素\n拉链法 注意：e.next = newTable[i]，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素最终会被放到链表的尾部，这就会导致在旧数组中同一个链表上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。\n现在假设 hash 算法就是简单的用键的哈希值（一个 int 值）和数组大小取模（也就是 hashCode % table.length）。\n继续假设：\n数组 table 的长度为 2 键的哈希值为 3、7、5 取模运算后，哈希冲突都到 table[1] 上了，因为余数为 1。那么扩容前的样子如下图所示。\n数组的容量为 2， key 3、7、5 都在 table[1] 的链表上。\n假设负载因子（后面会细讲） loadFactor 为 1，也就是当元素的个数大于 table 的长度时进行扩容。\n扩容后的数组容量为 2 \u0026raquo; 1 变成 4。\nkey 3 取模（3%4）后是 3，放在 table[3] 上。 key 7 取模（7%4）后是 3，放在 table[3] 上的链表头部。 key 5 取模（5%4）后是 1，放在 table[1] 上。 按照我们的预期，扩容后的 7 仍然应该在 3 这条链表的后面，但实际上呢？ 7 跑到 3 这条链表的头部了。\n针对 JDK 7 中的这个情况，JDK 8 做了哪些优化呢？\nn 为 table 的长度，默认值为 16。\nn-1 也就是二进制的 0000 1111 的 15 key1 哈希值的最后 8 位为 0000 0101 key2 哈希值的最后 8 位为 0001 0101（和 key1 不同） 做与运算后发生了哈希冲突，索引都在（0000 0101）上。 扩容后为 32。\nn-1 也就是二进制 0001 1111 的 31，扩容前是 0000 1111。 key1 哈希值的低位为 0000 0101 key2 哈希值的低位为 0001 0101（和 key1 不同） key1 做与运算后，索引为 0000 0101。 key2 做与运算后，索引为 0001 0101。 新的索引就会发生这样的变化：\n原来的索引是 5（0 0101） 原来的容量是 16 扩容后的容量是 32 扩容后的索引是 21（1 0101），也就是 5+16，也就是原来的索引+原来的容量 也就是说，JDK 8 不需要像 JDK 7 那样重新计算 hash，只需要看原来的 hash 值新增的那个 bit 是 1 还是 0 就好了，是 0 的话就表示索引没变，是 1 的话，索引就变成了“原索引+原来的容量”。\nJDK8 的这个设计非常巧妙，既省去了重新计算 hash 的时间，同时，由于新增的 1 bit 是 0 还是 1 是随机的，因此扩容的过程，可以均匀地把之前的节点分散到新的位置上。\nJDK8HashMap 扩容源码 Java8 扩容源代码：\nfinal Node\u0026lt;K,V\u0026gt;[] resize() { Node\u0026lt;K,V\u0026gt;[] oldTab = table; // 获取原来的数组 table int oldCap = (oldTab == null) ? 0 : oldTab.length; // 获取数组长度 oldCap int oldThr = threshold; // 获取阈值 oldThr int newCap, newThr = 0; if (oldCap \u0026gt; 0) { // 如果原来的数组 table 不为空 if (oldCap \u0026gt;= MAXIMUM_CAPACITY) { // 超过最大值就不再扩充了，就只好随你碰撞去吧 threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap \u0026lt;\u0026lt; 1) \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; oldCap \u0026gt;= DEFAULT_INITIAL_CAPACITY) //如果新数组小于最大容量 与 旧数组大于等于默认容量 newThr = oldThr \u0026lt;\u0026lt; 1; // double threshold } else if (oldThr \u0026gt; 0) // 旧数组阈值 oldThr大于0，oldCap大小等于0 newCap = oldThr; else { //阈值 oldThr等于0，大小oldCap也等于0 newCap = DEFAULT_INITIAL_CAPACITY;//新数组等于默认容量 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);//新数组阈值等于0.75*默认容量 } // 计算新的 resize 上限 if (newThr == 0) { float ft = (float)newCap * loadFactor;//新容量*扩容因子 //新数组的阈值等于 （新数组是否小于最大容量 与 ft是否小于最大容量），如果括号为真就等于ft否则等于最下值 newThr = (newCap \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; ft \u0026lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; // 将新阈值赋值给成员变量 threshold @SuppressWarnings({\u0026#34;rawtypes\u0026#34;,\u0026#34;unchecked\u0026#34;}) Node\u0026lt;K,V\u0026gt;[] newTab = (Node\u0026lt;K,V\u0026gt;[])new Node[newCap]; // 创建新数组 newTab table = newTab; // 将新数组 newTab 赋值给成员变量 table if (oldTab != null) { // 如果旧数组 oldTab 不为空 for (int j = 0; j \u0026lt; oldCap; ++j) { // 遍历旧数组的每个元素 Node\u0026lt;K,V\u0026gt; e; if ((e = oldTab[j]) != null) { // 如果该元素不为空 oldTab[j] = null; // 将旧数组中该位置的元素置为 null，以便垃圾回收 if (e.next == null) // 如果该元素没有冲突 newTab[e.hash \u0026amp; (newCap - 1)] = e; // 直接将该元素放入新数组 else if (e instanceof TreeNode) // 如果该元素是树节点 ((TreeNode\u0026lt;K,V\u0026gt;)e).split(this, newTab, j, oldCap); // 将该树节点分裂成两个链表 else { // 如果该元素是链表 Node\u0026lt;K,V\u0026gt; loHead = null, loTail = null; // 低位链表的头结点和尾结点 Node\u0026lt;K,V\u0026gt; hiHead = null, hiTail = null; // 高位链表的头结点和尾结点 Node\u0026lt;K,V\u0026gt; next; do { // 遍历该链表 next = e.next; if ((e.hash \u0026amp; oldCap) == 0) { // 如果该元素在低位链表中 if (loTail == null) // 如果低位链表还没有结点 loHead = e; // 将该元素作为低位链表的头结点 else loTail.next = e; // 如果低位链表已经有结点，将该元素加入低位链表的尾部 loTail = e; // 更新低位链表的尾结点 } else { // 如果该元素在高位链表中 if (hiTail == null) // 如果高位链表还没有结点 hiHead = e; // 将该元素作为高位链表的头结点 else hiTail.next = e; // 如果高位链表已经有结点，将该元素加入高位链表的尾部 hiTail = e; // 更新高位链表的尾结点 } } while ((e = next) != null); // if (loTail != null) { // 如果低位链表不为空 loTail.next = null; // 将低位链表的尾结点指向 null，以便垃圾回收 newTab[j] = loHead; // 将低位链表作为新数组对应位置的元素 } if (hiTail != null) { // 如果高位链表不为空 hiTail.next = null; // 将高位链表的尾结点指向 null，以便垃圾回收 newTab[j + oldCap] = hiHead; // 将高位链表作为新数组对应位置的元素 } } } } } return newTab; // 返回新数组 } 当 hashmap 的负载因子大于阈值时，会进行扩容\n扩容的流程：\n1、获取原来的数组 table、数组长度 oldCap 和阈值 oldThr。初始化新数组长度和阈值都为 0\n2、如果原来的数组 table 不为空，新数组小于最大容量 与 旧数组大于等于默认容量时，则根据扩容规则计算新数组长度 newCap 和新阈值 newThr，然后将原数组中的元素复制到新数组中。\n3、如果原来的数组 table 为空但阈值 oldThr 不为零，则说明是通过带参数构造函数创建的 HashMap，此时将旧数组的阈值作为新数组长度 newCap。\n4、如果原来的数组 table 和阈值 oldThr 都为零，则说明是通过无参数构造函数创建的 HashMap，此时将默认初始容量 DEFAULT_INITIAL_CAPACITY（16）赋值给新数组，新数组阈值 newThr 等于默认容量与默认负载因子 DEFAULT_LOAD_FACTOR（0.75）的乘积\n5、计算新阈值 threshold，并将其赋值给成员变量 threshold。\n6、创建新数组 newTab，并将其赋值给成员变量 table。\n7、如果旧数组 oldTab 不为空，则遍历旧数组的每个元素，将其复制到新数组中。\n8、返回新数组 newTab。\n小结 **HashMap 的内部实现是通过一个数组和链表或红黑树的组合来实现的。**当我们往 HashMap 中不断添加元素时，HashMap 会自动进行扩容操作（条件是元素数量达到负载因子（load factor）乘以数组长度时），以保证其存储的元素数量不会超出其容量限制。下面是 HashMap 的扩容机制：\n1、在进行扩容操作时，HashMap 会先将数组的长度扩大一倍，然后将原来的元素重新散列（这个词还是挺贴切的）到新的数组中。由于元素的散列位置是通过 key 的 hashcode 和数组长度取模得到的，因此在数组长度扩大后，元素的散列位置也会发生一些改变。\n2、在重新散列元素时，如果一个元素的散列位置发生了改变，那么它需要被移动到新的位置。如果新的位置上已经有元素了，那么这个元素就会被添加到链表的末尾，如果链表的长度超过了阈值（8 个），那么它将会被转换成红黑树。\n总之，HashMap 的扩容机制是通过增加数组长度和重新散列元素来实现的，它可以保证 HashMap 的存储容量足够大，同时也可以保证 HashMap 的存储效率和检索效率。但是，由于扩容操作需要耗费一定的时间和空间，因此我们需要在使用 HashMap 时，合理地设置初始容量和负载因子，以避免过多的扩容操作\n加载因子为什么是 0.75 //在HashMap的静态属性中定义 static final float DEFAULT_LOAD_FACTOR = 0.75f; HashMap 是用数组+链表/红黑树实现的，我们要想往 HashMap 中添加数据（元素/键值对）或者取数据，就需要确定数据在数组中的下标（索引）。\n先把数据的键进行一次 hash：\nstatic final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } 再做一次取模运算确定下标：\ni = (n - 1) \u0026amp; hash 那这样的过程容易产生两个问题：\n数组的容量过小，经过哈希计算后的下标，容易出现冲突； 数组的容量过大，导致空间利用率不高。 加载因子是用来表示 HashMap 中数据的填满程度：\n加载因子 = 填入哈希表中的数据个数 / 哈希表的长度\n这就意味着：\n加载因子越小，填满的数据就越少，哈希冲突的几率就减少了，但浪费了空间，而且还会提高扩容的触发几率； 加载因子越大，填满的数据就越多，空间利用率就高，但哈希冲突的几率就变大了。 这就必须在“哈希冲突”与“空间利用率”两者之间有所取舍，尽量保持平衡，谁也不碍着谁。\n我们知道，HashMap 是通过拉链法来解决哈希冲突的。\n为了减少哈希冲突发生的概率，当 HashMap 的数组长度达到一个临界值的时候，就会触发扩容，扩容后会将之前小数组中的元素转移到大数组中，这是一个相当耗时的操作。\n这个临界值由什么来确定呢？\n临界值 = 初始容量 * 加载因子\n一开始，HashMap 的容量是 16：\nstatic final int DEFAULT_INITIAL_CAPACITY = 1 \u0026lt;\u0026lt; 4; // aka 16 加载因子是 0.75：\nstatic final float DEFAULT_LOAD_FACTOR = 0.75f; 也就是说，当 16*0.75=12 时，会触发扩容机制。\n为什么加载因子会选择 0.75 呢？为什么不是 0.8、0.6 呢？\n这跟统计学里的一个很重要的原理——泊松分布有关。\n是时候上维基百科了：\n泊松分布，是一种统计与概率学里常见到的离散概率分布，由法国数学家西莫恩·德尼·泊松在 1838 年时提出。它会对随机事件的发生次数进行建模，适用于涉及计算在给定的时间段、距离、面积等范围内发生随机事件的次数的应用情形。\n阮一峰老师曾在一篇博文中详细的介绍了泊松分布和指数分布，大家可以去看一下。\n链接：https://www.ruanyifeng.com/blog/2015/06/poisson-distribution.html\n考虑到 HashMap 的容量有一个要求：它必须是 2 的 n 次幂。当加载因子选择了 0.75 就可以保证它与容量的乘积为整数。\n16*0.75=12 32*0.75=24 除了 0.75，0.5~1 之间还有 0.625（5/8）、0.875（7/8）可选，从中位数的角度，挑 0.75 比较完美。另外，维基百科上说，拉链法（解决哈希冲突的一种）的加载因子最好限制在 0.7-0.8 以下，超过 0.8，查表时的 CPU 缓存不命中（cache missing）会按照指数曲线上升。\n综上，0.75 是个比较完美的选择\n小结 **HashMap 的加载因子（load factor，直译为加载因子，意译为负载因子）是指哈希表中填充元素的个数与桶的数量的比值，当元素个数达到负载因子与桶的数量的乘积时，就需要进行扩容。**这个值一般选择 0.75，是因为这个值可以在时间和空间成本之间做到一个折中，使得哈希表的性能达到较好的表现。\n如果负载因子过大，填充因子较多，那么哈希表中的元素就会越来越多地聚集在少数的桶中，这就导致了冲突的增加，这些冲突会导致查找、插入和删除操作的效率下降。同时，这也会导致需要更频繁地进行扩容，进一步降低了性能。\n如果负载因子过小，那么桶的数量会很多，虽然可以减少冲突，但是在空间利用上面也会有浪费，因此选择 0.75 是为了取得一个平衡点，即在时间和空间成本之间取得一个比较好的平衡点。\n总之，选择 0.75 这个值是为了在时间和空间成本之间达到一个较好的平衡点，既可以保证哈希表的性能表现，又能够充分利用空间。\n线程不安全 三方面原因：\n多线程下扩容会死循环 多线程下 put 会导致元素丢失 put 和 get 并发时会导致 get 到 null 多线程下 put 会导致元素丢失 正常情况下，当发生哈希冲突时，HashMap 是这样的：\n但多线程同时执行 put 操作时，如果计算出来的索引位置是相同的，那会造成前一个 key 被后一个 key 覆盖，从而导致元素的丢失。\nput 的源码：\nfinal V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) \u0026amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node\u0026lt;K,V\u0026gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash \u0026amp;\u0026amp; ((k = p.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode\u0026lt;K,V\u0026gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); //链表长度大于8转换为红黑树进行处理 if (binCount \u0026gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } // key已经存在直接覆盖value if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) break; p = e; } } // 步骤⑥、直接覆盖 if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; // 步骤⑦：超过最大容量 就扩容 if (++size \u0026gt; threshold) resize(); afterNodeInsertion(evict); return null; } 问题发生在步骤 ② 这里：\nif ((p = tab[i = (n - 1) \u0026amp; hash]) == null) tab[i] = newNode(hash, key, value, null); 两个线程都执行了 if 语句，假设线程 A 先执行了 tab[i] = newNode(hash, key, value, null)，那 table 是这样的：\n接着，线程 B 执行了 tab[i] = newNode(hash, key, value, null)，那 table 是这样的：\n3 就被覆盖了\nput 和 get 并发时会导致 get 到 null 线程 A 执行 put 时，因为元素个数超出阈值而出现扩容，线程 B 此时执行 get，有可能导致这个问题。\n注意来看 resize 源码：\nfinal Node\u0026lt;K,V\u0026gt;[] resize() { Node\u0026lt;K,V\u0026gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap \u0026gt; 0) { // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap \u0026gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap \u0026lt;\u0026lt; 1) \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; oldCap \u0026gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr \u0026lt;\u0026lt; 1; // double threshold } else if (oldThr \u0026gt; 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } // 计算新的resize上限 if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; ft \u0026lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({\u0026#34;rawtypes\u0026#34;,\u0026#34;unchecked\u0026#34;}) Node\u0026lt;K,V\u0026gt;[] newTab = (Node\u0026lt;K,V\u0026gt;[])new Node[newCap]; table = newTab; } 线程 A 执行完 table = newTab 之后，线程 B 中的 table 此时也发生了变化，此时去 get 的时候当然会 get 到 null 了，因为元素还没有转移。\n参考链接：\nhttps://blog.csdn.net/lonyw/article/details/80519652open in new window https://zhuanlan.zhihu.com/p/91636401open in new window https://www.zhihu.com/question/20733617open in new window https://zhuanlan.zhihu.com/p/21673805 小结 HashMap 是线程不安全的主要是因为它在进行插入、删除和扩容等操作时可能会导致链表的结构发生变化，从而破坏了 HashMap 的不变性。具体来说，如果在一个线程正在遍历 HashMap 的链表时，另外一个线程对该链表进行了修改（比如添加了一个节点），那么就会导致链表的结构发生变化，从而破坏了当前线程正在进行的遍历操作，可能导致遍历失败或者出现死循环等问题。\n为了解决这个问题，Java 提供了线程安全的 HashMap 实现类 ConcurrentHashMap。ConcurrentHashMap 内部采用了分段锁（Segment），将整个 Map 拆分为多个小的 HashMap，每个小的 HashMap 都有自己的锁，不同的线程可以同时访问不同的小 Map，从而实现了线程安全。在进行插入、删除和扩容等操作时，只需要锁住当前小 Map，不会对整个 Map 进行锁定，提高了并发访问的效率\n总结 HashMap 是 Java 中最常用的集合之一，它是一种键值对存储的数据结构，可以根据键来快速访问对应的值。以下是对 HashMap 的总结：\nHashMap 采用数组+链表/红黑树的存储结构，能够在 O(1)的时间复杂度内实现元素的添加、删除、查找等操作。 HashMap 是线程不安全的，因此在多线程环境下需要使用ConcurrentHashMap来保证线程安全。 HashMap 的扩容机制是通过扩大数组容量和重新计算 hash 值来实现的，扩容时需要重新计算所有元素的 hash 值，因此在元素较多时扩容会影响性能。 在 Java 8 中，HashMap 的实现引入了拉链法、树化等机制来优化大量元素存储的情况，进一步提升了性能。 HashMap 中的 key 是唯一的，如果要存储重复的 key，则后面的值会覆盖前面的值。 HashMap 的初始容量和加载因子都可以设置，初始容量表示数组的初始大小，加载因子表示数组的填充因子。一般情况下，初始容量为 16，加载因子为 0.75。 HashMap 在遍历时是无序的，因此如果需要有序遍历，可以使用TreeMap。 综上所述，HashMap 是一种高效的数据结构，具有快速查找和插入元素的能力，但需要注意线程安全和性能问题。\nHashMap 可能问的面试题 HashMap 扩容原理 HashMap 的扩容是通过 resize 方法来实现的，JDK 8 中融入了红黑树（链表长度超过 8 的时候，会将链表转化为红黑树来提高查询效率）\njdk1.7 的 hashmap 扩容原理\n该方法接收一个新的容量 newCapacity，然后将 HashMap 的容量扩大到 newCapacity。\n首先，方法获取当前 HashMap 的旧数组 oldTable 和旧容量 oldCapacity。如果旧容量已经达到 HashMap 支持的最大容量 MAXIMUM_CAPACITY（ 2 的 30 次方），就将新的阈值 threshold 调整为 Integer.MAX_VALUE（2 的 31 次方 - 1），这是因为 HashMap 的容量不能超过 MAXIMUM_CAPACITY。\n因为 2,147,483,647（Integer.MAX_VALUE） - 1,073,741,824（MAXIMUM_CAPACITY） = 1,073,741,823，刚好相差一倍（HashMap 每次扩容都是之前的一倍）。\n接着，**方法创建一个新的数组 newTable，并将旧数组 oldTable 中的元素转移到新数组 newTable 中。转移过程是通过调用 transfer 方法来实现的。**该方法遍历旧数组中的每个桶，并将每个桶中的键值对重新计算哈希值后，将其插入到新数组对应的桶中。\n转移完成后，方**法将 HashMap 内部的数组引用 table 指向新数组 newTable，并重新计算阈值 threshold。**新的阈值是新容量 newCapacity 乘以负载因子 loadFactor 的结果，但如果计算结果超过了 HashMap 支持的最大容量 MAXIMUM_CAPACITY，则将阈值设置为 MAXIMUM_CAPACITY + 1，这是因为 HashMap 的元素数量不能超过 MAXIMUM_CAPACITY\njdk1.8hashmap 扩容原理\n当 hashmap 的负载因子 loadFactor 大于阈值 threshold 时，会进行扩容\n扩容的流程：\n1、获取原来的数组 table、数组长度 oldCap 和阈值 oldThr。初始化新数组长度和阈值都为 0\n2、如果原来的数组 table 不为空，新数组小于最大容量 与 旧数组大于等于默认容量时，则根据扩容规则计算新数组长度 newCap 和新阈值 newThr，然后将原数组中的元素复制到新数组中。\n3、如果原来的数组 table 为空但阈值 oldThr 不为零，则说明是通过带参数构造函数创建的 HashMap，此时将旧数组的阈值作为新数组长度 newCap。\n4、如果原来的数组 table 和阈值 oldThr 都为零，则说明是通过无参数构造函数创建的 HashMap，此时将默认初始容量 DEFAULT_INITIAL_CAPACITY（16）赋值给新数组，新数组阈值 newThr 等于默认容量与默认负载因子 DEFAULT_LOAD_FACTOR（0.75）的乘积\n5、计算新阈值 threshold，并将其赋值给成员变量 threshold。\n6、创建新数组 newTab，并将其赋值给成员变量 table。\n7、如果旧数组 oldTab 不为空，则遍历旧数组的每个元素，将其复制到新数组中。\n8、返回新数组 newTab。\n哈希表（HashMap）的扩容原理是为了应对哈希冲突和提高哈希表的性能。在哈希表中，当元素数量增多，哈希冲突可能会增加，导致哈希表的查找、插入和删除操作的性能下降。为了解决这个问题，当哈希表中的元素数量达到一定阈值时，就会进行扩容。\n哈希表的扩容过程如下：\n创建新的哈希表： 在开始扩容时，会创建一个新的哈希表，通常是原来哈希表大小的两倍或其他倍数。新的哈希表会有更多的桶（bucket）来容纳元素，从而降低哈希冲突的概率。\n重新哈希： 接下来，会遍历原有哈希表中的每个桶，将桶中的元素重新计算哈希值，并根据新的哈希值插入到新的哈希表中的对应桶中。这个过程被称为重新哈希。\n数据迁移： 在重新哈希的过程中，可能会发现原有哈希表中的元素需要放置到新的哈希表的不同桶中，这时会进行数据迁移的操作。数据迁移是将元素从原有的桶复制到新的桶中。\n替换原哈希表： 在重新哈希和数据迁移完成后，新的哈希表将会取代原有的哈希表，成为新的哈希表来存储元素。\n哈希表扩容的过程是一个相对耗时的操作，因为涉及到重新计算哈希值、数据迁移等操作。为了避免在扩容期间影响正常的哈希表操作，一般情况下，哈希表在扩容时会继续处理新的插入和删除请求，并且新的操作会同时更新原有的哈希表和新的哈希表，直到数据迁移完成，最终将原有哈希表替换为新的哈希表。\n通过扩容，哈希表能够保持较低的哈希冲突率，并且在元素数量增多时仍然能够保持较好的性能。哈希表的扩容是保证哈希表操作效率的一个重要机制。\n在 JDK 1.8 中，HashMap 的扩容机制与之前的版本有所不同，主要是为了解决并发扩容带来的问题。JDK 1.8 对 HashMap 进行了一些优化，引入了红黑树（Red-Black Tree）来优化链表的性能，并对扩容进行了改进。\n红黑树优化： 在 JDK 1.8 中，当哈希表中某个桶（bucket）中的链表长度超过一定阈值（默认为 8）时，该桶中的链表会转换为红黑树，以提高查找、插入和删除的性能。这样可以避免链表过长时，性能退化的问题。\n树化和退树化： 当一个桶中的链表长度达到一定阈值时，链表会被转化为红黑树，这个过程称为树化。当红黑树节点数量变少（小于等于 6）时，树会转化回链表，这个过程称为退树化。通过树化和退树化的操作，HashMap 可以根据元素数量的变化来动态调整数据结构，以提高性能。\n扩容优化： 在 JDK 1.8 中，HashMap 的扩容策略做了改进。在扩容时，原来版本中采用的是一次性将所有元素从旧数组复制到新数组的方式，这可能会导致大量元素在短时间内同时迁移，造成性能抖动和资源竞争。而在 JDK 1.8 中，扩容过程被分成了多个步骤，每次只扩容一小部分，从而减缓了扩容的影响，提高了并发性能。\n具体来说，在 JDK 1.8 中，扩容是通过resize()方法实现的。在进行扩容时，会创建一个新的数组，其大小是原数组的两倍，并将原数组中的元素逐个重新计算哈希值，并放入新数组的对应位置。而并发的写操作会同时在新旧数组中进行，从而避免了全量复制时的竞争和抖动。\n计算哈希值优化： 在 JDK 1.8 中，计算哈希值的方法也进行了优化。在旧版本中，哈希值的计算可能存在碰撞（不同的键计算出相同的哈希值），而在 JDK 1.8 中，采用了更好的哈希算法来减少碰撞的概率，提高了哈希表的性能。\n总体来说，JDK 1.8 对 HashMap 进行了一系列优化，使其在并发情况下表现更好，并且减少了哈希冲突的概率，提高了性能和稳定性。然而，仍然需要根据实际情况合理使用 HashMap，并注意避免负载过高和哈希碰撞等问题。\nHashMap 底层数据结构 JDK 7 中，HashMap 由“数组+链表”组成，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的。\n**在 JDK 8 中，HashMap 由“数组+链表+红黑树”组成。**链表过长，会严重影响 HashMap 的性能，而红黑树搜索的时间复杂度是 O(logn)，而链表是 O(n)。因此，JDK 8 对数据结构做了进一步的优化，引入了红黑树，链表和红黑树在达到一定条件会进行转换：\n当链表超过 8 且数组长度超过 64 时会转红黑树。 将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树，以减少搜索时间。 链表长度超过 8 体现在 putVal 方法中的这段代码：\n//链表长度大于8转换为红黑树进行处理 if (binCount \u0026gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); table 长度为 64 体现在 treeifyBin 方法中的这段代码：\nfinal void treeifyBin(Node\u0026lt;K,V\u0026gt;[] tab, int hash) { int n, index; Node\u0026lt;K,V\u0026gt; e; if (tab == null || (n = tab.length) \u0026lt; MIN_TREEIFY_CAPACITY) resize(); } MIN_TREEIFY_CAPACITY 的值正好为 64。\nstatic final int MIN_TREEIFY_CAPACITY = 64; Hash 方法原理 static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } HashMap 的实现原理是基于哈希表的，它的底层是一个数组，数组的每个位置可能是一个链表或红黑树，也可能只是一个键值对\nHash 方法原理：对 key 的 HashCode 进行处理，得到最终的哈希值\nhash 方法混合了原来 key 哈希值的高位和低位，所以低位的随机性加大了（掺杂了部分高位的特征，高位的信息也得到了保留）。\nhash 方法是用来做哈希值优化的，把哈希值右移 16 位，也就正好是自己长度的一半，之后与原哈希值做异或运算，这样就混合了原哈希值中的高位和低位，增大了随机性。说白了，hash 方法就是为了增加随机性，让数据元素更加均衡的分布，减少 hash 碰撞\n","permalink":"https://lidengxm.github.io/posts/java/hashmap%E8%AF%A6%E8%A7%A3/","summary":"HashMap 基本用法 这篇文章将通过源码的方式，详细透彻地讲清楚 Java 的 HashMap，包括 hash 方法的原理、HashMap 的扩容机制、HashMap 的加载因子为什么是 0.75 而不是 0.6、0.8，以及 HashMap 为什么是线程不安全的，基本上 HashMap 的常见面试题 open in new window，都会在这一篇文章里讲明白。 HashMap 是 Java 中常用的","title":"HashMap详解"},{"content":"集合框架结构 看一下集合框架的结构图\nJava 集合框架分为两条支线：Collection 和 Map\nCollection，主要由 List、Set、Queue 组成\nList 代表有序、可重复的集合，典型代表就是封装了动态数组的 ArrayList 和封装了链表的 LinkedList； Set 代表无序、不可重复的集合，典型代表就是 HashSet 和 TreeSet； Queue 代表队列，典型代表就是双端队列 ArrayDeque，以及优先级队列 PriorityQueue。 Map，代表键值对的集合，典型代表就是 HashMap\nList List 的特点是存取有序，可以存放重复的元素，可以用下标对元素进行操作\nList 集合的实现主要有 ArrayList（封装了数组）和 LinkedList（封装了链表）\nVector 和 Stack\nArrayList 的 API：\n添加元素\nadd()方法 修改元素\nset()方法 删除元素\nremove(int index) 删除指定下标的元素 remove(Object o) 删除指定元素 查询元素\nindexOf(Object o) 正序查询元素 lastIndexOf(Object o) 倒序查询元素 ArrayList（基于数组实现 List） ArrayList 的特点：\nArrayList 是由数组实现的，支持随机存取，也就是可以通过下标直接存取元素； 从尾部插入和删除元素会比较快捷，从中间插入和删除元素会比较低效，因为涉及到数组元素的复制和移动； 如果内部数组的容量不足时会自动扩容，因此当元素非常庞大的时候，效率会比较低。 先来一段 ArrayList 的增删改查：\n//创建ArrayList集合 ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); //新增元素 list.add(\u0026#34;小三\u0026#34;); list.add(\u0026#34;小明\u0026#34;); list.add(\u0026#34;小明\u0026#34;); list.add(\u0026#34;小红\u0026#34;); //遍历集合输出元素 for(String i : list) { System.out.println(i); } //for循环遍历集合 for (int i = 0; i \u0026lt; list.size(); i++) { System.out.println(list.get(i)); } //修改元素，将下标为3的元素修改为小4 list.set(3,\u0026#34;小4\u0026#34;); //删除元素，删除下标为0的元素 list.remove(0); ArrayList 可以称得上是集合框架方面最常用的类了，可以和 HashMap 一较高下\nArrayList 实现了 List 接口，基于数组实现的，数组有个特点，即定义数组的时候就要给数组指定大小。\n一旦创建的时候指定了大小，就不能再调整了。也就是说，如果数组满了，就不能再添加任何元素了。\nArrayList 在数组的基础上实现了自动扩容，并且提供了各种增删改查方法，非常灵活。\n创建 ArrayList（ArrayLIst 初始化）\n//创建了一个以String类型的ArrayList集合 ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); 可以通过上面的语句来创建一个字符串类型的 ArrayList（通过尖括号来限定 ArrayList 中元素的类型，如果尝试\n添加其他类型的元素，将会产生编译错误）\n创建集合的同时 ArrayList 底层调用了无参构造方法\npublic ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } 如果可以，也可以直接在创建 ArrayList 集合的时候给集合指定大小，这样可以避免添加元素时不必要的扩容\n//创建了一个以String类型的大小为20的ArrayList集合 ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(20); 向 ArrayList 集合中添加元素\nlist.add(\u0026#34;xiaoming\u0026#34;); 那 ArrayList 底层是怎么添加元素的呢？这就不得不阅读源码了\n阅读源码之前先看个结论\n堆栈过程图示： add(element) └── if (size == elementData.length) // 判断是否需要扩容 ├── grow(minCapacity) // 扩容 │ └── newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1) // 计算新的数组容量 │ └── Arrays.copyOf(elementData, newCapacity) // 创建新的数组 ├── elementData[size++] = element; // 添加新元素 └── return true; // 添加成功 开始阅读源码了\n首先从 add 方法的源码开始：\n/** * 将指定元素添加到 ArrayList 的末尾 * @param e 要添加的元素 * @return 添加成功返回 true */ public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } 参数 e 是要添加到集合的元素，此时的值为“xiaoming”，size 为 ArrayList 集合的长度，此时为 0\n继续，进入到 ensureCapacityInternal()方法\n/** * 确保 ArrayList 能够容纳指定容量的元素 * @param minCapacity 指定容量的最小值 */ private void ensureCapacityInternal(int minCapacity) { ensureExplicitCapacity(calculateCapacity(elementData, minCapacity)); } 再进入 calculateCapacity 计算容量\n//进入calculateCapacity方法计算容量 private static int calculateCapacity(Object[] elementData, int minCapacity) { // // 如果 elementData 还是默认的空数组 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { // 使用 DEFAULT_CAPACITY 和指定容量的最小值中的较大值 return Math.max(DEFAULT_CAPACITY, minCapacity); } return minCapacity; } 此时：\n参数 minCapacity 为 1（size+1 传过来的） elementData 为存放 ArrayList 元素的底层数组，初始为 0 DEFAULTCAPACITY_EMPTY_ELEMENTDATA 前面也讲过了，为 {} 如果数组长度等于 DEFAULTCAPACITY_EMPTY_ELEMENTDATA\n就比较 DEFAULT_CAPACITY 和 minCapacity 的大小并返回较大的那个，显然，相等，返回 10\nprivate static final int DEFAULT_CAPACITY = 10;//初始为10 进入 ensureExplicitCapacity，传入的参数是上面返回的 10\n//进入到ensureExplicitCapacity判断容量是否足够 //检查并确保集合容量足够，如果需要则增加集合容量 private void ensureExplicitCapacity(int minCapacity) { modCount++;//修改次数 //如果最小容量大于数组范围，就增加容量 if (minCapacity - elementData.length \u0026gt; 0) grow(minCapacity); } 此时：\n参数 minCapacity 为 10 elementData.length 为 0（数组为空） 所以需要增加数组的容量，进入 grow 方法\n/** * 扩容 ArrayList 的方法，确保能够容纳指定容量的元素 * @param minCapacity 指定容量的最小值 */ private void grow(int minCapacity) { // 检查是否会导致溢出，oldCapacity 为当前数组长度 int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1); // 扩容至原来的1.5倍 if (newCapacity - minCapacity \u0026lt; 0) // 如果还是小于指定容量的最小值 newCapacity = minCapacity; // 直接扩容至指定容量的最小值 if (newCapacity - MAX_ARRAY_SIZE \u0026gt; 0) // 如果超出了数组的最大长度 newCapacity = hugeCapacity(minCapacity); // 扩容至数组的最大长度 // 将当前数组复制到一个新数组中，长度为 newCapacity elementData = Arrays.copyOf(elementData, newCapacity); } 此时：\n参数 minCapacity 为 10 变量 oldCapacity 为 0 newCapacity 也是 0，于是 newCapacity - minCapacity 等于 -10 小于 0，newCapacity=10，数组的第一次扩容，扩容容量为 10\n回到 add 方法\npublic boolean add(E e) { ensureCapacityInternal(size + 1); elementData[size++] = e; return true; } 数组的第一个元素，下标为 0。被赋值为“xiaoming”，至此，添加第一个元素分析完毕\n那 ArrayList 第二次扩容会发生在什么时候呢？\n答案是添加第 11 个元素时\n分析：\n添加第 11 个元素时，进入到 calculateCapacity 计算容量，if 语句不成立，返回最小容量 11\n//进入calculateCapacity方法计算容量 private static int calculateCapacity(Object[] elementData, int minCapacity) { // // 如果 elementData 还是默认的空数组 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { // 使用 DEFAULT_CAPACITY 和指定容量的最小值中的较大值 return Math.max(DEFAULT_CAPACITY, minCapacity); } return minCapacity; } 然后进入 ensureExplicitCapacity 方法，判断容量是否足够\n//进入到ensureExplicitCapacity判断容量是否足够 //检查并确保集合容量足够，如果需要则增加集合容量 private void ensureExplicitCapacity(int minCapacity) { modCount++;//修改次数 //如果最小容量大于数组范围，就增加容量 if (minCapacity - elementData.length \u0026gt; 0) grow(minCapacity); } 这时，数组长度仍然为 10，而需要的最小容量是 11，所以需要扩容，进入 grow 方法\nprivate void grow(int minCapacity) { int oldCapacity = elementData.length;//10 int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1); //扩容到15 if (newCapacity - minCapacity \u0026lt; 0) // 15-10 \u0026gt; 0 newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE \u0026gt; 0) //超出了数组的最大长度,很大很大 newCapacity = hugeCapacity(minCapacity); // 扩容至数组的最大长度 // 将当前数组复制到一个新数组中，长度为 newCapacity elementData = Arrays.copyOf(elementData, newCapacity);//扩容成功 } 数组的最大长度很大很大 2147483639，所以不会超过\n扩容数组成 15，ArrayList 的第二次扩容，在添加第 11 个元素时\n向 ArrayList 集合指定位置添加元素\n添加元素是 add(int index, E element)方法\nlist.add(1,\u0026#34;xiao6\u0026#34;); add(int index, E element)方法的源码如下：\n//在指定位置插入一个元素 public void add(int index, E element) { rangeCheckForAdd(index);//检查索引是否越界 ensureCapacityInternal(size + 1);//确保容量足够，不然就扩容 System.arraycopy(elementData, index, elementData, index + 1, size - index);//将Index及其后面的元素向后移动 elementData[index] = element;//将元素插入到指定位置 size++;//元素个数+1 } add(int index, E element)方法会调用到一个非常重要的本地方法 System.arraycopy()，它会对数组进行复制（要插入位置上的元素往后复制）\narraycopy() 的语法：\nSystem.arraycopy(Object src, int srcPos, Object dest, int destPos, int length); 在 ArrayList.add(int index, E element) 方法中，具体用法如下：\nSystem.arraycopy(elementData, index, elementData, index + 1, size - index); elementData：表示要复制的源数组，即 ArrayList 中的元素数组。 index：表示源数组中要复制的起始位置，即需要将 index 及其后面的元素向后移动一位。 elementData：表示要复制到的目标数组，即 ArrayList 中的元素数组。 index + 1：表示目标数组中复制的起始位置，即将 index 及其后面的元素向后移动一位后，应该插入到的位置。 size - index：表示要复制的元素个数，即需要将 index 及其后面的元素向后移动一位，需要移动的元素个数为 size - index。 具体添加图示如下：\n修改 ArrayList 集合中的元素\nArrayList 中可以通过 set 方法更改 ArrayList 中的元素，需要提供下标和新元素值\nlist.set(0,\u0026#34;liming\u0026#34;); 即将下标为 0 的元素修改为 liming\n看一下 set 方法的源码：\n//用指定元素替换指定位置的元素。 public E set(int index, E element) { rangeCheck(index);//检查所以是否越界 E oldValue = elementData(index);//获取index位置上的值 elementData[index] = element;//修改为新的值 return oldValue;//返回在原来位置上的元素 } 该方法会先对指定的下标进行检查，看是否越界，然后替换新值并返回旧值\n删除 ArrayList 中的元素\nremove(int index)方法可以删除指定下标位置上的元素，remove(Object o)用于删除指定值的元素\nlist.remove(0);//删除下标为0的元素 list.remove(\u0026#34;liming\u0026#34;);//删除集合中为\u0026#34;liming\u0026#34;的元素 看一下 remove(int index)的源码\n/** * 删除指定位置的元素。 * * @param index 要删除的元素的索引 * @return 先前在指定位置的元素 * @throws IndexOutOfBoundsException 如果索引超出范围，则抛出此异常 */ public E remove(int index) { rangeCheck(index); // 检查索引是否越界 E oldValue = elementData(index); // 获取要删除的元素 int numMoved = size - index - 1; // 计算需要移动的元素个数 if (numMoved \u0026gt; 0) // 如果需要移动元素，就用 System.arraycopy 方法实现 System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // 将数组末尾的元素置为 null，让 GC 回收该元素占用的空间 return oldValue; // 返回被删除的元素 } 需要注意的是，在 ArrayList 中，删除元素时，需要将删除位置后面的元素向前移动一位，以填补删除位置留\n下的空缺。如果需要移动元素，则需要使用 System.arraycopy 方法将删除位置后面的元素向前移动一位。最后，\n将数组末尾的元素置为 null，以便让垃圾回收机制回收该元素占用的空间。\n再看一下 remove(Object o)方法的源码\n/** * 删除列表中第一次出现的指定元素（如果存在）。 * * @param o 要删除的元素 * @return 如果列表包含指定元素，则返回 true；否则返回 false */ public boolean remove(Object o) { if (o == null) { // 如果要删除的元素是 null for (int index = 0; index \u0026lt; size; index++) // 遍历列表 if (elementData[index] == null) { // 如果找到了 null 元素 fastRemove(index); // 调用 fastRemove 方法快速删除元素 return true; // 返回 true，表示成功删除元素 } } else { // 如果要删除的元素不是 null for (int index = 0; index \u0026lt; size; index++) // 遍历列表 if (o.equals(elementData[index])) { // 如果找到了要删除的元素 fastRemove(index); // 调用 fastRemove 方法快速删除元素 return true; // 返回 true，表示成功删除元素 } } return false; // 如果找不到要删除的元素，则返回 false } 该方法通过遍历的方式找到要删除的元素，null 的时候使用 == 操作符判断，非 null 的时候使用 equals() 方法，然后调用 fastRemove() 方法。\n注意：\n有相同元素时，只会删除第一个 判断两个元素是否相等，可以参考Java 如何判断两个字符串是否相等 看一下 fastRemove 方法的源码：\n/** * 快速删除指定位置的元素。 * * @param index 要删除的元素的索引 */ private void fastRemove(int index) { int numMoved = size - index - 1; // 计算需要移动的元素个数 if (numMoved \u0026gt; 0) // 如果需要移动元素，就用 System.arraycopy 方法实现 System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // 将数组末尾的元素置为 null，让 GC 回收该元素占用的空间 } 同样是调用 System.arraycopy() 方法对数组进行复制和移动。\n删除元素时 ArrayList 集合的图示：\n查找 ArrayList 集合中的元素\n如果要正序查找一个元素，可以使用 indexOf() 方法；如果要倒序查找一个元素，可以使用 lastIndexOf() 方法。\nlist.indexOf(\u0026#34;liming\u0026#34;);//从集合第一个元素开始寻找\u0026#34;liming\u0026#34; list.lastIndexOf(\u0026#34;liming\u0026#34;);//从集合最后一个元素开始寻找\u0026#34;liming\u0026#34; 看看 indexOf 方法的源码：\n/** * 返回指定元素在列表中第一次出现的位置。 * 如果列表不包含该元素，则返回 -1。 * * @param o 要查找的元素 * @return 指定元素在列表中第一次出现的位置；如果列表不包含该元素，则返回 -1 */ public int indexOf(Object o) { if (o == null) { // 如果要查找的元素是 null for (int i = 0; i \u0026lt; size; i++) // 遍历列表 if (elementData[i]==null) // 如果找到了 null 元素 return i; // 返回元素的索引 } else { // 如果要查找的元素不是 null for (int i = 0; i \u0026lt; size; i++) // 遍历列表 if (o.equals(elementData[i])) // 如果找到了要查找的元素 return i; // 返回元素的索引 } return -1; // 如果找不到要查找的元素，则返回 -1 } 如果元素为 null 的时候使用“==”操作符，否则使用 equals() 方法。\nlastIndexOf() 方法和 indexOf() 方法类似，不过遍历的时候从最后一个元素开始\n看看 lastIndexOf 方法的源码：\n/** * 返回指定元素在列表中最后一次出现的位置。 * 如果列表不包含该元素，则返回 -1。 * * @param o 要查找的元素 * @return 指定元素在列表中最后一次出现的位置；如果列表不包含该元素，则返回 -1 */ public int lastIndexOf(Object o) { if (o == null) { // 如果要查找的元素是 null for (int i = size-1; i \u0026gt;= 0; i--) // 从后往前遍历列表 if (elementData[i]==null) // 如果找到了 null 元素 return i; // 返回元素的索引 } else { // 如果要查找的元素不是 null for (int i = size-1; i \u0026gt;= 0; i--) // 从后往前遍历列表 if (o.equals(elementData[i])) // 如果找到了要查找的元素 return i; // 返回元素的索引 } return -1; // 如果找不到要查找的元素，则返回 -1 } contains() 方法可以判断 ArrayList 中是否包含某个元素，其内部就是通过 indexOf() 方法实现的：\npublic boolean contains(Object o) { return indexOf(o) \u0026gt;= 0; } 如果不包含某个元素即查找不到这个元素，indexOf 返回-1，contains 方法返回 false\n二分查找法\n如果 ArrayList 中的元素是经过排序的，就可以使用二分查找法，效率更快。\nCollections 类的 sort() 方法可以对 ArrayList 进行排序，该方法会按照字母顺序对 String 类型的列表进行排序。如果是自定义类型的列表，还可以指定 Comparator 进行排序。\nList\u0026lt;String\u0026gt; copy = new ArrayList\u0026lt;\u0026gt;(alist); copy.add(\u0026#34;a\u0026#34;); copy.add(\u0026#34;c\u0026#34;); copy.add(\u0026#34;b\u0026#34;); copy.add(\u0026#34;d\u0026#34;); Collections.sort(copy);//对copy集合进行正序排序 System.out.println(copy); 输出结果：[a, b, c, d]\n排序之后就可以使用二分查找法了\nint index = Collections.binarySearch(copy, \u0026#34;b\u0026#34;); 二分查找算法更快，效率更高，数组专题的算法中就有二分查找\nArrayList 增删改查的时间复杂度\n总结一下 ArrayList 的时间复杂度吧，方便后面学习 LinkedList 时对比\n查询：O（1）\nArrayList 内部使用数组来存储元素，所以可以直接根据索引来访问元素。\n插入元素：O（1）or O（n）\n为什么插入时由两种时间啊复杂度？\n数组是顺序存储结构，只能从尾部插入元素，所以\n如果要在列表末尾添加元素，时间复杂度为 O（1） 如果要在列表的中间或开头插入元素，则需要将插入位置之后的元素全部向后移动一位，时间复杂度为 O(n)。 删除元素：O（1）or O（n）\n删除元素同样有两种情况的时间复杂度\n如果要删除列表末尾的元素，时间复杂度为 O(1)。 如果要删除列表中间或开头的元素，则需要将删除位置之后的元素全部向前移动一位，时间复杂度为 O(n)。 修改：O（1）\n修改一个元素（调用 set()方法时）与查询操作类似，可以直接根据索引来访问元素，时间复杂度为 O(1)。\nArrayList 总结：\nArrayList 也可以叫动态数组，也就是创建的 ArrayList 集合可以调整大小，即扩容机制，扩容因子为 0.5 ArrayList 的源码主要是掌握扩容机制，即 \u0026raquo; 右移操作符，二进制移位运算 LinkedList（基于链表实现 List） LinkedList 是基于链表实现的 List 集合\nLinkedList 的 API：\n添加元素：\nadd(E element): 将元素添加到链表末尾。 addFirst(E element): 将元素添加到链表头部。 addLast(E element): 将元素添加到链表末尾。 获取元素：\nget()方法可以获取指定位置的元素 getFirst() 方法用于获取第一个元素； getLast() 方法用于获取最后一个元素； 删除元素：\nremove()方法移除并返回链表的第一个元素 remove(int index)方法移除指定索引位置处的元素 removeFirst()方法移除并返回链表的第一个元素 removeLast()方法移除并返回链表的最后一个元素 链表大小和清空：\nsize(): 返回链表中元素的数量。 isEmpty(): 检查链表是否为空。 clear(): 清空链表，移除所有元素。 包含元素：\ncontains(Object o): 检查链表是否包含指定的元素。 转换为数组：\ntoArray(): 将链表转换为数组。 迭代器：\niterator(): 返回一个迭代器，用于遍历链表中的元素。 ArrayList 的缺陷：\n基于数组实现，扩容时复制原数组，如果存储很大的数据量会很耗时间 链表的三种等级\n单链表，只有一个后指针，即 next，指向下一个数据 双链表，一个前指针 prev，一个后指针 next，前指针指向前一个元素，后指针指向后一个元素 二叉树，后指针去掉，换成左右指针 LinkedList 中的一个重要的节点 Node\n看一下 Node 类的源码：\n/** * 链表中的节点类。 */ private static class Node\u0026lt;E\u0026gt; { E item; // 节点中存储的元素 Node\u0026lt;E\u0026gt; next; // 指向下一个节点的指针 Node\u0026lt;E\u0026gt; prev; // 指向上一个节点的指针 /** * 构造一个新的节点。 * * @param prev 前一个节点 * @param element 节点中要存储的元素 * @param next 后一个节点 */ Node(Node\u0026lt;E\u0026gt; prev, E element, Node\u0026lt;E\u0026gt; next) { this.item = element; // 存储元素 this.next = next; // 设置下一个节点 this.prev = prev; // 设置上一个节点 } } 节点类由三部分组成：\n节点上的元素 下一个节点 上一个节点 对于第一个节点来说，prev 为 null； 对于最后一个节点来说，next 为 null； 其余的节点呢，prev 指向前一个，next 指向后一个。 LinkedList 的增删改查分析\n操作 LinkedList 之前要先初始化\nLinkedList\u0026lt;String\u0026gt; list = new LinkedList\u0026lt;\u0026gt;(); ArrayList 初始化的时候可以指定大小，也可以不指定，等到添加第一个元素的时候进行第一次扩容。而 LinkedList，没有大小，只要内存够大，就可以无穷大\n新增元素\nlist.add(\u0026#34;张三\u0026#34;); list.add(\u0026#34;王五\u0026#34;); list.add(\u0026#34;李四\u0026#34;); 看一下 add 方法内部：其实是调用的 linkLast 方法：\n/** * 将指定的元素添加到列表的尾部。 * * @param e 要添加到列表的元素 * @return 始终为 true（根据 Java 集合框架规范） */ public boolean add(E e) { linkLast(e); // 在列表的尾部添加元素 return true; // 添加元素成功，返回 true } linkLast 即就是在链表的尾部插入元素\n看一下 linkLast 方法的源码：\n/** * 在列表的尾部添加指定的元素。 * * @param e 要添加到列表的元素 */ void linkLast(E e) { final Node\u0026lt;E\u0026gt; l = last; // 获取链表的最后一个节点 // 创建一个新的节点，并将其设置为链表的最后一个节点 final Node\u0026lt;E\u0026gt; newNode = new Node\u0026lt;\u0026gt;(l, e, null); last = newNode; // 将新的节点设置为链表的最后一个节点 if (l == null) // 如果链表为空，则将新节点设置为头节点 first = newNode; else l.next = newNode; // 否则将新节点链接到链表的尾部 size++; // 增加链表的元素个数 } 空链表添加第一个元素的时候，first 和 last 都为 null。 然后新建一个节点 newNode，它的 prev 和 next 也为 null。 然后把 last 和 first 都赋值为 newNode。 此时还不能称之为链表，因为前后节点都是断裂的。\n添加第二个元素的时候，first 和 last 都指向的是第一个节点。 然后新建一个节点 newNode，它的 prev 指向的是第一个节点，next 为 null。 然后把第一个节点的 next 赋值为 newNode。 此时的链表还不完整\n添加第三个元素的时候，first 指向的是第一个节点，last 指向的是最后一个节点。 然后新建一个节点 newNode，它的 prev 指向的是第二个节点，next 为 null。 然后把第二个节点的 next 赋值为 newNode。 此时的链表已经完整了。\nLinkedList 的插入元素还有两个方法：\naddFirst()：将元素插入到链表头部 addLast()：将元素插入到链表尾部 看一下 addFirst 方法的源码：\npublic void addFirst(E e) { linkFirst(e); // 在列表的开头添加元素 } 进入到 linkFirst 方法，将新的元素链到链表的头部\nprivate void linkFirst(E e) { final Node\u0026lt;E\u0026gt; f = first; // 获取链表的第一个节点 final Node\u0026lt;E\u0026gt; newNode = new Node\u0026lt;\u0026gt;(null, e, f); // 创建一个新的节点，并将其设置为链表的第一个节点 first = newNode; // 将新的节点设置为链表的第一个节点 if (f == null) // 如果链表为空，则将新节点设置为尾节点 last = newNode; else f.prev = newNode; // 否则将新节点链接到链表的头部 size++; // 增加链表的元素个数 } addLast 方法与 addFirst 方法类似，内部也是调用的 linkLast 方法\nLinkedList 删除元素\nLinkedList 的删除方法有多个：\nremove()：删除第一个节点 remove(int)：删除指定位置的节点 remove(Object)：删除指定元素的节点 removeFirst()：删除第一个节点 removeLast()：删除最后一个节点 remove 方法调用的是 removeFirst 方法\n看一下 remove(int index)方法的源码\npublic E remove(int index) { checkElementIndex(index); // 检查索引是否越界 return unlink(node(index)); // 删除指定位置的节点，并返回节点的元素 } remove 方法内部是先判断要移除的元素下标是否越界，然后调用 unllink 方法从链表移除‘\n看一下 unlink 方法的源码：\n//return 从链表中删除的节点的元素 E unlink(Node\u0026lt;E\u0026gt; x) { final E element = x.item; // 获取要删除节点的元素 final Node\u0026lt;E\u0026gt; next = x.next; // 获取要删除节点的下一个节点 final Node\u0026lt;E\u0026gt; prev = x.prev; // 获取要删除节点的上一个节点 if (prev == null) { // 如果要删除节点是第一个节点 first = next; // 将链表的头节点设置为要删除节点的下一个节点 } else { prev.next = next; // 将要删除节点的上一个节点指向要删除节点的下一个节点 x.prev = null; // 将要删除节点的上一个节点设置为空 } if (next == null) { // 如果要删除节点是最后一个节点 last = prev; // 将链表的尾节点设置为要删除节点的上一个节点 } else { next.prev = prev; // 将要删除节点的下一个节点指向要删除节点的上一个节点 x.next = null; // 将要删除节点的下一个节点设置为空 } x.item = null; // 将要删除节点的元素设置为空 size--; // 减少链表的元素个数 return element; // 返回被删除节点的元素 } unlink 方法其实就是更新当前节点的 next 和 prev 指针，并将要删除的节点元素置为 null\n再看一下 remove(Object o)，删除指定元素的方法源码：\n//从链表中删除指定元素。 public boolean remove(Object o) { if (o == null) { // 如果要删除的元素为 null for (Node\u0026lt;E\u0026gt; x = first; x != null; x = x.next) { // 遍历链表 if (x.item == null) { // 如果节点的元素为 null unlink(x); // 删除节点 return true; // 返回 true 表示删除成功 } } } else { // 如果要删除的元素不为 null for (Node\u0026lt;E\u0026gt; x = first; x != null; x = x.next) { // 遍历链表 if (o.equals(x.item)) { // 如果节点的元素等于要删除的元素 unlink(x); // 删除节点 return true; // 返回 true 表示删除成功 } } } return false; // 如果链表中不包含要删除的元素，则返回 false 表示删除失败 } remove(Object o)方法内部也调用了 unlink 方法，只不过先找到要删除元素所在的节点\n元素为 null 的时候，必须使用 == 来判断；元素为非 null 的时候，要使用 equals 来判断。\nremoveFirst 方法内部调用的是 unlinkFirst 方法：\n//从链表中删除第一个元素并返回它。 public E removeFirst() { final Node\u0026lt;E\u0026gt; f = first; // 获取链表的第一个节点 if (f == null) // 如果链表为空 throw new NoSuchElementException(); // 抛出 NoSuchElementException 异常 return unlinkFirst(f); // 调用 unlinkFirst 方法删除第一个节点并返回它的元素 } 看一下 unlinkFirst 方法源码：\n//删除链表中的第一个节点并返回它的元素 private E unlinkFirst(Node\u0026lt;E\u0026gt; f) { final E element = f.item; // 获取要删除的节点的元素 final Node\u0026lt;E\u0026gt; next = f.next; // 获取要删除的节点的下一个节点 f.item = null; // 将要删除的节点的元素设置为 null f.next = null; // 将要删除的节点的下一个节点设置为 null first = next; // 将链表的头节点设置为要删除的节点的下一个节点 if (next == null) // 如果链表只有一个节点 last = null; // 将链表的尾节点设置为 null else next.prev = null; // 将要删除节点的下一个节点设为头结点 size--; // 减少链表的大小 return element; // 返回被删除节点的元素 } removeLast 方法与 removeFirst 方法类似，作用是删除链表的最后一个节点\nLinkedList 修改元素\nLinkedList 删除元素也是调用的 set 方法\n//修改元素 list.set(0,\u0026#34;lili\u0026#34;); LinkedList 修改元素的值也是用 set 方法，传入要修改元素的下标和修改后的值\n看一下 set 方法源码：\n/** * 将链表中指定位置的元素替换为指定元素，并返回原来的元素。 * * @param index 要替换元素的位置（从 0 开始） * @param element 要插入的元素 * @return 替换前的元素 * @throws IndexOutOfBoundsException 如果索引超出范围（index \u0026lt; 0 || index \u0026gt;= size()） */ public E set(int index, E element) { checkElementIndex(index); // 检查索引是否超出范围 Node\u0026lt;E\u0026gt; x = node(index); // 获取要替换的节点 E oldVal = x.item; // 获取要替换节点的元素 x.item = element; // 将要替换的节点的元素设置为指定元素 return oldVal; // 返回替换前的元素 } 先看一下 node 方法的源码：\n/** * 获取链表中指定位置的节点。 * * @param index 节点的位置（从 0 开始） * @return 指定位置的节点 * @throws IndexOutOfBoundsException 如果索引超出范围（index \u0026lt; 0 || index \u0026gt;= size()） */ Node\u0026lt;E\u0026gt; node(int index) { if (index \u0026lt; (size \u0026gt;\u0026gt; 1)) { // 如果索引在链表的前半部分 Node\u0026lt;E\u0026gt; x = first; for (int i = 0; i \u0026lt; index; i++) // 从头节点开始向后遍历链表，直到找到指定位置的节点 x = x.next; return x; // 返回指定位置的节点 } else { // 如果索引在链表的后半部分 Node\u0026lt;E\u0026gt; x = last; for (int i = size - 1; i \u0026gt; index; i--) // 从尾节点开始向前遍历链表，直到找到指定位置的节点 x = x.prev; return x; // 返回指定位置的节点 } } size \u0026raquo; 1：右移一位，相当于除以 2，即 0.5size\nnode 方法会对下标进行一个初步判断，如果靠近前半截，就从下标 0 开始遍历；如果靠近后半截，就从末尾\n开始遍历，这样可以提高效率，最大能提高一半的效率。\n找到要替换元素的节点就 OK 了，直接将节点的元素替换成新的节点就 OK 了\nLinkedList 的查询\nLinkedList 查询元素的方法有两种：\nindexOf(Objext o)：查找指定元素所在的位置 get(int i)：查找指定位置上的元素 看一下 indexOf 方法的源码：\n/** * 返回链表中首次出现指定元素的位置，如果不存在该元素则返回 -1。 * * @param o 要查找的元素 * @return 首次出现指定元素的位置，如果不存在该元素则返回 -1 */ public int indexOf(Object o) { int index = 0; // 初始化索引为 0 if (o == null) { // 如果要查找的元素为 null for (Node\u0026lt;E\u0026gt; x = first; x != null; x = x.next) { // 从头节点开始向后遍历链表 if (x.item == null) // 如果找到了要查找的元素 return index; // 返回该元素的索引 index++; // 索引加 1 } } else { // 如果要查找的元素不为 null for (Node\u0026lt;E\u0026gt; x = first; x != null; x = x.next) { // 从头节点开始向后遍历链表 if (o.equals(x.item)) // 如果找到了要查找的元素 return index; // 返回该元素的索引 index++; // 索引加 1 } } return -1; // 如果没有找到要查找的元素，则返回 -1 } get 方法内部还是 node 方法\npublic E get(int index) { checkElementIndex(index); return node(index).item; } LinkedList 的其他查询方法：\ngetFirst() 方法用于获取第一个元素； getLast() 方法用于获取最后一个元素； poll() 和 pollFirst() 方法用于删除并返回第一个元素（两个方法尽管名字不同，但方法体是完全相同的）； pollLast() 方法用于删除并返回最后一个元素； peekFirst() 方法用于返回但不删除第一个元素。 ArrayList VS LinkedList ArrayList 对于 ArrayList\nArrayList 实现了 List 接口，继承了 AbstractList 抽象类。\nArrayList 是基于数组实现的，并且实现了动态扩容（当需要添加新元素时，如果 elementData 数组已满，则会自动扩容，新的容量将是原来的 1.5 倍）\nArrayList 实现了 RandomAccess 接口，这是一个标记接口：\npublic interface RandomAccess { } 内部是空的。标记“实现了这个接口的类支持快速（通常是固定时间）随机访问”。快速随机访问是什么意思呢？就是说不需要遍历，就可以通过下标（索引）直接访问到内存地址。而 LinkedList 没有实现该接口，表示它不支持高效的随机访问，需要通过遍历来访问元素。\nArrayList 实现了 Cloneable 接口，这表明 ArrayList 是支持拷贝 open in new window的。ArrayList 内部的确也重写了 Object 类的 clone() 方法。\nArrayList 还实现了 Serializable 接口，同样是一个标记接口：\npublic interface Serializable { } 内部也是空的，标记“实现了这个接口的类支持序列化”。序列化是什么意思呢？Java 的序列化是指，将对象转换成以字节序列的形式来表示，这些字节序中包含了对象的字段和方法。序列化后的对象可以被写到数据库、写到文件，也可用于网络传输。\nArrayList 中的关键字段 elementData 使用了 transient 关键字修饰，这个关键字的作用是，让它修饰的字段不被序列化。\nLinkedList 对于 LinkedList\nLinkedList 是一个继承自 AbstractSequentialList 的双向链表，因此它也可以被当作堆栈、队列或双端队列进行操作。是基于链表的 List 实现\nLinkedList 的部分源码：\npublic class LinkedList\u0026lt;E\u0026gt; extends AbstractSequentialList\u0026lt;E\u0026gt; implements List\u0026lt;E\u0026gt;, Deque\u0026lt;E\u0026gt;, Cloneable, java.io.Serializable { transient int size = 0; // 非序列化字段，表示链表中的节点个数 transient Node\u0026lt;E\u0026gt; first; // 非序列化字段，指向链表中的第一个节点 transient Node\u0026lt;E\u0026gt; last; // 非序列化字段，指向链表中的最后一个节点 // ... } LinkedList 实现了 Cloneable 接口，这表明 LinkedList 是支持拷贝的。\nLinkedList 还实现了 Serializable 接口，这表明 LinkedList 是支持序列化的。但 LinkedList 中的关键字段 size、first、last 都使用了 transient 关键字修饰，这是因为 LinkedList 想用自己的方式进行序列化\nLinkedList 在序列化的时候只保留了元素的内容 item，并没有保留元素的前后引用。这样就节省了不少内存空间；在反序列化时又把链表重新链接起来，这样就恢复了链表序列化之前的顺序\n新增元素时 ArrayList 和 LinkedList 谁更快？\nArrayList 在添加元素的时候如果不涉及到扩容，性能在两种情况下（中间位置新增元素、尾部新增元素）比 LinkedList 好很多，只有头部新增元素的时候比 LinkedList 差，因为数组复制的原因。\n遍历元素 ArrayList 和 LinkedList 谁更快？\n首先我们知道，集合遍历通常有两种做法：一种是使用 for 循环，另一种是使用迭代器（Iterator）\nfor 循环遍历元素时，LinkedList 每一次都要调用底层的 node 方法进行遍历，而 ArrayList 是根据索引找元素，很快 迭代器遍历元素时，LinkedList 与 ArrayList 遍历的速度就差不多了 总结：\nfor 循环遍历的时候，ArrayList 花费的时间远小于 LinkedList；迭代器遍历的时候，两者性能差不多\n所以 遍历 LinkedList 的时候，千万不要使用 for 循环，要使用迭代器\nLinkedList 可用于实现 LRU（Least Recently Used）缓存淘汰算法，LRU 缓存淘汰算法是一种常用的缓存淘汰策略，它的基本思想是，当缓存空间不够时，优先淘汰最近最少使用的缓存数据。\n使用 LinkedList 来存储缓存数据，每次访问缓存数据时，将该数据从链表中删除并移动到链表的头部，这样链表的尾部就是最近最少使用的缓存数据，当缓存空间不够时，只需要将链表尾部的缓存数据淘汰即可。\nArrayList 和 LinkedList 都是 Java 集合框架中的 List 接口的实现类，都可以用来存储一组有序的元素。不过两者的底层实现方式不同，它们各有优缺点，适用于不同的场景。\nArrayList 与 LinkedList 的区别 \\1. 底层数据结构：\nArrayList 是基于数组实现的，插入/删除元素时，因为需要移动后续元素的位置，所以在元素量很大时，性能会较差。 **LinkedList 是基于链表实现的，插入/删除元素时，只需要改变相邻元素的指针指向，不需要移动其他元素，所以在插入/删除元素时性能较好。**但是，链表不支持随机访问，因为每个节点只有指向前驱和后继节点的指针，访问中间元素的时间复杂度为 O(n)，性能较 ArrayList 差。 \\2. 访问元素的效率：\nArrayList 的元素是放置在一个连续的内存空间中，因此访问元素时更加高效，时间复杂度为 O(1) LinkedList 需要从头结点或尾结点开始遍历至指定位置，时间复杂度为 O(n)。 \\3. 内存使用情况：\nArrayList 在添加元素时，如果数组容量不足，需要重新分配一个更大的内存空间，并将原内存中的元素复制到新的内存空间中。内存占用较大 LinkedList 添加元素时不会出现空间浪费问题，但是因为每个元素都需要保存节点的指针，所以在小元素量的情况下，LinkedList 的内存会更加浪费。 \\4. 线程安全性：\nArrayList 不是线程安全的，在多线程同时进行修改操作时很容易导致问题。 LinkedList 同样不是线程安全的，但由于它对元素的操作不需要同时修改节点的指针，因此在多线程修改时可能会比 ArrayList 更安全。 缓存友好型 由于 ArrayList 是基于数组，它在内存中是连续存储的，这有助于利用 CPU 缓存机制，从而提高访问性能。相比之下，LinkedList 中的元素在内存中是分散存储的，这可能导致缓存未命中，对于大量元素的访问，ArrayList 可能更具优势。 使用场景：\n如果需要高效地对元素进行随机访问或数组存储的特性，那么应该使用 ArrayList； 如果需要对元素进行频繁的操作（添加、删除），并且不需要随机访问的特性，那么应该使用 LinkedList。 使用场景：\n**需要频繁随机访问元素的时候，例如读取大量数据并进行处理或者需要对数据进行排序或查找的场景，可以使用 ArrayList。**例如一个学生管理系统，需要对学生列表进行排序或查找操作，可以使用 ArrayList 存储学生信息，以便快速访问和处理。\n**当需要频繁插入和删除元素的时候，例如实现队列或栈，或者需要在中间插入或删除元素的场景，可以使用 LinkedList。**例如一个实时聊天系统，需要实现一个消息队列，可以使用 LinkedList 存储消息，以便快速插入和删除消息。\n在一些特殊场景下，可能需要同时支持随机访问和插入/删除操作。例如一个在线游戏系统，需要实现一个玩家列表，需要支持快速查找和遍历玩家，同时也需要支持玩家的加入和离开。在这种情况下，可以使用 LinkedList 和 ArrayList 的组合，例如使用 LinkedList 存储玩家，以便快速插入和删除玩家，同时使用 ArrayList 存储玩家列表，以便快速查找和遍历玩家。\n面试题：ArrayList 和 LinkedList 的区别？\nArrayLists 底层是基于数组，而 LinkedList 底层是基于链表，ArrayList 在插入/删除元素时效率不如 LinkedList 高效，当需要在链表中的任意位置进行插入和删除操作时，LinkedList 相较于 ArrayList 通常更具优势，因为它不需要移动其他元素。 访问元素时，ArrayList 可以随机访问元素，时间复杂度 O（1），LinkedList 需要遍历找到目标元素，时间复杂度 O（n） 内存使用方面，ArrayList 是动态数组，根据存储元素的大小进行适当的扩容，而 LinkedList 不需要扩容，但是需要保存节点的指针，在小数据量时 ArrayList 占用内存较少 线程安全方面，ArrayList 和 LiknedList 都是线程不安全的，但由于它对元素的操作不需要同时修改节点的指针，因此在多线程修改时可能会比 ArrayList 更安全。 缓存友好型 由于 ArrayList 是基于数组，它在内存中是连续存储的，这有助于利用 CPU 缓存机制，从而提高访问性能。相比之下，LinkedList 中的元素在内存中是分散存储的，这可能导致缓存未命中，对于大量元素的访问，ArrayList 可能更具优势。 补充：ArrayList 和 LinkedList 的使用场景：\n**需要频繁随机访问元素的时候，例如读取大量数据并进行处理或者需要对数据进行排序或查找的场景，可以使用 ArrayList。**例如一个学生管理系统，需要对学生列表进行排序或查找操作，可以使用 ArrayList 存储学生信息，以便快速访问和处理。\n**当需要频繁插入和删除元素的时候，例如实现队列或栈，或者需要在中间插入或删除元素的场景，可以使用 LinkedList。**例如一个实时聊天系统，需要实现一个消息队列，可以使用 LinkedList 存储消息，以便快速插入和删除消息。\n泛型 泛型，即“参数化类型”，解决不确定对象具体类型的问题。在编译阶段有效。\n泛型的优秀之处：\n使用类型参数解决了元素的不确定性——参数类型为 String 的集合中是不允许存放其他类型元素的，取出数据的时候也不需要强制类型转换了\n在 Java 中，泛型是一种强类型约束机制，可以在编译期间检查类型安全性，并且可以提高代码的复用性和可读性。\n泛型的特性：\n1）类型参数化\n泛型的本质是参数化类型，也就是说，在定义类，接口或方法时，可以使用一个或多个类型参数来表示参数化类型\n泛型在实际开发中的应用非常广泛，例如集合框架中的 List、Set、Map 等容器类，以及并发框架中的 Future、Callable 等工具类都使用了泛型。\n2）类型擦除\n在 Java 的泛型机制中，有两个重要的概念：类型擦除和通配符\n泛型在编译时会将泛型类型擦除，将泛型类型替换成 Object 类型。这是为了向后兼容，避免对原有的 Java 代码造成影响。\n对于下面代码：\nList\u0026lt;Integer\u0026gt; intList = new ArrayList\u0026lt;\u0026gt;(); intList.add(123); int value = intList.get(0); 在编译时，Java 编译器会将泛型类型 List 替换成 List，将 get 方法的返回值类型 Integer 替换成 Object，生成的字节码与下面的代码等价：\nList intList = new ArrayList(); intList.add(Integer.valueOf(123)); int value = (Integer) intList.get(0); Java 泛型只会在编译时起作用，运行时并不会保留泛型类信息\n3）通配符\n通配符用于表示某种未知的类型，例如 List\u003c?\u003e 表示一个可以存储任何类型对象的 List，但是不能对其中的元素进行添加操作。通配符可以用来解决类型不确定的情况，例如在方法参数或返回值中使用。\n例如定义一个泛型方法：\npublic static void printList(List\u0026lt;?\u0026gt; list) { for (Object obj : list) { System.out.print(obj + \u0026#34; \u0026#34;); } System.out.println(); } 这个方法可以接受任意类型的 List，例如 List、List 等等。\n上限通配符\n泛型还提供了上限通配符 \u0026lt;? extends T\u0026gt;，表示通配符只能接受 T 或 T 的子类。使用上限通配符可以提高程序的类型安全性。\n例如定义了一个方法，只接受 Number 类及子类的 List\npublic static void printNumberList(List\u0026lt;? extends Number\u0026gt; list) { for (Number num : list) { System.out.print(num + \u0026#34; \u0026#34;); } System.out.println(); } 这个方法可以接受 List、List 等等。\n下限通配符\n下限通配符（Lower Bounded Wildcards）用 super 关键字来声明，其语法形式为 \u0026lt;? super T\u0026gt;，其中 T 表示类型参数。它表示的是该类型参数必须是某个指定类的超类（包括该类本身）。\n当我们需要往一个泛型集合中添加元素时，如果使用的是上限通配符，集合中的元素类型可能会被限制，从而无法添加某些类型的元素。但是，如果我们使用下限通配符，可以将指定类型的子类型添加到集合中，保证了元素的完整性。\n举个例子，假设有一个类 Animal，以及两个子类 Dog 和 Cat。现在我们有一个 List\u0026lt;? super Dog\u0026gt; 集合，它的类型参数必须是 Dog 或其父类类型。我们可以向该集合中添加 Dog 类型的元素，也可以添加它的子类。但是，不能向其中添加 Cat 类型的元素，因为 Cat 不是 Dog 的子类。\n下面是一个使用下限通配符的示例：\nList\u0026lt;? super Dog\u0026gt; animals = new ArrayList\u0026lt;\u0026gt;(); // 可以添加 Dog 类型的元素和其子类型元素 animals.add(new Dog()); animals.add(new Bulldog()); // 不能添加 Cat 类型的元素 animals.add(new Cat()); // 编译报错 需要注意的是，虽然使用下限通配符可以添加某些子类型元素，但是在读取元素时，我们只能确保其是 Object 类型的，无法确保其是指定类型或其父类型。因此，在读取元素时需要进行类型转换，如下所示：\nList\u0026lt;? super Dog\u0026gt; animals = new ArrayList\u0026lt;\u0026gt;(); animals.add(new Dog()); // 读取元素时需要进行类型转换 Object animal = animals.get(0); Dog dog = (Dog) animal; 总的来说，Java 的泛型机制是一种非常强大的类型约束机制，可以在编译时检查类型安全性，并提高代码的复用性和可读性。但是，在使用泛型时也需要注意类型擦除和通配符等问题，以确保代码的正确性。\n","permalink":"https://lidengxm.github.io/posts/java/arraylist%E4%B8%8Elinkedlist%E4%B8%80%E7%BD%91%E6%89%93%E9%80%9A/","summary":"集合框架结构 看一下集合框架的结构图 Java 集合框架分为两条支线：Collection 和 Map Collection，主要由 List、Set、Queue 组成 List 代表有序、可重复的集合，典型代表就是封装了动态数组的 ArrayList 和封装了链表的 LinkedList； Set 代表无序、不可重复的集合，典型代表就是 HashSet 和 T","title":"ArrayList与LinkedList一网打通"},{"content":"线程与进程篇 线程和进程的区别？ 在操作系统中，进程是指一个正在执行中的程序，而线程是进程的一部分，是一个程序中执行的代码片段。\n进程是操作系统资源分配的最小单位，一个进程至少包括一个线程，进程拥有自己的内存空间、文件句柄、环境变量等系统资源。进程间相互独立，互不干扰，每个进程都拥有自己的地址空间。进程通信需要通过进程间通信机制（IPC）来实现。\n线程是程序执行的最小单位，一个进程中可以包含多个线程，它们共享进程的内存空间和系统资源。多个线程可以并发执行，从而提高了程序的运行效率，同时也会带来线程安全等问题。线程之间的通信可以通过共享内存、信号量等机制实现。\n进程和线程的区别和联系如下：\n资源分配：进程拥有自己的内存空间等系统资源，而线程共享进程的资源； 独立性：进程之间相互独立，互不干扰，而线程是进程的一部分，线程之间共享进程的资源； 调度：进程间调度的开销比线程大，线程的调度开销小，可以并发执行； 并发性：多个进程之间相互独立，多个线程可以并发执行； 同步：进程间通信需要通过 IPC 机制，线程间同步可以通过共享内存、信号量等机制实现。 进程：\n进程是操作系统中的一个执行单元， 它拥有自己的地址空间、文件描述符、环境变量等资源。每个进程都是独立的，它们之间不能直接共享内存，需要通过进程间通信（IPC）的方式来进行数据交换。 进程是操作系统中资源分配的基本单位， 每个进程都有自己的进程控制块（PCB），用于记录进程的状态、优先级、资源占用情况等信息。 进程之间的切换需要保存和恢复进程的上下文，因此进程切换的开销比较大。 线程：\n线程是进程中的一个执行单元，它与同一进程中的其他线程共享进程的地址空间和资源。 线程之间可以直接访问共享内存，因此线程间通信比进程间通信更加高效。 线程是操作系统中调度的基本单位， 每个线程都有自己的线程控制块（TCB），用于记录线程的状态、优先级、资源占用情况等信息。 线程之间的切换开销比进程切换小得多，因为它们共享进程的地址空间和资源。 协程（Coroutine）：\n协程是一种更轻量级的并发机制，与线程和进程不同，协程是由程序控制的，而不是由操作系统调度的。 协程可以在单个线程内切换执行，因此切换开销很小，适用于高并发任务。 协程之间的切换需要程序显式地进行，通常在某些特定的点显式地进行暂停和恢复。 协程常用于处理 I/O 密集型任务，如网络请求和文件读写。 进程 线程 协程 单位 操作系统资源分配的最小单位 进程内的执行单元 程序控制的轻量级并发单元 资源空间 拥有自己独立的地址空间、环境等资源 多个线程共享进程的地址空间和资源 同步机制 需要特定的机制（IPC） 可以直接读写共享内存 需要程序显式控制切换 系统开销 上下文切换，开销较大 比进程小的多 开销很小 使用场景 多核处理器 高并发任务和 I/O 密集型任务 高并发任务和 I/O 密集型任务 创建线程的方法有哪些？ 创建线程的两种方法：\n继承 Thread 类并重写 run() 方法：该方法是最基本的创建线程的方式，一般通过继承 Thread 类并覆盖 run() 方法实现。然后可以通过调用线程的 start() 方法来启动线程。 实现 Runnable 接口：通过实现 Runnable 接口来完成，实现 run 方法，并将实现 Runnable 类的实例对象作为 Thread 对象的参数，也要调用线程的 start() 方法来启动线程。 实现 Callable 接口 线程池创建线程 创建线程有三种方式，分别是继承 Thread 类、实现 Runnable 接口、实现 Callable 接口和线程池创建线程\n通过继承 Thread 类来创建并启动线程的步骤如下：\n定义 Thread 类的子类，并重写该类的 run()方法，该 run()方法将作为线程执行体。 创建 Thread 子类的实例，即创建了线程对象。 调用线程对象的 start()方法来启动该线程。 通过实现 Runnable 接口来创建并启动线程的步骤如下：\n定义 Runnable 接口的实现类，并实现该接口的 run()方法，该 run()方法将作为线程执行体。 创建 Runnable 实现类的实例，并将其作为 Thread 的参数来创建 Thread 对象，Thread 对象为线程对象。 调用线程对象的 start()方法来启动该线程。 通过实现 Callable 接口来创建并启动线程的步骤如下：\n创建 Callable 接口的实现类，并实现 call()方法，该 call()方法将作为线程执行体，且该 call()方法有返回值。（返回值类型就是实现 Callable 接口的泛型类型）然后再创建 Callable 实现类的实例。 使用 FutureTask 类来包装 Callable 对象，该 FutureTask 对象封装了该 Callable 对象的 call()方法的返回值。 使用 FutureTask 对象作为 Thread 对象的参数创建并启动新线程。 调用 FutureTask 对象的 get()方法来获得子线程执行结束后的返回值。 启动 Callable 对象的线程需要调用 FutureTask 对象的 run() 方法，而不是直接调用 Callable 对象的 call() 方法。启动后线程会调用 run() 方法，再由 run() 方法调用 call() 方法。另外，可以通过 FutureTask 的 get() 方法阻塞当前线程，并等待 Callable 对象的 call() 方法执行完成并返回值。\nimport java.util.concurrent.*; class MyCallable implements Callable\u0026lt;Integer\u0026gt; { public Integer call() throws Exception { // 线程执行的任务逻辑，返回一个结果 return 42; } } // 创建线程池 ExecutorService executor = Executors.newFixedThreadPool(1); // 提交任务并获取Future对象 Future\u0026lt;Integer\u0026gt; future = executor.submit(new MyCallable); // 获取线程的返回结果 Integer result = future.get(); // 关闭线程池 executor.shutdown(); 使用线程池创建线程（项目中一般都使用）\nExecutorService threadPool = Executors.newFixedThreadPool(3); //提交任务就是启动了线程 threadPool.submit(); Runnbale 和 Callable 的区别 Runnable 是 Java 中定义的一个接口，表示可执行的任务。它只有一个 run 方法，没有返回值，也不能抛出受检查的异常。通常用于创建线程并执行任务。\nCallable 也是 Java 中定义的一个接口，表示可调用的任务。它有一个 call 方法，可以返回一个结果，并且可以抛出受检查的异常。通常用于创建线程并执行任务，需要获取任务的返回结果。\nRunnable 和 Callable 接口的主要区别：\n返回值类型：Runnable 接口的 run 方法没有返回值，而 Callable 接口的 call 方法可以返回一个结果。 异常处理：Runnable 的 run 方法不能抛出受检查的异常，只能捕获处理，而 Callable 的 call 方法可以抛出受检查的异常。 使用方式：Runnable 接口方法没有返回值，而 Callable 接口的 call 方法有返回值，类型就是实现 Callable 接口的泛型类型，通过 FutureTask 可以获取异步执行的结果 run 和 start 方法的区别 run 和 start 方法的区别：\nrun() 方法是线程的一个普通方法，可以多次直接调用，但却不会创建一个新的线程，只会在当前线程中按顺序执行 run() 方法的代码。 start() 方法是用来启动一个新线程的方法，它会在新的线程中执行 run() 方法的代码。只能调用一次。 需要注意的是：\nstart() 方法只能被调用一次，因为一个线程只能被启动一次。 run 方法是同步执行，start 是异步执行，不会阻塞当前线程 线程有哪些通信方式？拓展进程间通信 线程间的通信方式\n互斥锁提供了以排他方式防止数据结构被并发修改的方法。\n信号量 和互斥锁的区别在于：互斥锁只允许一个线程进入临界区，信号量允许多个线程同时进入临界区 互斥锁使用对同一个资源的互斥的方式达到线程同步的目的，信号量可以同步多个资源以达到线程同步。PV 操作\n读写锁允许多个线程同时读共享数据，而对写操作是互斥的。\n条件变量可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。\n自旋锁与互斥量类似，也是只有解锁和加锁两种状态，它与互斥量的区别在于，它不会阻塞线程。\n信号机制(Signal) 类似进程间的信号处理，主要用于处理异常状况，用特定的码代指信息。\n进程间的通信方式\n管道\n管道是一种单向的数据传输方式，实现线程间的通信需要两个管道。\n管道这种通信方式的缺点就是通信效率低，好处就是实现简单。\n消息队列\n为了解决管道通信效率低，可以使用消息队列。\n消息队列这种通信方式就是有一个生产者，一个消费者，运送数据的效率取决于消息队列的容量。\n消息队列的缺点就是，不适合传输较大的数据。 另外传输数据要进行内核态与用户态之间的拷贝，效率比较低。\n共享内存\n为了解决内核态与用户态之间数据拷贝带来的开销，可以使用共享内存\n共享内存就是通过指针的指向改变来完成数据的访问。\n优点就在于可以省去拷贝开销，但是随之而来的问题就是对共享资源互斥访问需要控制，不然会带来安全性问题。\n信号量\n为了解决对共享资源访问的同步、互斥问题，可以使用信号量。\n信号量其实就是一个整型计数器，用来记录资源的数量，通过 PV 操作来实现进程间的同步、互斥流程。\n信号\n对于异常情况下的工作模式，就需要用「信号」的方式来通知进程。\n信号跟信号量虽然名字相似度很高，但两者用途完全不一样。信号是通过固定的信号标志来传达信息。\nSocket\n前面涉及到的都是同一台主机间通信方式，还有不同主机间的通信，那就要用到 Socket.\n创建 Socket 的系统调用：\nint socket(int domain, int type, int protocal) domain 参数用来指定协议族 type 参数用来指定通信特性 protocal 参数原本是用来指定通信协议的，但现在基本废弃。\n线程间通信与进程间通信的区别\n线程是轻量级的进程， 系统进行资源调度的基本单位是进程，但是因为进程上下文切换开销太大，所以有了线程，节省开销。 线程本身也是共享进程的内存，上下文切换方便。\n安全性 线程间通信的安全性相对较低，需要采用同步机制来保证共享变量的正确性； 而进程间通信的安全性相对较高，进程之间相互隔离，不会对对方的内存进行非法操作。\n并发与并行？同步与异步？ 并发和并行、同步和异步是计算机领域中常用的概念。\n并发（Concurrency）：并发是指两个或多个任务在同一时间段内执行。这些任务可以同时启动，但不一定同时完成。在单个处理器系统中，通过快速轮换任务的执行，给人一种同时执行的感觉。并发可以提高系统的吞吐量和响应能力，更有效地利用系统资源。\n并行（Parallelism）：**并行是指两个或多个任务同时执行，可以在多个处理器或多个处理核心上实现。**不同于并发，真正的并行需要同时具备多个任务执行的硬件条件。通过并行，可以同时处理更多的任务，提高系统的计算能力和处理速度。\n同步（Synchronous）：**同步是指一个任务在完成之前，需要等待另一个任务的执行结果。**在同步操作中，任务按顺序执行，前一个任务完成后，下一个任务才能开始执行。同步操作可以简化代码逻辑，但可能会导致等待时间过长的问题。\n异步（Asynchronous）：**异步是指一个任务的执行不会阻塞其他任务的执行。**在异步操作中，任务可以同时进行，不需要等待前一个任务完成。异步操作通常通过回调函数、Promise、async/await 等方式实现，并可以提高系统的响应能力和执行效率。\n总结区别：\n并发和并行针对任务执行的时间特性，一个是同时启动但不一定同时完成，一个是同时执行。 同步和异步关注任务之间的依赖关系，同步需要等待前一个任务完成，异步则不需要等待。 需要注意的是，并发和并行以及同步和异步虽然有区别，但在某些情况下可以同时存在。例如，可以通过多线程实现并发和并行，而在异步编程中也可以实现并发的效果。这些概念在程序设计和系统优化中都有重要的应用。\nThread 类的常用方法 Thread 类常用构造方法：\nThread() Thread(String name) Thread(Runnable target) Thread(Runnable target, String name) 其中，参数 name 为线程名，参数 target 为包含线程体的目标对象。\nThread 类常用静态方法：\ncurrentThread()：返回当前正在执行的线程； interrupted()：返回当前执行的线程是否已经被中断； start()：启动线程，调用线程的 run 方法 sleep(long millis)：使当前执行的线程睡眠多少毫秒数； yield()：使当前执行的线程放弃对 CPU 的使用权并允许其他线程执行； Thread 类常用实例方法：\ngetId()：返回该线程的 id； getName()：返回该线程的名字； getPriority()：返回该线程的优先级； interrupt()：使该线程中断； isInterrupted()：返回该线程是否被中断； isAlive()：返回该线程是否处于活动状态； isDaemon()：返回该线程是否是守护线程； setDaemon(boolean on)：将该线程标记为守护线程或用户线程，如果不标记默认是非守护线程； setName(String name)：设置该线程的名字； setPriority(int newPriority)：改变该线程的优先级； join()：等待该线程终止； join(long millis)：等待该线程终止,至多等待多少毫秒数。 线程的生命周期是什么，线程有几种状态，什么是上下文切换？ Thread 类的枚举 state\n1、六种状态 Java中有六种状态：新建状态(New)、就绪状态(Runnable)、阻塞状态(Blocked)、等待状态(Waiting)、超时等待(Timed_Waiting)、终止状态(Terminated)\nNEW：初始状态，线程被创建出来但没有被调用 start() 。 RUNNABLE：可运行状态，线程被调⽤了 start() 等待运行的状态。 BLOCKED：阻塞状态，需要等待锁释放。 WAITING：等待状态，表示该线程需要等待其他线程做出⼀些特定动作（通知或中断）。 TIME_WAITING：超时等待状态，可以在指定的时间自行返回而不是像 WAITING 那样⼀直等待。 TERMINATED：终⽌状态，表示该线程已经运行完毕。 2、五种状态\n从操作系统层面的划分，线程有五种状态：新建、就绪、运行、阻塞和死亡状态。\n新建状态(New) ： 新创建了一个线程对象 就绪状态(runnable) ： 线程对象创建后，其他线程调用了该对象的 start 方法。该状态的线程位于可运行线程池中，变得可运行，等待获取 CPU 的使用权 运行状态(Running) ： 就绪状态的线程获取了 CPU，执行程序代码 阻塞状态(Blocked)：阻塞状态是线程因为某种原因放弃 CPU 使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态 死亡状态(Dead) ：线程执行完了或者因异常退出了 run 方法，该线程结束生命周期。 阻塞情况又分为三种：\n等待阻塞：运行的线程执行 wait 方法，该线程会释放占用的所有资源，JVM 会把该线程放入 \u0026ldquo;等待池\u0026quot;中，进入这个状态后，是不能自动唤醒的，必须依靠其他线程调用 notify 或者 notifyAll 方法才能被唤醒，wait 是 object 类的方法。 同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则 JVM 会把该线程放入 \u0026ldquo;锁池\u0026quot;中。 其他阻塞：运行的线程执行 sleep 或者 join 方法，或者发出了 I/O 请求时，JVM 会把该线程设置为阻塞状态。当 sleep 状态超时、join 等待线程终止或者超时、或者 I/O 处理完毕时，线程重新转入就绪状态。 sleep 是 Thread 类的方法。 3、上下文切换\n定义：保存当前线程的运行状态，然后加载另一个线程的状态继续执行。\n线程在执行过程中会有自己的运行条件和状态（也称上下文），比如上文所说到过的程序计数器，栈信息等。当出现如下情况的时候，线程会从占用 CPU 状态中退出。\n主动让出 CPU，比如调用了 sleep(), wait() 等。 时间片用完，因为操作系统要防止一个线程或者进程长时间占用 CPU 导致其他线程或者进程饿死。 调用了阻塞类型的系统中断，比如请求 IO，线程被阻塞。 被终止或结束运行 这其中前三种都会发生线程切换，**保存当前线程的上下文，留待线程下次占用 CPU 的时候恢复现场。并加载下一个将要占用 CPU 的线程上下文。**这就是所谓的 上下文切换。\n上下文切换是现代操作系统的基本功能，因其每次需要保存信息恢复信息，这将会占用 CPU，内存等系统资源进行处理，也就意味着效率会有一定损耗，如果频繁切换就会造成整体效率低下\n什么是线程死锁?如何避免死锁? 线程死锁发生在**两个或多个线程相互等待彼此持有的资源，导致所有线程都无法继续执行，从而陷入无限等待的状态。**在死锁中，每个线程都在等待一个资源，而这个资源却被其他线程持有，因此无法继续执行。\n死锁的四个必要条件称为“死锁条件”，它们是：\n互斥条件（Mutual Exclusion）：至少有一个资源被设置为只能被一个线程同时访问。 请求与保持条件（Hold and Wait）：线程持有至少一个资源，同时又请求其他线程持有的资源。 不可剥夺条件（No Preemption）：资源不能被强制从持有者那里收回，只能由持有者自愿释放。 循环等待条件（Circular Wait）：一组线程互相等待彼此持有的资源，形成一个闭环。 如何预防死锁？ 破坏死锁的产生的必要条件即可：\n破坏请求与保持条件：一次性申请所有的资源。 破坏不剥夺条件：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件：靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。 如何避免死锁？\n避免死锁就是在资源分配时，借助于算法（比如银行家算法）对资源分配进行计算评估，使其进入安全状态。\n安全状态 指的是系统能够按照某种线程推进顺序（P1、P2、P3\u0026hellip;..Pn）来为每个线程分配所需资源，直到满足每个线程对资源的最大需求，使每个线程都可顺利完成。称 \u0026lt;P1、P2、P3.....Pn\u0026gt; 序列为安全序列\n下面是一段可以避免死锁的实例代码 new Thread(() -\u0026gt; { synchronized (resource1) { System.out.println(Thread.currentThread() + \u0026#34;get resource1\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \u0026#34;waiting get resource2\u0026#34;); synchronized (resource2) { System.out.println(Thread.currentThread() + \u0026#34;get resource2\u0026#34;); } } }, \u0026#34;线程 2\u0026#34;).start(); 线程 1 首先获得到 resource1 的监视器锁,这时候线程 2 就获取不到了。然后线程 1 再去获取 resource2 的监视器锁，可以获取到。然后线程 1 释放了对 resource1、resource2 的监视器锁的占用，线程 2 获取到就可以执行了。这样就破坏了破坏循环等待条件，因此避免了死锁\n说一说 Java 同步机制中的 wait 和 notify wait 和 notify 方法必须在 synchronized 同步代码块中使用\nwait 方法会让当前线程释放对象锁进入等待状态，无限制等待，直到有线程调用 notify 方法唤醒当前线程（带参数的 wait 方法会让线程等待有限的时间，时间结束结束等待，或者被唤醒 notify）\nnotify 方法用于唤醒一个正在等待对象锁的线程，使其进入就绪队列，可以争夺锁进而得到 CPU 的执行\nnotifyAll 方法会唤醒所有正在等待相应对象锁的线程，使它们进入就绪队列，以便在当前线程释放锁后竞争锁，进而得到 CPU 的执行。\nsleep()和 wait()的区别 共同点：\n两个方法都可以暂停线程的执行，线程状态都是 TIMED_WAITING 区别：\nsleep方法通常用于线程休眠一段时间后再继续执行，wait方法通常用于让线程进入等待状态 sleep() 方法没有释放锁，而 wait() 方法释放了锁 。 wait() 通常被用于线程间交互/通信，sleep()通常被用于暂停执行。 wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify()或者 notifyAll() 方法。sleep()方法执行完成后，线程会自动苏醒，或者也可以使用 wait(long timeout) 超时后线程会自动苏醒。 sleep() 是 Thread 类的静态本地方法，wait() 则是 Object 类的本地方法。 sleep()可以在任何地方使用，而wait()只能在同步方法或同步代码块中使用； 可以直接调用 Thread 类的 run 方法吗？ 这是另一个非常经典的 Java 多线程面试问题，而且在面试中会经常被问到。很简单，但是很多人都会答不上来！\nnew 一个 Thread，线程进入了新建状态。调用 start()方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start() 会执行线程的相应准备工作，然后自动执行 run() 方法的内容，这是真正的多线程工作。\n但是，直接执行 run() 方法，会把 run() 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。\n总结：调用 start() 方法可启动线程并使线程进入就绪状态，直接执行 run() 方法的话不会以多线程的方式执行。\nvolatile 关键字的作用 volatile 关键字可以保证共享变量在多线程中的可见性和有序性\nvolatile 保证可见性：\n当一个线程修改了共享变量的值时，这个新值会立即被其他线程看到。这是因为在进行 volatile 变量的读写操作时，会强制从主内存中读取或写入，而不是从线程的本地内存中读写。 volatile 保证有序性：（底层是内外屏障）\n确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面； 有序性保证了读取和写入操作是按照一定的顺序执行的。volatile 底层通过读写屏障保证了有序性。 写屏障保证了在写屏障之前的对共享变量的操作 都保存到主存中，读屏障保证了在该屏障之后读取的共享变量的值是主存中最新数据 volatile 只可以保证多线程环境下共享变量的可见性与有序性，但不能保证多线程环境下的原子性\n面试中面试官经常会说：“单例模式了解吗？来给我手写一下！给我解释一下双重检验锁方式实现单例模式的原理呗！”\n双重校验锁实现对象单例（线程安全）：\npublic class Singleton { private volatile static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) { //类对象加锁 synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行：\n为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-\u0026gt;3-\u0026gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化\n线程安全的特性有哪些？如何实现？ 线程安全特性有哪些？\n原子性：原子性是指一个操作在执行的过程中不会被其他线程干扰或分割为多个步骤。在原子操作中，要么所有的操作都执行成功，要么所有的操作都执行失败，没有中间状态。\n可见性：可见性是指一个线程修改的状态对其他线程是可见的，即当一个线程修改了某个共享变量的值之后，其他线程也能够看到这个新值。\n有序性：有序性是指程序中的语句按照一定的顺序执行，即使在不同的线程中执行。 保证指令不会受 cpu 指令并行优化的影响\n互斥性：互斥性是指对共享资源的访问是互斥的，即同一时刻只能有一个线程访问共享资源，避免了竞争和数据冲突。\n无死锁：当多个线程访问共享资源的时候，如果所有线程都无法继续执行，就会出现死锁。线程安全的程序应该能够避免出现死锁情况。\n如何实现？\n**原⼦性：**J M M 只能保证基本的原⼦性，如果要保证⼀个代码块的原⼦性，需要使⽤ s y n c h r o n i z e d\n使⽤循环原⼦类，例如 A t o m i c I n t e g e r，实现 i + +原⼦操作 使⽤ j u c 包下的锁，如 R e e n t r a n t L o c k ，对 i + +操作加锁 l o c k . l o c k ( )来实现原⼦性 使⽤ synchronized 锁，对 i + +操作加锁 可见性：\nvolatile 可以用于保证共享变量的可见性。volatile 的主要作用是强制线程每次读取该变量时都从主存中读取，而不是从本地缓存中读取。当一个线程修改了 volatile 变量的值后，会立即更新到主存中，并通知所有其他线程。 通过 synchronized 关键字也可实现可见性，synchronized 可以用于实现多线程之间的同步访问。同步块中的所有共享变量都可以保证可见性 有序性： s y n c h r o n i z e d 或者 v o l a t i l e 都可以保证多线程之间操作的有序性。\nvolatile 关键字底层读写屏障可以保证线程内的有序性 写屏障会确保指令重排序时，不会将写屏障之前的代码排在写屏障之后 读屏障会确保指令重排序时，不会将读屏障之后的代码排在读屏障之前 如何理解 Java 内存模型 JMM（Java Memory Model，Java 内存模型）是 Java 中用来描述多线程并发访问共享内存中的变量和对象的规范。JMM 定义了线程之间如何访问共享内存以及如何保证数据的可见性、一致性和顺序性。\n以下是 JMM 的一些关键概念：\n主内存（Main Memory）： 主内存是所有线程共享的内存存储区域，用于存储共享变量和对象。主内存中的数据可以被多个线程访问和修改。\n工作内存（Working Memory）： 每个线程都有自己的工作内存，用于存储从主内存中读取的数据副本。线程对变量的操作都在工作内存中进行。\n共享变量（Shared Variables）： 共享变量是存储在主内存中的变量，可以被多个线程同时访问和修改。共享变量包括类的静态变量和实例变量。\n原子性（Atomicity）： JMM 保证了对共享变量的读取和写入操作是原子的，即不会被中断。\n可见性（Visibility）： JMM 确保一个线程对共享变量的修改对其他线程是可见的。这意味着当一个线程修改了共享变量的值后，其他线程能够立即看到这个变化。\n有序性（Ordering）： JMM 定义了对共享变量操作的顺序，即一个线程的操作对其他线程是有序的。这防止了指令重排和优化导致的不确定行为。\nhappens-before 关系： happens-before 是 JMM 中的一个重要概念，用来定义不同操作之间的顺序关系。如果操作 A happens-before 操作 B，那么操作 A 的结果对操作 B 是可见的。\nJMM 的目标是提供一种标准的方式来处理多线程程序中的内存访问，以确保线程之间的协同工作是正确的。开发人员可以通过使用关键字 synchronized、volatile、final 以及 java.util.concurrent 包中的工具来编写线程安全的代码，并遵循 JMM 的规范，以确保数据的一致性和可靠性。\n乐观锁与悲观锁 什么是悲观锁？ 悲观锁总是假设最坏的情况，认为共享资源每次被访问的时候就会出现问题(比如共享数据被修改)，所以每次在获取资源操作的时候都会上锁，这样其他线程想拿到这个资源就会阻塞直到锁被上一个持有者释放。也就是说，共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程。\n像 Java 中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现\npublic void performSynchronisedTask() { synchronized (this) { // 需要同步的操作 } } private Lock lock = new ReentrantLock(); lock.lock(); try { // 需要同步的操作 } finally { lock.unlock(); } 高并发的场景下，激烈的锁竞争会造成线程阻塞，大量阻塞线程会导致系统的上下文切换，增加系统的性能开销。并且，悲观锁还可能会存在死锁问题，影响代码的正常运行\n什么是乐观锁？ 乐观锁总是假设最好的情况，认为共享资源每次被访问的时候不会出现问题，线程可以不停地执行，无需加锁也无需等待，只是在提交修改的时候去验证对应的资源（也就是数据）是否被其它线程修改了（具体方法可以使用版本号机制或 CAS 算法）。\n在 Java 中java.util.concurrent.atomic包下面的原子变量类（比如AtomicInteger、LongAdder）就是使用了乐观锁的一种实现方式 CAS 实现的。\n// LongAdder 在高并发场景下会比 AtomicInteger 和 AtomicLong 的性能更好 // 代价就是会消耗更多的内存空间（空间换时间） LongAdder sum = new LongAdder(); sum.increment(); 高并发的场景下，乐观锁相比悲观锁来说，不存在锁竞争造成线程阻塞，也不会有死锁的问题，在性能上往往会更胜一筹。但是，如果冲突频繁发生（写占比非常多的情况），会频繁失败和重试，这样同样会非常影响性能，导致 CPU 飙升。\n不过，大量失败重试的问题也是可以解决的，像我们前面提到的 LongAdder以空间换时间的方式就解决了这个问题。\n理论上来说：\n悲观锁通常多用于写比较多的情况（多写场景，竞争激烈），这样可以避免频繁失败和重试影响性能，悲观锁的开销是固定的。不过，如果乐观锁解决了频繁失败和重试这个问题的话（比如LongAdder），也是可以考虑使用乐观锁的，要视实际情况而定。 乐观锁通常多用于写比较少的情况（多读场景，竞争较少），这样可以避免频繁加锁影响性能。不过，乐观锁主要针对的对象是单个共享变量（参考java.util.concurrent.atomic包下面的原子变量类） 如何实现乐观锁？ 乐观锁一般会使用版本号机制或 CAS 算法实现，CAS 算法相对来说更多一些，这里需要格外注意。\n版本号机制 一般是在数据表中加上一个数据版本号 version 字段，表示数据被修改的次数。当数据被修改时，version 值会加一。当线程 A 要更新数据值时，在读取数据的同时也会读取 version 值，在提交更新时，若刚才读取到的 version 值为当前数据库中的 version 值相等时才更新，否则重试更新操作，直到更新成功。\n举一个简单的例子：假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。\n操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 在操作员 A 操作的过程中，操作员 B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 操作员 A 完成了修改工作，将数据版本号（ version=1 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本等于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 操作员 B 完成了操作，也将版本号（ version=1 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 1 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须等于当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员 A 的操作结果的可能\nCAS 算法 CAS 的全称是 Compare And Swap（比较与交换） ，用于实现乐观锁，被广泛应用于各大框架中。CAS 的思想很简单，就是用一个预期值和要更新的变量值进行比较，两值相等才会进行更新。\nCAS 是一个原子操作，底层依赖于一条 CPU 的原子指令。\n原子操作 即最小不可拆分的操作，也就是说操作一旦开始，就不能被打断，直到操作完成。\nCAS 涉及到三个操作数：\nV：要更新的变量值(Var) E：预期值(Expected) N：拟写入的新值(New) 当且仅当 V 的值等于 E 时，CAS 通过原子方式用新值 N 来更新 V 的值。如果不等，说明已经有其它线程更新了 V，则当前线程放弃更新。\n举一个简单的例子：线程 A 要修改变量 i 的值为 6，i 原值为 1（V = 1，E=1，N=6，假设不存在 ABA 问题）。\ni 与 1 进行比较，如果相等， 则说明没被其他线程修改，可以被设置为 6 。 i 与 1 进行比较，如果不相等，则说明被其他线程修改，当前线程放弃更新，CAS 操作失败。 当多个线程同时使用 CAS 操作一个变量时，只有一个会胜出，并成功更新，其余均会失败，但失败的线程并不会被挂起，仅是被告知失败，并且允许再次尝试，当然也允许失败的线程放弃操作。\nJava 语言并没有直接实现 CAS，CAS 相关的实现是通过 C++ 内联汇编的形式实现的（JNI 调用）。因此， CAS 的具体实现和操作系统以及 CPU 都有关系。\nsun.misc包下的Unsafe类提供了compareAndSwapObject、compareAndSwapInt、compareAndSwapLong方法来实现的对Object、int、long类型的 CAS 操作\n乐观锁存在哪些问题？ ABA 问题是乐观锁最常见的问题。\nABA 问题 如果一个变量 V 初次读取的时候是 A 值，并且在准备赋值的时候检查到它仍然是 A 值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回 A，那 CAS 操作就会误认为它从来没有被修改过。这个问题被称为 CAS 操作的 \u0026ldquo;ABA\u0026quot;问题。\nABA 问题的解决思路是在变量前面追加上版本号或者时间戳。JDK 1.5 以后的 AtomicStampedReference 类就是用来解决 ABA 问题的，其中的 compareAndSet() 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。\npublic boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) { Pair\u0026lt;V\u0026gt; current = pair; return expectedReference == current.reference \u0026amp;\u0026amp; expectedStamp == current.stamp \u0026amp;\u0026amp; ((newReference == current.reference \u0026amp;\u0026amp; newStamp == current.stamp) || casPair(current, Pair.of(newReference, newStamp))); } 循环时间长开销大 CAS 经常会用到自旋操作来进行重试，也就是不成功就一直循环执行直到成功。如果长时间不成功，会给 CPU 带来非常大的执行开销。\n如果 JVM 能支持处理器提供的 pause 指令那么效率会有一定的提升，pause 指令有两个作用：\n可以延迟流水线执行指令，使 CPU 不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。 可以避免在退出循环的时候因内存顺序冲而引起 CPU 流水线被清空，从而提高 CPU 的执行效率。 只能保证一个共享变量的原子操作 CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5 开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作\nSynchronized 锁 常考面试题：\n什么是synchronized关键字？它在 Java 中的作用是什么？\nsynchronized关键字可以用于哪些地方？分别是什么意义？\n什么是对象级别的锁和类级别的锁？\n请解释一下 Java 中的内置锁（Intrinsic Lock）或监视器锁（Monitor Lock）是如何工作的？\nsynchronized方法和synchronized代码块有什么区别？\n对于以下代码片段，会发生什么情况？\njavaCopy codepublic synchronized void method1() { // 一些操作 } public void method2() { synchronized (this) { // 一些操作 } } synchronized关键字如何保证线程安全性？有哪些缺点？\n什么是死锁？如何避免死锁？\n如果一个线程正在执行一个synchronized方法，另一个线程能否同时执行另一个非synchronized方法？\n什么是可重入锁（Reentrant Lock）？synchronized关键字是否支持可重入性？\n如何在多个线程之间共享数据，同时又保证线程安全？\n什么是线程的安全性问题？如何通过synchronized来解决这些问题？\nvolatile关键字和synchronized关键字有什么区别？\nsynchronized关键字会导致性能问题吗？如果有，有什么方法来优化？\nsynchronized关键字在 Java 5 之后是否有优化？如果有，可以简要描述一下。\n除了synchronized，还有哪些用于实现线程同步的方式？\nsynchronized 锁的原理 synchronized 是同步锁，有两种使用方式：\n作用在代码块上 作用在方法上 synchronized 作用在代码块上：\nsynchronized(obj) { //锁住的代码块 } 其中，obj 表示要锁定的对象，通常是共享资源或者一个专门用于同步的对象。当某个线程进入到这个\nsynchronized 代码块中时，会首先尝试获得 obj 对象的锁。如果当前没有其他线程持有 obj 对象的锁，那么该线\n程就会成功获得锁，并执行被锁定的代码；否则，该线程就会阻塞等待，直到它获得 obj 对象的锁为止。\n当一个线程执行完 synchronized 代码块后，会立即释放 obj 对象的锁，以便其他线程可以获得锁并执行相应的代码块。\nsynchronized 作用在方法上：\npublic synchronized void getName() { //锁住的方法体 } 当某个线程调用带有 synchronized 关键字的方法时，它会自动获得该对象的锁，从而避免了多个线程同时访问这\n个方法的问题。相当于在方法代码块的前面加上了 synchronized(this)。\n需要注意的是，如果一个类中有多个带有 synchronized 关键字的方法，那么不同的线程可能会同时访问这些方法，因为它们使用的是不同的锁。\n无论是 synchronized 代码块还是 synchronized 方法，都是通过获取对象锁来实现同步的。\n什么是 synchronized 关键字？作用是什么？ synchronized是 Java 编程语言中的关键字，**用于实现线程之间的同步和互斥访问共享资源。**它提供了一种简单的方式来确保在多线程环境下对临界区（Critical Section）的访问是安全的，从而避免竞态条件（Race Condition）和其他线程安全问题。\nsynchronized的主要作用是：\n确保线程安全性： 在多线程环境中，多个线程可能会并发地访问共享资源，如果没有适当的同步机制，可能会导致数据损坏、不一致性或其他意外情况。通过使用synchronized，可以保证在同一时间只有一个线程能够进入被同步的代码块或方法，从而避免了多个线程同时修改共享资源的问题。\n实现互斥访问： 当一个线程获得了对象的synchronized锁后，其他线程需要等待该锁被释放才能继续执行。这确保了同一时刻只有一个线程能够执行被锁定的代码，从而避免了并发访问问题。\n如何使用 synchronized？ synchronized关键字有三种主要的用法：\n修饰方法： （锁当前对象实例） 使用synchronized修饰方法时，整个方法体被视为临界区，同一时间只有一个线程能够执行该方法。\npublic synchronized void synchronizedMethod() { // 临界区代码 } 修饰代码块： 给括号内指定对象加锁（锁指定对象/类） synchronized(object) 表示进入同步代码库前要获得 给定对象的锁。 synchronized(类.class) 表示进入同步代码前要获得 给定 Class 的锁 public void someMethod() { synchronized (this) { //业务逻辑 } } 修饰静态方法 （锁当前类） 给当前类加锁，会作用于类的所有对象实例 ，进入同步代码前要获得 当前 class 的锁。\n这是因为静态成员不属于任何一个实例对象，归整个类所有，不依赖于类的特定实例，被类的所有实例共享。\nsynchronized static void method() { //业务代码 } 静态 synchronized 方法和非静态 synchronized 方法之间的调用互斥么？不互斥！如果一个线程 A 调用一个实例对象的非静态 synchronized 方法，而线程 B 需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象\n总结：\nsynchronized 关键字加到 static 静态方法和 synchronized(class) 代码块上都是是给 Class 类上锁； synchronized 关键字加到实例方法上是给对象实例上锁； 尽量不要使用 synchronized(String a) 因为 JVM 中，字符串常量池具有缓存功 synchronized 底层原理？ synchronized 底层原理属于 JVM 层面的东西\nsynchronized 同步代码块的情况： public class SynchronizedDemo { public void method() { synchronized (this) { System.out.println(\u0026#34;synchronized 代码块\u0026#34;); } } } synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。\nsynchronzied 实现过程：\n当执行 monitorenter 指令时，线程试图获取锁也就是获取 对象监视器 monitor 的持有权。 在执行monitorenter时，会尝试获取对象的锁，如果锁的计数器为 0 则表示锁可以被获取，获取后将锁计数器设为 1 也就是加 1。 对象锁的的拥有者线程才可以执行 monitorexit 指令来释放锁。在执行 monitorexit 指令后，将锁计数器设为 0，表明锁被释放，其他线程可以尝试获取锁 如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止 synchronized 修饰方法的情况： public class SynchronizedDemo2 { public synchronized void method() { System.out.println(\u0026#34;synchronized 方法\u0026#34;); } } synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。\n如果是实例方法，JVM 会尝试获取实例对象的锁。如果是静态方法，JVM 会尝试获取当前 class 的锁\n总结：\nsynchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。 synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。 不过两者的本质都是对对象监视器 monitor 的获取 synchronized关键字的原理涉及 Java 中的内置锁（Intrinsic Lock）或监视器锁（Monitor Lock）。它基于对象头（Object Header）和对象监视器（Monitor）的概念来实现线程同步和互斥访问。\n以下是synchronized锁的原理：\n对象头（Object Header）： 每个 Java 对象都有一个对象头，其中包含了与对象相关的一些元信息，包括用于实现线程同步的信息。这些信息通常包括指向锁的指针、持有锁的线程标识等。\n对象监视器（Monitor）： 每个 Java 对象都与一个关联的 Monitor 相关联，它用于实现synchronized锁的语义。Monitor 内部维护了一个等待队列（Wait Queue）和一个持有锁的线程队列（Owner Queue）。\n进入临界区： 当一个线程尝试进入一个被synchronized修饰的方法或代码块时，它会尝试获取该对象的锁（Monitor）。如果锁没有被其他线程持有，该线程就会成功获取锁，并且进入临界区执行代码。\n竞争与等待： 如果锁已经被其他线程持有，那么当前线程会进入锁的等待队列，等待其他线程释放锁。此时，其他线程仍然可以尝试获取锁，但只有一个线程能够成功。未获得锁的线程会进入等待状态，直到锁被释放。\n退出临界区： 当线程执行完synchronized代码块或方法时，它会释放持有的锁，从而允许等待队列中的其他线程获取锁并进入临界区。\n需要注意的是，synchronized锁是可重入的，这意味着同一个线程可以多次获取同一个锁，而不会出现死锁。在可重入的情况下，线程已经持有的锁可以被多次获取，而不会阻塞自己。\nsynchronized锁的工作原理保证了线程安全和互斥访问，但在高并发场景下可能引起性能问题。因此，在使用synchronized时，需要注意锁的粒度、避免过多的锁竞争，以及考虑使用更高级别的同步机制如java.util.concurrent包中的工具来优化性能。\nJDK1.6 之后的 synchronized 底层做了哪些优化？ JDK1.6 对锁的实现引入了大量的优化，如偏向锁、轻量级锁、自旋锁、适应性自旋锁、锁消除、锁粗化等技术来减少锁操作的开销。\n锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率\n参考文章：https://www.cnblogs.com/wuqinglong/p/9945618.html\nsynchronized 和 volatile 有什么区别？ synchronized 关键字和 volatile 关键字是两个互补的存在，而不是对立的存在！\nvolatile 关键字是线程同步的轻量级实现，所以 volatile性能肯定比synchronized关键字要好 。但是 volatile 关键字只能用于变量而 synchronized 关键字可以修饰方法以及代码块 。 volatile 关键字能保证数据的可见性，但不能保证数据的原子性。synchronized 关键字两者都能保证。 volatile关键字主要用于解决变量在多个线程之间的可见性，而 synchronized 关键字解决的是多个线程之间访问资源的同步性 锁升级流程？ 在 Java 中，synchronized 锁是通过监视器锁（Monitor Lock）来实现的。在 JVM 中，锁的升级过程指的是将偏向锁（Biased Locking）升级为轻量级锁（Lightweight Locking）或重量级锁（Heavyweight Locking）的过程。\n偏向锁（Biased Locking）：当一个线程获取锁时，会在对象头中的标记字段设置为该线程的标识，表示这个对象被该线程偏向。此时，其他线程在尝试获取锁时会看到对象头中的标记，判断是否是同一个线程，如果是就无需进入同步状态，直接执行。偏向锁适用于短时间内只有一个线程访问锁的场景。 轻量级锁（Lightweight Locking）：如果有第二个线程尝试获取锁，偏向锁会升级为轻量级锁。升级的过程中，JVM 会尝试使用 CAS（Compare and Swap）操作将对象头中的线程标记字段改为指向当前线程的锁记录，同时会将对象的原始数据拷贝到锁记录中，以便在解锁时恢复对象数据。如果 CAS 操作成功，表示当前线程获取了锁；否则，表示有竞争，锁会升级为重量级锁。 重量级锁（Heavyweight Locking）：当轻量级锁升级失败（即 CAS 操作失败）或者发生多线程竞争的情况下，锁会进一步升级为重量级锁。重量级锁会让等待的线程进入阻塞状态，通过操作系统的互斥量（Mutex）实现，确保只有持有锁的线程可以访问受保护的资源。 synchronized 关键字是什么，有什么作用？ 来自：回家养猪\nsynchronized 原理 synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置, monitorexit 指令则指明同步代码块的结束位置\n当多个线程同时访问该方法，那么这些线程会先被放进对象的锁池，此时线程处于 blocking 状态 当一个线程获取到了实例对象的监视器（monitor）锁，那么就可以进入 running 状态，执行方法，此时 lock record 中的 owner 设置为当前线程，count加 1 表示当前对象锁被一个线程获取 当 running 状态的线程调用 wait()方法，那么当前线程释放 monitor 对象，进入 waiting 状态, lock record 中的 owner 变为 null，count减 1，同时线程进入等待池，直到有线程调用 notify()方法唤醒该线程，则该线程重新获取 monitor 对象进入owner 如果当前线程执行完毕，那么也释放 monitor 对象，进入 waiting 状态，lock record 中的 owner 变为 null，count减 1 JDK1.6 之后的 synchronized 底层做了哪些优化？ java 的线程模型是 1 对 1 的, 加锁需要调用操作系统的底层原语 mutex, 所以每次切换线程都需要操作系统切换到内核态, 开销很大. 这也是之前 synchronized 的问题所在, jdk1.6 对其进行了优化, 从无锁到偏向锁到轻量级锁到重量级锁 自旋锁就不需要阻塞, 也就不需要操作系统切换为内核态去做, 所以短时间的自旋开销是比较低的.\nJDK 1.6 引入了偏向锁和轻量级锁，从而让锁拥有了四个状态：无锁状态（unlocked）、偏向锁状态（biasble）、轻量级锁状态（lightweight locked）和重量级锁状态（inflated）。 虚拟机对象头里锁标志位, 就记录了这 4 中状态.\n偏向锁\n大多数时候是不存在锁竞争的，常常是一个线程多次获得同一个锁，因此如果每次都要竞争锁会增大很多没有必要付出的代价，为了降低获取锁的代价，才引入的偏向锁。\n当锁对象第一次被线程获得的时候，使用 CAS 操作将线程 ID 记录到对象头的 MarkWord 中，这个线程以后每次进入这个锁相关的同步块就不需要再进行任何同步操作。 当有另外一个线程去尝试获取这个锁对象时，偏向状态就宣告结束，此时撤销偏向后恢复到未锁定状态或者轻量级锁状态。 轻量级锁\n轻量级锁考虑的是竞争锁对象的线程不多，而且线程持有锁的时间也不长的情景。因为阻塞线程需要 CPU 从用户态转到内核态，代价较大，如果刚刚阻塞不久这个锁就被释放了，那这个代价就有点得不偿失了，因此这个时候就干脆不阻塞这个线程，让它自旋这等待锁释放。\n轻量级锁是相对于传统的重量级锁而言，它使用自旋 + CAS 操作来避免重量级锁使用互斥量的开销。\n长时间的自旋会使 CPU 一直空转, 浪费 CPU, 所以这里自旋是适应性自旋, 自旋时间由上一个线程自旋的时间决定的.\n线程自旋的次数到了阈值, 另外一个线程还没释放锁, 那么就膨胀为重量级锁。 如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁。 锁消除\n锁消除是指对于被检测出不可能存在竞争的共享数据的锁进行消除。\n锁消除主要是通过逃逸分析来支持，如果堆上的共享数据不可能逃逸出去被其它线程访问到，那么就可以把它们当成私有数据对待，也就可以将它们的锁进行消除。\n锁粗化\n如果一系列的连续操作都对同一个对象反复加锁和解锁，频繁的加锁操作就会导致性能损耗。\n比如连续使用 StringBuffer 的 append() 方法就属于这类情况。如果虚拟机探测到由这样的一串零碎的操作都对同一个对象加锁，将会把加锁的范围扩展（粗化）到整个操作序列的外部, 这样只需要加锁一次就可以了。\nReentrantLock ReentrantLock 是什么？ ReentrantLock 实现了 Lock 接口，是一个可重入且独占式的锁，和 synchronized 关键字类似。不过，ReentrantLock 更灵活、更强大，增加了轮询、超时、中断、公平锁和非公平锁等高级功能。\npublic class ReentrantLock implements Lock, java.io.Serializable {} ReentrantLock 里面有一个内部类 Sync，Sync 继承 AQS（AbstractQueuedSynchronizer），添加锁和释放锁的大部分操作实际上都是在 Sync 中实现的。Sync 有公平锁 FairSync 和非公平锁 NonfairSync 两个子类。\nReentrantLock 默认使用非公平锁，也可以通过构造器来显式的指定使用公平锁。\n// 传入一个 boolean 值，true 时为公平锁，false 时为非公平锁 public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } 公平锁和非公平锁有什么区别？ 公平锁 : 锁被释放之后，先申请的线程先得到锁。性能较差一些，因为公平锁为了保证时间上的绝对顺序，上下文切换更频繁。 非公平锁：锁被释放之后，后申请的线程可能会先获取到锁，是随机或者按照其他优先级排序的。性能更好，但可能会导致某些线程永远无法获取到锁。 synchronized 和 ReentrantLock 有什么区别？ 两者都是可重入锁 可重入锁 也叫递归锁，指的是线程可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果是不可重入锁的话，就会造成死锁。\nJDK 提供的所有现成的 Lock 实现类，包括 synchronized 关键字锁都是可重入的。\nsynchronized 依赖于 JVM 而 ReentrantLock 依赖于 API synchronized 是依赖于 JVM 实现的， JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。\nReentrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock() 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的\nReentrantLock 比 synchronized 增加了一些高级功能 相比synchronized，ReentrantLock增加了一些高级功能。主要来说主要有三点：\n等待可中断 : ReentrantLock提供了一种能够中断等待锁的线程的机制，通过 lock.lockInterruptibly() 来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 可实现公平锁 : ReentrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。ReentrantLock默认情况是非公平的，可以通过 ReentrantLock类的ReentrantLock(boolean fair)构造方法来制定是否是公平的。 可实现选择性通知（锁可以绑定多个条件）: synchronized关键字与wait()和notify()/notifyAll()方法相结合可以实现等待/通知机制。ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition()方法。 关于 Condition接口的补充：\nCondition是 JDK1.5 之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify()/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，这个功能非常重要，而且是 Condition 接口默认提供的。而synchronized关键字就相当于整个 Lock 对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程，这样会造成很大的效率问题。而Condition实例的signalAll()方法，只会唤醒注册在该Condition实例中的所有等待线程\n可中断锁和不可中断锁有什么区别？ 可中断锁：获取锁的过程中可以被中断，不需要一直等到获取锁之后 才能进行其他逻辑处理。ReentrantLock 就属于是可中断锁。 不可中断锁：一旦线程申请了锁，就只能等到拿到锁以后才能进行其他的逻辑处理。 synchronized 就属于是不可中断锁 ReadReentrantReadWriteLock 关于 ReadReentrantReadWriteLock ReentrantReadWriteLock 实现了 ReadWriteLock ，是一个可重入的读写锁，既可以保证多个线程同时读的效率，同时又可以保证有写入操作时的线程安全\n一般锁进行并发控制的规则：读读互斥、读写互斥、写写互斥。 读写锁进行并发控制的规则：读读不互斥、读写互斥、写写互斥（只有读读不互斥）。 ReentrantReadWriteLock 其实是两把锁，一把是 WriteLock (写锁)，一把是 ReadLock（读锁） 。读锁是共享锁，写锁是独占锁。读锁可以被同时读，可以同时被多个线程持有，而写锁最多只能同时被一个线程持有。\n和 ReentrantLock 一样，ReentrantReadWriteLock 底层也是基于 AQS 实现的。\n适用场景\n由于 ReentrantReadWriteLock 既可以保证多个线程同时读的效率，同时又可以保证有写入操作时的线程安全。因此，在读多写少的情况下，使用 ReentrantReadWriteLock 能够明显提升系统性能\n共享锁和独占锁有什么区别？\n共享锁：一把锁可以被多个线程同时获得。 独占锁：一把锁只能被一个线程获得 线程持有读锁还能获取写锁吗？\n在线程持有读锁的情况下，该线程不能取得写锁(因为获取写锁的时候，如果发现当前的读锁被占用，就马上获取失败，不管读锁是不是被当前线程持有)。 在线程持有写锁的情况下，该线程可以继续获取读锁（获取读锁时如果发现写锁被占用，只有写锁没有被当前线程占用的情况才会获取失败）。 读写锁的源码分析，推荐阅读：https://mp.weixin.qq.com/s/h3VIUyH9L0v14MrQJiiDbw\nStampedLock StampedLock 是什么？ StampedLock 是 JDK 1.8 引入的性能更好的读写锁，不可重入且不支持条件变量 Conditon。\n不同于一般的 Lock 类，StampedLock 并不是直接实现 Lock或 ReadWriteLock接口，而是基于 CLH 锁 独立实现的（AQS 也是基于这玩意）。\npublic class StampedLock implements java.io.Serializable { } StampedLock 提供了三种模式的读写控制模式：读锁、写锁和乐观读。\n写锁：独占锁，一把锁只能被一个线程获得。当一个线程获取写锁后，其他请求读锁和写锁的线程必须等待。类似于 ReentrantReadWriteLock 的写锁，不过这里的写锁是不可重入的。 读锁 （悲观读）：共享锁，没有线程获取写锁的情况下，多个线程可以同时持有读锁。如果己经有线程持有写锁，则其他线程请求获取该读锁会被阻塞。类似于 ReentrantReadWriteLock 的读锁，不过这里的读锁是不可重入的。 乐观读：允许多个线程获取乐观读以及读锁。同时允许一个写线程获取写锁 StampedLock 的性能为什么更好？ 相比于传统读写锁多出来的乐观读是StampedLock比 ReadWriteLock 性能更好的关键原因。StampedLock 的乐观读允许一个写线程获取写锁，所以不会导致所有写线程阻塞，也就是当读多写少的时候，写线程有机会获取写锁，减少了线程饥饿的问题，吞吐量大大提高\nStampedLock 适合什么场景？ 和 ReentrantReadWriteLock 一样，StampedLock 同样适合读多写少的业务场景，可以作为 ReentrantReadWriteLock的替代品，性能更好。\n不过，需要注意的是StampedLock不可重入，不支持条件变量 Conditon，对中断操作支持也不友好（使用不当容易导致 CPU 飙升）。如果你需要用到 ReentrantLock 的一些高级性能，就不太建议使用 StampedLock 了。\n另外，StampedLock 性能虽好，但使用起来相对比较麻烦，一旦使用不当，就会出现生产问题。强烈建议你在使用StampedLock 之前，看看 StampedLock 官方文档中的案例 open in new window。\nStampedLock 的底层原理了解吗？ StampedLock 不是直接实现 Lock或 ReadWriteLock接口，而是基于 CLH 锁 实现的（AQS 也是基于这玩意），CLH 锁是对自旋锁的一种改良，是一种隐式的链表队列。StampedLock 通过 CLH 队列进行线程的管理，通过同步状态值 state 来表示锁的状态和类型。\nAQS 详解 open in new window StampedLock 底层原理分析 AQS AQS 是什么？ AQS 的全称为 AbstractQueuedSynchronizer ，翻译过来的意思就是抽象队列同步器。这个类在 java.util.concurrent.locks 包下面\nAQS 就是一个抽象类，主要用来构建锁和同步器。\npublic abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable { } AQS 为构建锁和同步器提供了一些通用功能的实现，因此，使用 AQS 能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的 ReentrantLock，Semaphore，其他的诸如 ReentrantReadWriteLock，SynchronousQueue等等皆是基于 AQS 的\n你对 AQS 原理的理解？ 要说出自己的理解，不是完全背题。\nAQS 核心思想是，**如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，**这个机制 AQS 是基于 CLH 锁 （Craig, Landin, and Hagersten locks） 实现的\nAQS 使用 int 成员变量 state 表示同步状态，通过内置的 FIFO 线程等待/等待队列 来完成获取资源线程的排队工作。\nstate 变量由 volatile 修饰，用于展示当前临界资源的获锁情况。\n// 共享变量，使用volatile修饰保证线程可见性 private volatile int state; 另外，状态信息 state 可以通过 protected 类型的getState()、setState()和compareAndSetState() 进行操作。并且，这几个方法都是 final 修饰的，在子类中无法被重写。\n//返回同步状态的当前值 protected final int getState() { return state; } // 设置同步状态的值 protected final void setState(int newState) { state = newState; } //原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值） protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 以可重入的互斥锁 ReentrantLock 为例，它的内部维护了一个 state 变量，用来表示锁的占用状态。state 的初始值为 0，表示锁处于未锁定状态。当线程 A 调用 lock() 方法时，会尝试通过 tryAcquire() 方法独占该锁，并让 state 的值加 1。如果成功了，那么线程 A 就获取到了锁。如果失败了，那么线程 A 就会被加入到一个等待队列（CLH 队列）中，直到其他线程释放该锁。假设线程 A 获取锁成功了，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加）。这就是可重入性的体现：一个线程可以多次获取同一个锁而不会被阻塞。但是，这也意味着，一个线程必须释放与获取的次数相同的锁，才能让 state 的值回到 0，也就是让锁恢复到未锁定状态。只有这样，其他等待的线程才能有机会获取该锁。\n线程 A 尝试获取锁的过程如下图所示（图源从 ReentrantLock 的实现看 AQS 的原理及应用 - 美团技术团队）：\nAQS 独占模式获取锁\n再以倒计时器 CountDownLatch 以例，任务分为 N 个子线程去执行，state 也初始化为 N（注意 N 要与线程个数一致）。这 N 个子线程开始执行任务，每执行完一个子线程，就调用一次 countDown() 方法。该方法会尝试使用 CAS(Compare and Swap) 操作，让 state 的值减少 1。当所有的子线程都执行完毕后（即 state 的值变为 0），CountDownLatch 会调用 unpark() 方法，唤醒主线程。这时，主线程就可以从 await() 方法（CountDownLatch 中的await() 方法而非 AQS 中的）返回，继续执行后续的操作\nAQS 资源共享方式\nAQS 定义两种资源共享方式：Exclusive（独占，只有一个线程能执行，如ReentrantLock）和Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch）。\n一般来说，自定义同步器的共享方式要么是独占，要么是共享，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。\n自定义同步器\n同步器的设计是基于模板方法模式的，如果需要自定义同步器一般的方式是这样（模板方法模式很经典的一个应用）：\n使用者继承 AbstractQueuedSynchronizer 并重写指定的方法。 将 AQS 组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法。 这和我们以往通过实现接口的方式有很大区别，这是模板方法模式很经典的一个运用。\nAQS 使用了模板方法模式，自定义同步器时需要重写下面几个 AQS 提供的钩子方法：\n//独占方式。尝试获取资源，成功则返回true，失败则返回false。 protected boolean tryAcquire(int) //独占方式。尝试释放资源，成功则返回true，失败则返回false。 protected boolean tryRelease(int) //共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 protected int tryAcquireShared(int) //共享方式。尝试释放资源，成功则返回true，失败则返回false。 protected boolean tryReleaseShared(int) //该线程是否正在独占资源。只有用到condition才需要去实现它。 protected boolean isHeldExclusively() \u0026ldquo;AQS（AbstractQueuedSynchronizer）是 Java 并发编程中的一个重要概念，它提供了一种通用的框架，用于实现各种同步机制，如锁、信号量等。AQS 的核心思想是通过一个等待队列来管理等待访问共享资源的线程，以及通过状态的管理来控制线程的访问。\nAQS 的主要特点包括：\n状态管理： AQS 通过一个整数状态来表示资源的可用情况。这个状态可以被子类自由定义和操作，以满足不同同步场景的需求。 等待队列： AQS 维护一个基于 FIFO（先进先出）原则的等待队列，用于存储被阻塞的线程。这些线程将在资源不可用时被放入队列中等待。 独占模式和共享模式： AQS 支持独占模式和共享模式。独占模式用于实现类似 ReentrantLock 的互斥访问，而共享模式用于实现类似 CountDownLatch 和 Semaphore 的多线程协作。 CAS 操作： AQS 使用 CAS（Compare-and-Swap）操作来更新状态，保证在多线程环境下的原子性和可见性。 AQS 的工作流程大致如下：\n当线程需要获取锁或访问共享资源时，它首先会尝试更新 AQS 的状态。如果状态符合预期，线程就可以继续执行临界区代码。 如果状态不符合预期，线程会被放入等待队列，并被阻塞。 当资源释放时，AQS 会按照一定的策略从等待队列中选择一个线程来唤醒，使其可以继续执行。 底层数据结构 AQS（AbstractQueuedSynchronizer）的底层数据结构主要包括两部分：同步队列（Sync Queue）和状态变量。\n同步队列（Sync Queue）： **AQS 的核心是一个基于双向链表的同步队列，用于管理竞争同步资源的线程。**这个队列中的每个节点代表一个等待线程，按照 FIFO 的顺序排列。队列头节点是当前持有资源的线程（获取锁的线程），而后续节点是正在等待资源的线程。当线程尝试获取资源失败时，它会被包装成一个节点，插入到等待队列中。等待队列的管理是 AQS 实现并发同步的关键。\n状态变量： AQS 使用一个整数变量来表示同步资源的状态。这个状态变量在不同的同步器中有不同的含义，它可以用于表示锁的可用性、信号量的剩余资源数量等等。**AQS 内部通过 CAS（Compare-And-Swap）等操作来修改和管理这个状态变量，以实现资源的获取和释放。**状态变量的更新需要保证原子性，以防止多线程竞争时出现问题。\n这两部分数据结构共同构成了 AQS 的基础框架，使得它可以支持不同类型的同步器。当线程需要获取同步资源时，它会先尝试通过 CAS 等操作修改状态变量，如果成功则表示获取资源成功，否则线程会被封装成一个节点，插入到同步队列中等待。当资源被释放时，AQS 会从等待队列中选择一个线程唤醒，让它有机会再次尝试获取资源。\n这种基于队列和状态变量的设计允许 AQS 实现一系列复杂的同步工具，如 ReentrantLock、CountDownLatch、Semaphore 等。不同类型的同步器在 AQS 的基础上通过实现不同的模板方法来定义不同的同步策略，从而满足不同场景下的需求。\n如何实现抢占锁和非抢占锁 AQS（AbstractQueuedSynchronizer）可以通过不同的方式实现抢占锁（preemptive lock）和非抢占锁（non-preemptive lock），具体取决于同步器的设计和实现。下面我会分别介绍这两种情况：\n抢占锁（Preemptive Lock）： 在抢占锁的情况下，一个线程可以在任何时间点尝试抢夺已经被其他线程占用的锁。这可能会导致占用锁的线程被中断或者挂起，以便让抢占锁的线程能够立即获得锁并执行。在抢占锁的情况下，AQS 的等待队列可能会被中断线程和非中断线程共享。\n非抢占锁（Non-preemptive Lock）： 在非抢占锁的情况下，一旦一个线程获取了锁，其他线程将无法强制抢夺该锁。只有持有锁的线程主动释放锁后，其他线程才能有机会获取锁。非抢占锁的实现更加简单，因为不需要考虑中断或挂起占用锁的线程。\n实现抢占锁和非抢占锁的关键在于 AQS 中的等待队列的管理。在抢占锁的情况下，当一个线程尝试获取锁失败时，它可能会被封装成一个节点插入等待队列，并且在适当的时候被中断以放弃等待。在非抢占锁的情况下，等待队列可能只需要简单地将等待线程按照 FIFO 顺序排列，并在锁被释放时依次唤醒。\n需要注意的是，Java 中的 ReentrantLock 提供了可重入锁的实现，而 ReentrantLock 可以通过构造函数的参数来选择是否为公平锁。公平锁会优先唤醒等待时间最长的线程，从而实现抢占锁的效果。非公平锁则允许新来的线程优先获取锁，但这可能会导致等待时间较长的线程饱受“饿死”问题。Semaphore 和 CountDownLatch 等同步工具在 AQS 的基础上也可以根据设计选择抢占锁或非抢占锁的机制。\n总之，AQS 的底层结构为不同类型的同步器提供了实现抢占锁和非抢占锁的基础，具体的实现会根据同步器的特性和设计目标进行调整。\n和 CAS 区别 AQS（AbstractQueuedSynchronizer）和 CAS（Compare-And-Swap）都是 Java 并发编程中的重要概念\nAQS（AbstractQueuedSynchronizer）： AQS 是一个抽象的同步框架，用于构建各种类型的同步器（synchronizer），如锁、信号量、倒计时门栓等。AQS 的核心思想是基于等待队列（wait queue）和状态变量，用于管理线程的等待和唤醒。\nCAS（Compare-And-Swap）： CAS 是一种原子操作，用于实现多线程环境下的无锁并发。它的基本思想是先比较内存中的值与期望值是否相等，如果相等则将新值写入内存，否则操作失败。CAS 操作是原子的，因此可以在不使用锁的情况下实现并发控制。CAS 通常用于实现一些原子性操作，如实现自旋锁、无锁数据结构、线程安全计数器等。\n区别：\n用途： AQS 用于构建同步工具的框架，提供了等待队列和状态变量的管理机制，以支持各种同步器的实现。CAS 用于实现无锁并发的原子操作，解决竞争条件和线程安全问题。\n抽象性： AQS 是一个高层抽象，需要进行具体的子类实现。CAS 是一种底层的原子操作，通常由底层的硬件指令或者操作系统的原子操作提供支持。\n使用场景： AQS 主要用于构建复杂的同步工具，如 ReentrantLock、Semaphore 等。CAS 主要用于实现简单的原子操作，如无锁的计数器、自旋锁等。\n关联： AQS 在其内部的实现中可能会使用 CAS 来保证状态变量的原子更新。CAS 本身不需要依赖 AQS，但 AQS 的实现可能会使用 CAS 操作。\n并发控制： AQS 提供了阻塞和唤醒机制，使得线程可以在等待和释放资源时进行协调。CAS 本身只是一种原子操作，不涉及线程的等待和唤醒。\nAtomic 原子类 根据操作的数据类型，可以将 JUC 包中的原子类分为 4 类\n基本类型\n使用原子的方式更新基本类型\nAtomicInteger：整型原子类 AtomicLong：长整型原子类 AtomicBoolean：布尔型原子类 数组类型\n使用原子的方式更新数组里的某个元素\nAtomicIntegerArray：整型数组原子类 AtomicLongArray：长整型数组原子类 AtomicReferenceArray：引用类型数组原子类 引用类型\nAtomicReference：引用类型原子类 AtomicMarkableReference：原子更新带有标记的引用类型。该类将 boolean 标记与引用关联起来，也可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 AtomicStampedReference：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 对象的属性修改类型\nAtomicIntegerFieldUpdater:原子更新整型字段的更新器 AtomicLongFieldUpdater：原子更新长整型字段的更新器 AtomicReferenceFieldUpdater：原子更新引用类型里的字段 基本类型原子类 使用原子的方式更新基本类型\nAtomicInteger：整型原子类 AtomicLong：长整型原子类 AtomicBoolean：布尔型原子类 上面三个类提供的方法几乎相同，所以我们这里以 AtomicInteger 为例子来介绍。\nAtomicInteger 类常用方法：\npublic final int get() //获取当前的值 public final int getAndSet(int newValue)//获取当前的值，并设置新的值 public final int getAndIncrement()//获取当前的值，并自增 public final int getAndDecrement() //获取当前的值，并自减 public final int getAndAdd(int delta) //获取当前的值，并加上预期的值 boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update） public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 基本数据类型原子类的优势：不用加锁在多线程下对于基本数据的修改也可以保证线程安全。\nAtomicInteger 线程安全原理简单分析\nAtomicInteger 类的部分源码：\n// setup to use Unsafe.compareAndSwapInt for updates（更新操作时提供“比较并替换”的作用） private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\u0026#34;value\u0026#34;)); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。\nCAS 的原理是拿期望的值和原本的一个值作比较，如果相同则更新成新的值。UnSafe 类的 objectFieldOffset() 方法是一个本地方法，这个方法是用来拿到“原来的值”的内存地址。另外 value 是一个 volatile 变量，在内存中可见，因此 JVM 可以保证任何时刻任何线程总能拿到该变量的最新值\n数组类型原子类 使用原子的方式更新数组里的某个元素\nAtomicIntegerArray：整形数组原子类 AtomicLongArray：长整形数组原子类 AtomicReferenceArray：引用类型数组原子类 上面三个类提供的方法几乎相同，所以我们这里以 AtomicIntegerArray 为例子来介绍。\nAtomicIntegerArray 类常用方法：\npublic final int get(int i) //获取 index=i 位置元素的值 public final int getAndSet(int i, int newValue)//返回 index=i 位置的当前的值，并将其设置为新值：newValue public final int getAndIncrement(int i)//获取 index=i 位置元素的值，并让该位置的元素自增 public final int getAndDecrement(int i) //获取 index=i 位置元素的值，并让该位置的元素自减 public final int getAndAdd(int i, int delta) //获取 index=i 位置元素的值，并加上预期的值 boolean compareAndSet(int i, int expect, int update) //如果输入的数值等于预期值，则以原子方式将 index=i 位置的元素值设置为输入值（update） public final void lazySet(int i, int newValue)//最终 将index=i 位置的元素设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 引用类型原子类 基本类型原子类只能更新一个变量，如果需要原子更新多个变量，需要使用 引用类型原子类。\nAtomicReference：引用类型原子类 AtomicStampedReference：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 AtomicMarkableReference：原子更新带有标记的引用类型。该类将 boolean 标记与引用关联起来，也可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 对象的属性修改类型原子类 如果需要原子更新某个类里的某个字段时，需要用到对象的属性修改类型原子类。\nAtomicIntegerFieldUpdater:原子更新整形字段的更新器 AtomicLongFieldUpdater：原子更新长整形字段的更新器 AtomicReferenceFieldUpdater：原子更新引用类型里的字段的更新器 要想原子地更新对象的属性需要两步。第一步，因为对象的属性修改类型原子类都是抽象类，所以每次使用都必须使用静态方法 newUpdater()创建一个更新器，并且需要设置想要更新的类和属性。第二步，更新的对象属性必须使用 public volatile 修饰符。\nThreadLocal 是什么？ ThreadLocal 类是在 java.lang 包中，ThreadLocal 顾名思义，就是线程的本地变量的意思，即线程在本地各有一份自己的变量副本，线程之间各自使用自己的“共享资源”，避免共享资源的竞争\n线程安全问题的核心是多个线程会共享临界区的资源，都可以对其操作。ThreadLocal 与 synchronized 和 lock 上锁保护临界资源不同，ThreadLocal 是让线程在本地都拥有自己的变量“副本”，每个线程各自使用自己的，不影响其他线程，即达到线程隔离的效果。\nThreadLocal 是 Java 中的一个类，它提供了线程局部变量的机制。线程局部变量是指每个线程都有自己独立的变量副本，线程之间互不影响。在多线程环境中，使用 ThreadLocal 可以在每个线程中存储和获取自己的数据，而不必担心线程安全问题。\nThreadLocal 主要用于解决多线程访问共享变量时可能出现的线程安全问题。如果多个线程同时访问共享变量，由于竞争条件，可能会导致数据不一致或者错误的结果。ThreadLocal 提供了一种线程隔离的方式，每个线程都可以独立地操作自己的变量副本，从而避免了线程安全问题。\n如何使用？要注意什么？ 使用 ThreadLocal 需要注意一些关键点，包括初始化、存储、获取、移除以及可能的内存泄漏问题。以下是如何正确使用 ThreadLocal 的步骤和注意事项：\n使用：\n初始化 ThreadLocal 对象： 在使用之前，需要初始化 ThreadLocal 对象。通常，你会创建一个静态的 ThreadLocal 变量，然后使用 initialValue() 方法为每个线程设置初始值。也可以在需要时通过 set() 方法设置值。\nprivate static ThreadLocal\u0026lt;Integer\u0026gt; threadLocal = ThreadLocal.withInitial(() -\u0026gt; 0); 存储和获取线程局部变量： 使用 get() 方法可以获取当前线程的局部变量值，使用 set(value) 方法可以设置当前线程的局部变量值。每个线程都会拥有自己的独立副本。\nint value = threadLocal.get(); // 获取当前线程的值 threadLocal.set(newValue); // 设置当前线程的值 移除线程局部变量： 在不再需要使用 ThreadLocal 存储的数据时，需要显式地调用 remove() 方法，以避免内存泄漏。如果不移除，可能会导致持有的数据无法释放，从而造成内存泄漏。\nthreadLocal.remove(); // 移除当前线程的值 注意点：\n注意线程池使用： 在使用线程池的情况下，要特别注意 ThreadLocal 的使用。由于线程池中的线程可能被多个任务复用，如果没有适当的管理，可能会导致线程之间的数据混淆。 避免数据共享问题： 尽管 ThreadLocal 可以避免线程间的数据共享问题，但是仍然需要确保存储在 ThreadLocal 中的数据本身是线程安全的。如果存储的数据本身不是线程安全的，可能会引发问题。 防止内存泄漏： 当不再需要使用 ThreadLocal 存储的数据时，一定要调用 remove() 方法，以避免内存泄漏。如果不移除，存储在 ThreadLocal 中的数据可能会长时间存在，导致内存泄漏。 总之，ThreadLocal 是一种有用的工具，可以在多线程环境中实现线程隔离的数据存储。然而，要小心使用，确保正确地初始化、存储、获取和移除数据，以及避免内存泄漏问题。\n底层结构是什么？ 在 Java 中，ThreadLocal 底层数据结构主要是一个 ThreadLocalMap，它是一个自定义的哈希表（散列表）实现。每个 Thread 都有一个与之关联的 ThreadLocalMap，ThreadLocal 对象作为键，存储的值作为值，这样可以确保每个线程都有独立的数据副本，互不影响。\n以下是 ThreadLocalMap 的一些关键特点：\n哈希表结构: ThreadLocalMap 是基于哈希表的数据结构，它使用哈希算法将 ThreadLocal 对象映射到具体的值。\n解决哈希冲突: 在哈希表中，不同的 ThreadLocal 对象可能映射到相同的哈希桶（哈希冲突）。为了解决哈希冲突，ThreadLocalMap 使用开放地址法来处理。\nEntry 存储: ThreadLocalMap 中的每个键值对被封装成一个 Entry 对象，每个 Entry 对象包含一个 ThreadLocal 键和对应的值。\n使用弱引用: ThreadLocalMap 使用了弱引用来防止内存泄漏。ThreadLocal 对象被用作键，如果没有其他强引用指向 ThreadLocal 对象，那么它会被垃圾回收，从而自动删除相应的条目。\n自动清理: ThreadLocalMap 会在 Thread 完成后自动清理与该 Thread 相关的所有条目，以避免潜在的内存泄漏。\n由于 ThreadLocalMap 是 Java 语言实现的一部分，其具体实现细节可能会因版本和厂商而异。在 Java 8 及之后的版本中，ThreadLocalMap 进行了一些优化，以减少内存消耗和增加查找效率。不过，无论具体的实现细节如何，ThreadLocal 作为 Java 中实现线程局部变量的机制，其底层都使用了类似哈希表的结构来管理线程间数据的隔离。\n适用于什么场景？ ThreadLocal 在以下场景中非常适用：\n线程安全的数据隔离：ThreadLocal 可以在多线程环境中实现线程安全的数据隔离，每个线程都有自己独立的数据副本，互不影响。这在需要在多个线程间共享数据，但又不希望使用锁或同步机制的情况下非常有用。 上下文信息传递：在某些情况下，需要在线程间传递上下文信息，例如用户认证信息、语言偏好等。通过 ThreadLocal 可以方便地将上下文信息与线程关联，避免在方法参数中传递这些信息。 数据库连接管理：在数据库连接池中，每个线程需要维护自己的数据库连接，使用 ThreadLocal 可以确保每个线程都使用自己的连接，避免线程间的连接混淆。 会话管理：Web 应用中，可以使用 ThreadLocal 存储用户会话信息，确保每个用户在不同的线程中都能访问自己的会话数据。 请求上下文传递：在 Web 请求处理过程中，可以将一些请求相关的信息存储在 ThreadLocal 中，以便在后续处理中使用，而无需显式地将这些信息传递给每个方法。 单例模式替代：在单线程环境下，可以使用 ThreadLocal 代替一些单例模式，每个线程都拥有自己的实例，不需要进行全局共享。 ThreadLocal 在上述情况下非常有用，但也需要小心使用，避免滥用。不当使用 ThreadLocal 可能会导致内存泄漏、难以排查的问题以及性能问题。在使用 ThreadLocal 时，务必确保正确地初始化、存储、获取和移除数据，以及避免内存泄漏问题。\n为什么会存在内存泄露？如何避免？ ThreadLocal 存在内存泄漏的主要原因是在使用完毕后没有正确地进行清理操作。如果不适当地管理 ThreadLocal，就可能导致数据在线程池、长时间运行的线程等情况下持续存在，从而占用了不必要的内存，造成内存泄漏。\n以下是可能导致 ThreadLocal 内存泄漏的情况：\n没有显式调用 remove() 方法： 当线程使用完 ThreadLocal 存储的数据后，应该显式地调用 remove() 方法来清除数据，以便释放相关资源。如果没有调用该方法，数据会一直保留在 ThreadLocalMap 中，可能导致长时间存在。\n线程池中的 ThreadLocal： 当使用线程池时，线程可能会被重用，但 ThreadLocal 的数据可能会残留。如果在一个线程中使用了 ThreadLocal，但没有在使用完毕后清理，当线程被回收再次分配给其他任务时，原有的 ThreadLocal 数据可能会继续存在，从而造成内存泄漏。\n长时间运行的线程： 在某些应用中，长时间运行的线程可能会持续存活，而 ThreadLocal 中的数据也会持续存在。如果这些数据不再需要，但没有被清理，就会造成内存泄漏。\n没有及时释放对象引用： 如果存储在 ThreadLocal 中的对象引用没有及时释放，可能会导致相关对象无法被垃圾回收。\n为了避免 ThreadLocal 内存泄漏，需要遵循以下几点：\n使用完毕后，务必显式地调用 remove() 方法，清除 ThreadLocal 存储的数据。 在线程池中使用 ThreadLocal 时，务必确保在任务结束后清理 ThreadLocal 数据。 避免在长时间运行的线程中存储大量数据，确保数据的生命周期与线程的生命周期相匹配。 确保存储在 ThreadLocal 中的对象引用能够被及时释放，避免引发内存泄漏。 ThreadLocal 内存泄露问题是怎么导致的？ ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用，而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。\n这样一来，ThreadLocalMap 中就会出现 key 为 null 的 Entry。假如我们不做任何措施的话，value 永远无法被 GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap 实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。使用完 ThreadLocal方法后最好手动调用remove()方法\nstatic class Entry extends WeakReference\u0026lt;ThreadLocal\u0026lt;?\u0026gt;\u0026gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal\u0026lt;?\u0026gt; k, Object v) { super(k); value = v; } } 弱引用介绍：\n如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它 所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。\n弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中\n工作原理？ ThreadLocal 的工作原理主要涉及到每个线程都拥有自己的 ThreadLocalMap，以及使用 ThreadLocal 对象作为键来存储和获取线程局部变量的值。以下是 ThreadLocal 的工作原理步骤：\n创建 ThreadLocal 对象： 首先，需要创建一个 ThreadLocal 对象。每个 ThreadLocal 对象都代表一个线程局部变量，通过它可以存储和获取线程独有的数据。\n线程与 ThreadLocalMap 关联： 当线程首次访问某个 ThreadLocal 对象的 get() 或 set() 方法时，会在当前线程中创建一个 ThreadLocalMap。这个 ThreadLocalMap 是与当前线程关联的哈希表。\n使用 ThreadLocal 存储数据： 使用 set(value) 方法将数据存储到 ThreadLocal 中，实际上是将当前 ThreadLocal 对象作为键，值作为值，存储到当前线程的 ThreadLocalMap 中。这样，每个线程都可以有自己的数据副本。\n获取 ThreadLocal 数据： 使用 get() 方法从当前线程的 ThreadLocalMap 中获取与当前 ThreadLocal 对象关联的数据值。因为每个线程都有自己的 ThreadLocalMap，所以可以确保数据隔离。\n内存回收与清理： 当线程执行完成，或者不再需要存储在 ThreadLocal 中的数据时，需要显式调用 ThreadLocal 的 remove() 方法来清除数据。另外，ThreadLocal 使用弱引用来防止内存泄漏，如果没有其他强引用指向 ThreadLocal 对象，它会被垃圾回收。\n需要注意的是，ThreadLocal 的工作原理使得每个线程都拥有自己独立的数据副本，这可以避免线程安全问题，但也需要谨慎使用，避免内存泄漏问题。特别是在使用线程池的情况下，要确保在任务结束后显式清理 ThreadLocal 数据，以避免数据残留和内存泄漏。\nJUC 篇 对 JUC 的理解 JUC（Java.util.concurrent）是 Java 标准库中提供的用于多线程编程的工具包，包含了许多高级并发工具类。JUC 提供了对线程池、锁、并发集合、原子操作、线程同步器、并发工具类等多种支持，并且提供了比传统并发工具更高效、更灵活的实现。\nJUC 类图如下：\nJUC 的主要特点有：\n支持并发编程的高级工具类，如线程池、锁、原子操作等。\n通过原子操作、CAS（Compare and Swap）等方法实现了非阻塞算法，提高了多线程执行效率。\n实现了比传统同步器更加灵活和高效的 AQS（AbstractQueuedSynchronized）同步框架。\n在并发集合类中提供了线程安全的数据结构，如 ConcurrentHashMap、CopyOnWriteArrayList 等。\n支持定时器、Semaphore、CountDownLatch 等常用的并发工具类。\nJUC 包含了以下主要组成部分：\n并发集合\n原子操作类\n线程同步器\n和读写锁\n线程池\n其他并发工具类。\nSemaphore（信号量） 是什么？\nsynchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，而Semaphore(信号量)可以用来控制同时访问特定资源的线程数量。\nSemaphore 的使用简单，我们这里假设有 N(N\u0026gt;5) 个线程来获取 Semaphore 中的共享资源，下面的代码表示同一时刻 N 个线程中只有 5 个线程能获取到共享资源，其他线程都会阻塞，只有获取到共享资源的线程才能执行。等到有线程释放了共享资源，其他阻塞的线程才能获取到。\n// 初始共享资源数量 final Semaphore semaphore = new Semaphore(5); // 获取1个许可 semaphore.acquire(); // 释放1个许可 semaphore.release(); 当初始的资源个数为 1 的时候，Semaphore 退化为排他锁。\nSemaphore 有两种模式：。\n公平模式： 调用 acquire() 方法的顺序就是获取许可证的顺序，遵循 FIFO； 非公平模式： 抢占式的 Semaphore 有两个构造方法，都必须提供许可的数量，第二个构造方法可以指定是公平模式还是非公平模式，默认非公平模式。\n原理？\nSemaphore 是共享锁的一种实现，它默认构造 AQS 的 state 值为 permits，你可以将 permits 的值理解为许可证的数量，只有拿到许可证的线程才能执行。\n以无参 acquire 方法为例，调用semaphore.acquire() ，线程尝试获取许可证，如果 state \u0026gt; 0 的话，则表示可以获取成功，如果 state \u0026lt;= 0 的话，则表示许可证数量不足，获取失败。\n如果可以获取成功的话(state \u0026gt; 0 )，会尝试使用 CAS 操作去修改 state 的值 state=state-1。如果获取失败则会创建一个 Node 节点加入等待队列，挂起当前线程。\n以无参 release 方法为例，调用semaphore.release(); ，线程尝试释放许可证，并使用 CAS 操作去修改 state 的值 state=state+1。释放许可证成功之后，同时会唤醒等待队列中的一个线程。被唤醒的线程会重新尝试去修改 state 的值 state=state-1 ，如果 state \u0026gt; 0 则获取令牌成功，否则重新进入等待队列，挂起线程。\nSemaphore 与 CountDownLatch 一样，也是共享锁的一种实现。它默认构造 AQS 的 state 为 permits。当执行任务的线程数量超出 permits，那么多余的线程将会被放入等待队列 Park,并自旋判断 state 是否大于 0。只有当 state 大于 0 的时候，阻塞的线程才能继续执行,此时先前执行任务的线程继续执行 release() 方法，release() 方法使得 state 的变量会加 1，那么自旋的线程便会判断成功。 如此，每次只有最多不超过 permits 数量的线程能自旋成功，便限制了执行任务线程的数量\nCountdownLatch（倒计时器） CountdownLatch 介绍\nCountDownLatch 允许 count 个线程阻塞在一个地方，直至所有线程的任务都执行完毕。\nCountDownLatch 是一次性的，计数器的值只能在构造方法中初始化一次，之后没有任何机制再次对其设置值，当 CountDownLatch 使用完毕后，它不能再次被使用。\n原理\nCountDownLatch 是共享锁的一种实现，它默认构造 AQS 的 state 值为 count。这个我们通过 CountDownLatch 的构造方法即可看出。\npublic CountDownLatch(int count) { if (count \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;count \u0026lt; 0\u0026#34;); this.sync = new Sync(count); } private static final class Sync extends AbstractQueuedSynchronizer { Sync(int count) { setState(count); } //... } 当线程调用 countDown() 时，其实使用了tryReleaseShared方法以 CAS 的操作来减少 state，直至 state 为 0 。当 state 为 0 时，表示所有的线程都调用了 countDown 方法，那么在 CountDownLatch 上等待的线程就会被唤醒并继续执行。\npublic void countDown() { // Sync 是 CountDownLatch 的内部类 , 继承了 AbstractQueuedSynchronizer sync.releaseShared(1); } releaseShared方法是 AbstractQueuedSynchronizer 中的默认实现。\n// 释放共享锁 // 如果 tryReleaseShared 返回 true，就唤醒等待队列中的一个或多个线程。 public final boolean releaseShared(int arg) { //释放共享锁 if (tryReleaseShared(arg)) { //释放当前节点的后置等待节点 doReleaseShared(); return true; } return false; } tryReleaseShared 方法是CountDownLatch 的内部类 Sync 重写的一个方法， AbstractQueuedSynchronizer中的默认实现仅仅抛出 UnsupportedOperationException 异常。\n// 对 state 进行递减，直到 state 变成 0； // 只有 count 递减到 0 时，countDown 才会返回 true protected boolean tryReleaseShared(int releases) { // 自选检查 state 是否为 0 for (;;) { int c = getState(); // 如果 state 已经是 0 了，直接返回 false if (c == 0) return false; // 对 state 进行递减 int nextc = c-1; // CAS 操作更新 state 的值 if (compareAndSetState(c, nextc)) return nextc == 0; } } 以无参 await方法为例，当调用 await() 的时候，如果 state 不为 0，那就证明任务还没有执行完毕，await() 就会一直阻塞，也就是说 await() 之后的语句不会被执行（main 线程被加入到等待队列也就是 CLH 队列中了）。然后，CountDownLatch 会自旋 CAS 判断 state == 0，如果 state == 0 的话，就会释放所有等待的线程，await() 方法之后的语句得到执行。\n// 等待（也可以叫做加锁） public void await() throws InterruptedException { sync.acquireSharedInterruptibly(1); } // 带有超时时间的等待 public boolean await(long timeout, TimeUnit unit) throws InterruptedException { return sync.tryAcquireSharedNanos(1, unit.toNanos(timeout)); } acquireSharedInterruptibly方法是 AbstractQueuedSynchronizer 中的默认实现。\n// 尝试获取锁，获取成功则返回，失败则加入等待队列，挂起线程 public final void acquireSharedInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); // 尝试获得锁，获取成功则返回 if (tryAcquireShared(arg) \u0026lt; 0) // 获取失败加入等待队列，挂起线程 doAcquireSharedInterruptibly(arg); } tryAcquireShared 方法是CountDownLatch 的内部类 Sync 重写的一个方法，其作用就是判断 state 的值是否为 0，是的话就返回 1，否则返回 -1。\nprotected int tryAcquireShared(int acquires) { return (getState() == 0) ? 1 : -1; } CountDownLatch 的两种典型用法：\n某一线程在开始运行前等待 n 个线程执行完毕 : 将 CountDownLatch 的计数器初始化为 n （new CountDownLatch(n)），每当一个任务线程执行完毕，就将计数器减 1 （countdownlatch.countDown()），当计数器的值变为 0 时，在 CountDownLatch 上 await() 的线程就会被唤醒。一个典型应用场景就是启动一个服务时，主线程需要等待多个组件加载完毕，之后再继续执行。 实现多个线程开始执行任务的最大并行性：注意是并行性，不是并发，强调的是多个线程在某一时刻同时开始执行。类似于赛跑，将多个线程放到起点，等待发令枪响，然后同时开跑。做法是初始化一个共享的 CountDownLatch 对象，将其计数器初始化为 1 （new CountDownLatch(1)），多个线程在开始执行任务前首先 coundownlatch.await()，当主线程调用 countDown() 时，计数器变为 0，多个线程同时被唤醒 CyclicBarrier(循环栅栏) 介绍\nCyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。\nCountDownLatch 的实现是基于 AQS 的，而 CycliBarrier 是基于 ReentrantLock(ReentrantLock 也属于 AQS 同步器)和 Condition 的。\nCyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是：让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。\n原理\nCyclicBarrier 内部通过一个 count 变量作为计数器，count 的初始值为 parties 属性的初始化值，每当一个线程到了栅栏这里了，那么就将计数器减 1。如果 count 值为 0 了，表示这是这一代最后一个线程到达栅栏，就尝试执行我们构造方法中输入的任务。\n//每次拦截的线程数 private final int parties; //计数器 private int count; ConpyOnWriteArrayLisy？ 线程池篇 介绍一下线程池 线程池是一种实现多线程的方式，其主要作用是为了避免频繁创建和销毁线程带来的性能开销。\n顾名思义，线程池就是管理一系列线程的资源池。当有任务要处理时，直接从线程池中获取线程来处理，处理完之后线程并不会立即被销毁，而是回到线程池等待等待下一个任务\n线程池的原理是将多个任务分配给多个线程，任务执行完后，线程不退出，而是继续为线程池服务，减少消耗处理器资源和内存空间的开销，提高性能。\n可以回答以下问题：\n线程池的优点和作用是什么？ Java 线程池如何创建和使用？ Java 线程池中核心参数的作用是什么？ 线程池的实现原理是什么？ 为什么要使用线程池？\n降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控 Java 线程池如何创建和使用?\n有两种方式：\n方式一：通过ThreadPoolExecutor构造函数来创建（推荐）\n方式二：通过 Executor 框架的工具类 Executors 来创建。\n我们可以创建多种类型的内置线程池\nFixedThreadPool固定线程池：该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 SingleThreadExecutor单例线程池： 该方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 **CachedThreadPool缓冲线程池：**可根据实际情况调整线程数量的线程池。初始大小为 0。当有新任务提交时，如果当前线程池中没有线程可用，它会创建一个新的线程来处理该任务。如果在一段时间内（默认为 60 秒）没有新任务提交，核心线程会超时并被销毁，从而缩小线程池的大小。 ScheduledThreadPool定时线程池：一个用来在给定的延迟后运行任务或者定期执行任务的线程池（@Scheduler 定时任务实现缓存预热原理就是这个） 创建线程池需要以下步骤：\n创建线程池：通过 Executors 工厂类的四个静态方法 创建四种不同类型的线程池，如：newCachedThreadPool()、newFixedThreadPool(nThreads)、newSingleThreadExecutor()、newScheduledThreadPool(corePoolSize)。\n在执行任务时，提交任务到线程池：通过 execute() 方法或 submit() 方法提交任务。\n在程序退出时，调用 shutdown() 方法：关闭线程池。\nJava 线程池中核心参数的作用是什么?\n线程池中的核心参数有：\ncorePoolSize：线程池中的核心线程数，表示线程池中允许执行任务的最大线程数。\nmaximumPoolSize: 线程池中最大的线程数，表示线程池最多能创建的工作线程数。\nkeepAliveTime：当线程池中的线程数大于 corePoolSize 时，多余的空闲线程的最大空闲时间，一定时间后将被回收。\nworkQueue：任务队列，用于存储等待执行的任务。\n线程池的实现原理是什么?\n线程池的实现原理如下:\n当一个任务被提交到线程池时，线程池会按照 corePoolSize 参数创建指定数量的线程来执行这些任务。\n当有新的任务到来时，线程池会先将任务放入任务队列中，等待空闲线程来执行。\n当任务队列已满且仍有新的任务到来时，线程池会创建新的工作线程来处理任务，直到线程数达到了 maximumPoolSize 参数指定的最大值。\n当线程池中的线程数大于 corePoolSize 时，多余的空闲线程将被回收，并且这些空闲线程的生存时间为 keepAliveTime。\n为什么不推荐使用内置线程池？ 在《阿里巴巴 Java 开发手册》“并发处理”这一章节，明确指出线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。\n为什么呢？\n使用线程池的好处是减少在创建和销毁线程上所消耗的时间以及系统资源开销，解决资源不足的问题。如果不使用线程池，有可能会造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题。\n另外，《阿里巴巴 Java 开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 构造函数的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险\nExecutors 返回线程池对象的弊端如下(后文会详细介绍到)：\nFixedThreadPool 和 SingleThreadExecutor：使用的是无界的 LinkedBlockingQueue，任务队列最大长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 CachedThreadPool：使用的是同步队列 SynchronousQueue, 允许创建的线程数量为 Integer.MAX_VALUE ，如果任务数量过多且执行速度较慢，可能会创建大量的线程，从而导致 OOM。 ScheduledThreadPool 和 SingleThreadScheduledExecutor : 使用的无界的延迟阻塞队列DelayedWorkQueue，任务队列最大长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 // 无界队列 LinkedBlockingQueue public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads,0L, TimeUnit.MILLISECONDS,new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;()); } // 无界队列 LinkedBlockingQueue public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1,0L, TimeUnit.MILLISECONDS,new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;())); } // 同步队列 SynchronousQueue，没有容量，最大线程数是 Integer.MAX_VALUE` public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE,60L, TimeUnit.SECONDS,new SynchronousQueue\u0026lt;Runnable\u0026gt;()); } // DelayedWorkQueue（延迟阻塞队列） public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize); } public ScheduledThreadPoolExecutor(int corePoolSize) { super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); } 线程池有哪些参数，各个参数的作用是什么？ /** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */ public ThreadPoolExecutor(int corePoolSize,//线程池的核心线程数量 int maximumPoolSize,//线程池的最大线程数 long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间 TimeUnit unit,//时间单位 BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,//任务队列，用来储存等待执行任务的队列 ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可 RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务 ) { if (corePoolSize \u0026lt; 0 || maximumPoolSize \u0026lt;= 0 || maximumPoolSize \u0026lt; corePoolSize || keepAliveTime \u0026lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } ThreadPoolExecutor 3 个最重要的参数：\ncorePoolSize : 核心线程数，任务队列未达到队列容量时，最大可以同时运行的线程数量。 maximumPoolSize : 任务队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。 ThreadPoolExecutor其他常见参数 :\nkeepAliveTime:线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁； unit : keepAliveTime 参数的时间单位。 threadFactory :executor 创建新线程的时候会用到。 handler :拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略 线程池中各个参数 关系：（JavaGuide）\n拒绝策略（RejectedExecutionHandler）：当任务队列和线程池都满了之后采用的策略。常见的有以下几种：\nCallerRunsPolicy：由提出请求的线程处理该任务\nAbortPolicy：直接抛出未处理异常\nDiscardPolicy：直接抛弃超过队列大小的任务\nDiscardOldestPolicy：抛弃队列中最早的一条任务，然后再重新尝试执行任务。\n扩展参数：\n线程执行前的 Hook 方法（beforeExecute）：在线程池中的线程执行前执行任务，也可以用来做线程上下文的一些准备工作。\n线程执行后的 Hook 方法（afterExecute）：在线程池中的线程执行完毕后执行，也可以做线程上下文的清理工作。\n线程存活时间和时间单位（TimeUnit）：用于指定线程存活时间的单位，例如 TimeUnit.SECONDS。\n任务超时时间（keepAliveTime）：即当任务等待了指定的时间后还没有被执行时，则会将其从队列中移除，并进行重试或抛出异常。\n线程池常用的阻塞队列有哪些？ 新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。\n不同的线程池会选用不同的阻塞队列，我们可以结合内置线程池来分析。\n容量为 Integer.MAX_VALUE 的 LinkedBlockingQueue（无界队列）：FixedThreadPool 和 SingleThreadExector 。由于队列永远不会被放满，因此FixedThreadPool最多只能创建核心线程数的线程。 SynchronousQueue（同步队列）：CachedThreadPool 。SynchronousQueue 没有容量，不存储元素，目的是保证对于提交的任务，如果有空闲线程，则使用空闲线程来处理；否则新建一个线程来处理任务。也就是说，CachedThreadPool 的最大线程数是 Integer.MAX_VALUE ，可以理解为线程数是可以无限扩展的，可能会创建大量线程，从而导致 OOM。 DelayedWorkQueue（延迟阻塞队列）：ScheduledThreadPool 和 SingleThreadScheduledExecutor 。DelayedWorkQueue 的内部元素并不是按照放入的时间排序，而是会按照延迟的时间长短对任务进行排序，内部采用的是“堆”的数据结构，可以保证每次出队的任务都是当前队列中执行时间最靠前的。DelayedWorkQueue 添加元素满了之后会自动扩容原来容量的 1/2，即永远不会阻塞，最大扩容可达 Integer.MAX_VALUE，所以最多只能创建核心线程数的线程 线程池的拒绝策略 如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任务时，ThreadPoolTaskExecutor 定义一些策略:\nAbortPolicy： 抛出 RejectedExecutionException来拒绝新任务的处理。 CallerRunsPolicy： 调用执行自己的线程运行任务，也就是直接在调用execute方法的线程中运行(run)被拒绝的任务，如果执行程序已关闭，则会丢弃该任务**。该策略不会抛出异常，而是将任务回退到调用者，由调用者自行执行任务，通常是主线程执行。** DiscardPolicy： 不处理新任务，直接丢弃掉。 DiscardOldestPolicy： 此策略将丢弃最早的未处理的任务请求 线程池处理任务的流程 如果当前运行的线程数小于核心线程数，那么就会新建一个线程来执行任务。 如果当前运行的线程数等于或大于核心线程数，但是小于最大线程数，那么就把该任务放入到任务队列里等待执行。 如果向任务队列投放任务失败（任务队列已经满了），但是当前运行的线程数是小于最大线程数的，就新建一个线程来执行任务。 如果当前运行的线程数已经等同于最大线程数了，新建线程将会使当前运行的线程超出最大线程数，那么当前任务会被拒绝，饱和策略会调用RejectedExecutionHandler.rejectedExecution()方法 线程池的工作流程 线程池的工作流程可以概括为以下几个步骤：\n创建线程池 线程池的创建是通过 java.util.concurrent.Executors 工具类的静态方法完成的，例如 newFixedThreadPool()、newSingleThreadExecutor() 等方法。这些方法会返回一个 ExecutorService 类型的对象，支持提交任务和释放资源等功能。\n接收任务 接下来，当一个任务需要被执行时，线程池需要判断是否有空闲的线程可用。如果当前有空闲线程，则利用一个空闲线程来执行任务；否则，线程池需要将任务添加到线程池中。\n任务管理 在任务执行之前，线程池会为任务分配线程，将任务放入任务队列中，等待线程执行。线程池可以为任务队列设定不同的策略，例如采用 LIFO（后进先出）或 FIFO（先进先出）的方式管理任务队列，或通过优先级来管理任务的执行顺序。\n线程管理 线程池中的线程受到线程池参数的管理，例如核心线程数 corePoolSize、最大线程数 maximumPoolSize、空闲线程存活时间 keepAliveTime 等参数等控制。线程池会检查当前活动线程数，如发现该数量已经达到最大值，则会拒绝新的任务提交，直到有线程资源可用。\n结束任务 当一个任务执行完毕后，线程会自动从线程池中移除。此时，线程可能会被销毁，或者如果线程数量仍然大于核心线程数，则会被转化为一个空闲线程，等待新的任务加入队列。\n销毁线程池 当线程池不再需要时，我们需要显式地释放线程池的资源，避免造成资源浪费。这可以通过调用 ExecutorService 对象的 shutdown() 或 shutdownNow() 方法实现。前者用于停止接收新任务并允许任务队列执行完毕，而后者则是直接中断所有任务执行，立即清理任务队列。\n线程池的实现类哪些实现类会导致 OOM\n在 Java 中，线程池的实现类ThreadPoolExecutor是最常用的线程池实现类，但是其使用不当也可能导致 OOM（Out of Memory）问题。以下是几种可能导致 OOM 的情况：\n使用无界队列：如果线程池的队列采用无界队列，例如LinkedBlockingQueue，当线程池中的任务提交速度大于任务处理速度时，队列会不断增长，可能导致内存溢出。\n使用有界队列：如果线程池的队列采用有界队列，例如ArrayBlockingQueue，当队列已满时，任务无法继续提交，而线程池中的线程数量已经达到最大值，此时可能导致 OOM。\n调整线程池的大小：如果线程池中的线程数量设置过大，超过了系统所能支持的上限，也可能导致 OOM。因此，合理配置线程池的核心线程数、最大线程数以及队列大小，以适应系统的负载情况是很重要的。\n任务执行时间过长：如果线程池中的任务执行时间过长，导致线程一直被占用，无法释放，也可能导致线程池的 OOM。在处理任务时，需要确保任务的执行时间合理，避免长时间占用线程。\n为了避免线程池的 OOM 问题，需要合理配置线程池的参数，并对任务执行的时间进行评估和优化。另外，及时关闭不再使用的线程池，释放资源，也是保证应用程序稳定性的重要措施。\n如何设定线程池的大小？ 很多人甚至可能都会觉得把线程池配置过大一点比较好！我觉得这明显是有问题的。就拿我们生活中非常常见的一例子来说：并不是人多就能把事情做好，增加了沟通交流成本。你本来一件事情只需要 3 个人做，你硬是拉来了 6 个人，会提升做事效率嘛？我想并不会。 线程数量过多的影响也是和我们分配多少人做事情一样，对于多线程这个场景来说主要是增加了上下文切换成本。不清楚什么是上下文切换的话，可以看我下面的介绍。\n上下文切换：\n多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。\n上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。\nLinux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。\n类比于实现世界中的人类通过合作做某件事情，我们可以肯定的一点是线程池大小设置过大或者过小都会有问题，合适的才是最好。\n如果我们设置的线程池数量太小的话，如果同一时间有大量任务/请求需要处理，可能会导致大量的请求/任务在任务队列中排队等待执行，甚至会出现任务队列满了之后任务/请求无法处理的情况，或者大量任务堆积在任务队列导致 OOM。这样很明显是有问题的，CPU 根本没有得到充分利用。 如果我们设置线程数量太大，大量线程可能会同时在争取 CPU 资源，这样会导致大量的上下文切换，从而增加线程的执行时间，影响了整体执行效率。 有一个简单并且适用面比较广的公式：\nCPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1。比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。 I/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。 线程池的状态有哪些？ 线程池主要有以下几种状态：\nRunning 运行状态：线程池创建后，初识状态为 Running 状态，表示线程池正在运行，可以接收任务。\nShutdown：调用线程池的 shutdown() 方法后，线程池进入 shutdown 状态，表示线程池已经不再接受新的任务提交，但是会执行完队列中已有的任务。\nStop 中断状态：调用线程池的 shutdownNow() 方法后，线程池进入 stop 状态，表示线程池将不再接受新的任务提交，并且会丢弃队列中未执行的任务，同时中断所有正在执行的任务。\nTidying 停止状态：线程池在执行完任务队列中的所有任务后会进入 tidying 状态，说明线程池要么已经变为停止状态，要么接收到了 shutdown() 方法的调用。\nTerminated 终止状态：线程池最终的状态是 Terminated，表示线程池已经彻底终止。\n线程池中的状态转换如下：\nRunning：线程池在（初始状态/创建）后进入 Running 状态 Running -\u0026gt; Shutdown：调用线程池的 shutdown() 方法，进入 Shutdown 状态 Running -\u0026gt; Stop：调用线程池的 shutdownNow() 方法，进入 Stop 状态 Shutdown -\u0026gt; Tidying：在未通过 shutdownNow() 方法调用停止线程池的情况下，所有任务都已完成，线程池将会进入 Tidying 状态 Stop -\u0026gt; Tidying：线程池已经全部终止，线程池将立即进入 Tidying 状态 Tidying -\u0026gt; Terminated：当线程池在 Tidying 状态下发现线程池中没有线程，那么线程池已经结束，将进入 Terminated 状态 使用多线程要注意哪些问题？ 避免死锁，保证数据的可见性或者多个线程对这个数据的一致性。\n线程安全问题：线程之间共享的变量可能会被多个线程同时访问，如果没有合适的控制，可能会出现数据不一致、死锁等问题。\n死锁问题：如果多个线程同时持有某些资源而互相等待释放对方所持有的资源，则可能会导致死锁。\n线程调度问题：线程调度是操作系统的工作，如果线程过多可能会影响系统的整体性能，因此需要考虑线程数量的合理分配和使用调度算法。\n线程上下文切换问题：线程切换需要花费一些时间，如果线程频繁切换，会增加系统开销。\n可读性和可维护性问题：多线程程序的代码比较复杂，可读性和可维护性较差，需要采用合适的编程模型和规范。\n性能优化问题：多线程程序需要考虑到并发和单线程间的性能差距，需要根据实际情况进行性能优化。\n调试难度问题：多线程程序的调试难度较大，需要使用适当的工具进行调试和排查问题。\n针对这些问题，需要采用相应的解决方案，如使用线程同步机制来解决线程安全问题，使用锁避免死锁问题，使用线程池进行线程调度，减少线程上下文切换，使用合适的编程模型来提高可读性和可维护性，采用优化技术提高程序性能，并且在编写代码时要遵循一定的规范和编程习惯，以方便后续的调试和维护。\n保证数据的一致性有哪些方案呢？ 比如有 violate 修饰一个变量，或者 sychonized 或者加锁。\n数据的一致性，即可见性\nvolatile 关键字修饰共享变量 srnchronized 给修改变量的代码块加锁，同步块中的共享变量都可以保持可见性 并发编程中，为了保证数据的一致性，可以采用以下几个方案：\n采用互斥锁或者其他同步机制：线程之间要想顺利地协作，就必须保证它们对共享变量的操作是有序的，通过 synchronized 关键字或者是 Lock 机制，可以保证临界区中的代码只被一个线程访问，从而保证共享资源的完整性和一致性。\n使用原子操作：Java 中的原子操作把多个非原子操作进行了封装，使得它们可以像原子操作一样被执行，而且不会被其他线程中断，从而保证了数据的一致性。原子操作包括了 AtomicInteger、AtomicLong、AtomicReference 等。\n采用读写锁：一般情况下，在多线程操作数据的时候，读的次数要比写的次数多，使用读写锁来优化性能，读写锁和互斥锁类似，不过在程序读共享资源的时候不需要加锁，因为多个线程同时读共享资源是不会产生问题的。只有当线程进行写操作的时候才会加写锁。\n利用 CAS 机制：CAS(Compare and Swap)机制是一种乐观锁，其基本思想是，先比较一下当前的值是不是自己想象的那个值，如果是，就执行操作，如果不是，就不再执行操作，直到期望的值和实际的值相同，CAS 操作才能成功。Java 中的 AtomicStampedReference 和 AtomicMarkableReference 就是使用 CAS 机制来保证数据的一致性。\n使用局部变量：当多个线程需要对同一个共享资源操作的时候，可以使用局部变量来减少对共享资源的访问次数，从而增加程序的效率，因为局部变量是线程私有的，多个线程之间不会产生数据一致性的问题。\n使用场景题 ","permalink":"https://lidengxm.github.io/posts/java/juc%E5%85%AB%E8%82%A1/","summary":"线程与进程篇 线程和进程的区别？ 在操作系统中，进程是指一个正在执行中的程序，而线程是进程的一部分，是一个程序中执行的代码片段。 进程是操作系统资源分配的最小单位，一个进程至少包括一个线程，进程拥有自己的内存空间、文件句柄、环境变量等系统资源。进程间相互独立，互不干扰，每个进程都拥有自","title":"JUC并发编程八股"},{"content":"这里是计算机网络 基础 OSI 七层模型是什么？X OSI 七层模型 是国际标准化组织提出一个网络分层模型，它将网络通信划分为七个不同的层次，每个层次都负责不同的功能。\n其大体结构以及每一层提供的功能如下图所示：\n计算机网络各层协议主要如下：\n应用层：HTTP、HTTPS、FTP、SMTP、POP3 等\n表示层：TLS/SSL（Transport Layer Security/Secure Sockets Layer）\n会话层：RPC（Remote Procedure Call）、NetBIOS（Network Basic Input/Output System）\n传输层：TCP、UDP\n网络层：IP、ICMP、ARP、RIP、OSPF 等\n数据链路层：PPP、HDLC、ARP、MAC 等\n物理层：无特定协议，主要涉及硬件、电气规范\n为什么要分层？\n每一层只专注于做一类事情。\nTCP/IP 网络模型有哪几层？ TCP/IP 四层模型 是目前被广泛采用的一种模型,我们可以将 TCP / IP 模型看作是 OSI 七层模型的精简版本，由以下 4 层组成：\n应用层 传输层 网络层 网络接口层 需要注意的是，我们并不能将 TCP/IP 四层模型 和 OSI 七层模型完全精确地匹配起来，不过可以简单将两者对应起来，如下图所示：\n浏览器输入一个网址后发生了什么？ X 当浏览器输入一个网址后，以下是发生的一系列步骤：\nDNS 解析=\u0026gt;建立 TCP 连接=\u0026gt;发起 HTTP 请求=\u0026gt;服务器处理请求=\u0026gt;服务器发送响应=\u0026gt;浏览器渲染页面=\u0026gt;关闭 TCP 连接\nDNS 解析：浏览器首先会将输入的网址发送给本地的 DNS 服务器，该服务器负责将网址解析为对应的 IP 地址。DNS 解析的过程中，会根据配置的缓存，查询本地缓存的 DNS 记录，如果没有找到，则继续向上级 DNS 服务器查询，直到找到对应的 IP 地址为止。\n建立 TCP 连接：**一旦浏览器获得了目标服务器的 IP 地址，它将使用 HTTP 协议通过 TCP/IP 建立到目标服务器的网络连接。**这个过程中会经历三次握手，确保浏览器和服务器之间的连接正常建立。\n发起 HTTP 请求：**一旦 TCP 连接建立，浏览器会使用 HTTP 协议向服务器发送请求。**请求的内容包括 HTTP 请求方法（GET、POST 等）、请求的 URL、请求头等。\n服务器处理请求：**目标服务器接收到请求后，会根据请求的 URL 和请求头等信息，进行相应的处理。**这可能包括读取数据库、处理业务逻辑等。\n服务器发送响应：**服务器处理请求后，会生成一个 HTTP 响应。**响应的内容包括状态码、响应头和响应体等信息。然后，服务器将该响应发送回浏览器。\n浏览器渲染页面：**浏览器接收到服务器的响应后，会根据响应头中的一些信息，对接收到的数据进行解析和处理。**浏览器会渲染 HTML、CSS 和 JavaScript 等资源，最终将页面呈现给用户。\n关闭 TCP 连接：**一旦页面加载完成，浏览器会关闭与服务器之间的 TCP 连接。**这个连接关闭的过程需要经历四次挥手，确保连接正常终止。\n描述一下页面敲了 http 请求，请求从前端到后端的过程 请求从前端到后端的过程主要涉及以下步骤：\n用户在前端浏览器中输入或点击页面的 URL 地址，触发 HTTP 请求。 浏览器根据 URL 确定请求的资源（页面或数据），构建 HTTP 请求报文。 浏览器向服务器发送 HTTP 请求报文。这个请求报文包含请求方法（GET、POST 等）、请求头（如 Content-Type、Cookie 等）、请求体（POST 请求的参数等）等信息。 通过网络，HTTP 请求报文从前端发送到服务器。 服务器接收到请求报文后，解析报文，根据其中的 URL 和其他信息进行处理和分析。 服务器根据处理的结果生成 HTTP 响应报文，包括状态码、响应头和响应体等内容。 通过网络，HTTP 响应报文从服务器发送到前端浏览器。 浏览器接收到 HTTP 响应报文后，解析报文并根据响应头的内容进行处理。 如果响应状态码是 200，表示请求成功，浏览器将会根据响应头的内容渲染页面或处理响应数据。如果状态码是其他的错误码，浏览器将根据错误码进行相应的处理（如重定向、显示错误页面等）。 前端浏览器根据需要显示页面或处理数据，进行页面渲染或动态交互等操作。 需要注意的是，上述过程是一个简化的描述，实际过程可能会更加复杂，涉及的步骤和处理细节也可能因具体场景而有所不同。另外，HTTP 请求和响应过程中还可能涉及到一些额外的操作，如请求的缓存、Cookie 的处理、会话管理等。\nDNS 协议？解析过程？ DNS 服务器专门保存了 Web 服务器域名与 IP 的对应关系，DNS 可以将域名网址自动转换为具体的 IP 地址。\n域名的层级关系\nDNS 中的域名都是用句点来分隔的，比如 www.server.com，这里的句点代表了不同层次之间的界限。\n在域名中，越靠右的位置表示其层级越高。根域是在最顶层，它的下一层就是 com 顶级域，再下面是 server.com。\n所以域名的层级关系类似一个树状结构：\n根 DNS 服务器 顶级域 DNS 服务器（com） 权威 DNS 服务器（server.com） DNS 域名解析的工作流程\n客户端首先会发出一个 DNS 请求，问 www.server.com 的 IP 是啥，并发给本地 DNS 服务器（也就是客户端的 TCP/IP 设置中填写的 DNS 服务器地址）。 本地域名服务器收到客户端的请求后，如果缓存里的表格能找到 www.server.com，则它直接返回 IP 地址。如果没有，本地 DNS 会去问它的根域名服务器：“老大， 能告诉我 www.server.com 的 IP 地址吗？” 根域名服务器是最高层次的，它不直接用于域名解析，但能指明一条道路。 根 DNS 收到来自本地 DNS 的请求后，发现后置是 .com，说：“www.server.com 这个域名归 .com 区域管理”，我给你 .com 顶级域名服务器地址给你，你去问问它吧。” 本地 DNS 收到顶级域名服务器的地址后，发起请求问“老二， 你能告诉我 www.server.com 的 IP 地址吗？” 顶级域名服务器说：“我给你负责 www.server.com 区域的权威 DNS 服务器的地址，你去问它应该能问到”。 本地 DNS 于是转向问权威 DNS 服务器：“老三，www.server.com对应的IP是啥呀？” server.com 的权威 DNS 服务器，它是域名解析结果的原出处。为啥叫权威呢？就是我的域名我做主。 权威 DNS 服务器查询后将对应的 IP 地址 X.X.X.X 告诉本地 DNS。 本地 DNS 再将 IP 地址返回客户端，客户端和目标建立连接。 DNS 域名解析的流程\n那是不是每次解析域名都要经过那么多的步骤呢？\n当然不是了，还有缓存这个东西的嘛。\n浏览器会先看自身有没有对这个域名的缓存，如果有，就直接返回，如果没有，就去问操作系统，操作系统也会去看自己的缓存，如果有，就直接返回，如果没有，再去 hosts 文件看，也没有，才会去问「本地 DNS 服务器」。\ncookie 和 session 的区别 这道面试题也是常见的面试题了，那我们要怎么回答呢？如果只是机械的背诵别人的答案不一定会获得面试官认可，那我们就先从 cookie 和 session 各自的故事开始吧\nsession 保存用户登录信息 我们先来回答一个问题：\n问：我们平时网站登录，那么多个用户同时进行登录访问，服务器是如何知道是哪个用户登录的呢？\n答：\n我们连接服务器后，会获得一个 Session 状态（匿名会话），然后返回给前端 当我们第一次登录成功后，自动生成 SessionID（每个 Session 都是独立的 Id），并且往该 SessionID 设置一个值（比如用户信息），返回 SessionID 给前端 前端接收到 SessionID，将其保存到 Cookie，然后 Cookie 会保存在本地/浏览器 我们再次向服务器发出相同的请求，会在请求头中**带上 Cookie ** 服务器从 Cookie 中得到 SessionID，确认该 SessionID 是之前已访问过的，直接从 SessionID 中取出设置的值（比如用户信息） 什么是 Cookie? 我们知道 HTTP 协议是无状态的，一次请求完成，不会持久化请求与相应的信息。那么，在购物车、用户登录状态、页面个性化设置等场景下，就无法识别特定用户的信息。这时 Cookie 就出现了。\nCookie 是客户端保存用户信息的一种机制，将服务器发送到浏览器的数据保存在本地，下次向同一服务器再发起请求时被携带发送。对于 Cookie，可以设置过期时间。\n通常，**Cookie 用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。**这样就解决了 HTTP 无状态的问题。\nCookie 主要用于以下方面：\n会话状态管理(如用户登录状态、购物车、游戏分数或其它需要记录的信息) 个性化设置(如用户自定义设置、主题等) 浏览器行为跟踪(如跟踪分析用户行为等) Cookie 存储在客户端，这就意味着，可以通过一些方式进行修改，欺骗服务器。\n针对这个问题，怎么解决呢?那就引入了 Session。\n什么是 Session? Session 代表服务器和客户端一次会话的过程。\n维基百科这样解释道：在计算机科学领域来说，尤其是在网络领域，会话(session)是一种持久网络协议，在用户(或用户代理)端和服务器端之间创建关联，从而起到交换数据包的作用机制，session 在网络协议(例如 telnet 或 FTP)中是非常重要的部分。\n对照 Cookie，Session 是一种在服务器端保存数据的机制，用来跟踪用户状态的数据结构，可以保存在文件、数据库或者集群中。\n当在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而会在整个用户会话中一直存在下去。当客户端关闭\n会话，或者 Session 超时失效时会话结束。\n目前大多数的应用都是用 Cookie 实现 Session 跟踪的。第一次创建 Session 时，服务端会通过在 HTTP 协议中返回给客户端，在 Cookie\n中记录 SessionID，后续请求时传递 SessionID 给服务，以便后续每次请求时都可分辨你是谁。\nCookie 与 Session 的区别 关于 Cookie 与 Session 的区别，就是在面试中经常回答的问题了。\n作用范围不同，Cookie 保存在客户端(浏览器)，Session 保存在服务器端。 存取方式的不同，Cookie 只能保存 ASCII，Session 可以存任意数据类型，比如 UserId 等。 有效期不同，Cookie 可设置为长时间保持，比如默认登录功能功能，Session 一般有效时间较短，客户端关闭或者 Session 超时都会失效。 隐私策略不同，Cookie 存储的内容可以被客户端浏览器修改，因此不适合存储敏感数据，Session 存储在服务端，相对安全一些，更适合存储敏感数据。 存储大小不同， 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie，可以存储几 MB 的数据 总的来说，Cookie 适合存储一些不敏感的数据，如用户的偏好设置等，而 Session 则适合存储一些敏感的数据，如用户的登录状态等。\n参考：\nhttps://www.51cto.com/article/679219.html https://articles.zsxq.com/id_wfouzza2xiz7.html HTTP 篇 HTTP 和 HTTPS 的区别 HTTP 和 HTTPS 协议的区别主要在如下几个方面：\n**HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。**HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。\n**HTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。**而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。\nURL 前缀：HTTP 的 URL 前缀是 http://，HTTPS 的 URL 前缀是 https://。\n**端口号：**HTTP 默认端口号是 80，HTTPS 默认端口号是 443。\n安全性和资源消耗：HTTP 协议运行在 TCP 之上，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份。HTTPS 是运行在 SSL/TLS 之上的 HTTP 协议，SSL/TLS 运行在 TCP 之上。所有传输的内容都经过加密，加密采用对称加密，但对称加密的密钥用服务器方的证书进行了非对称加密。所以说，HTTP 安全性没有 HTTPS 高，但是 HTTPS 比 HTTP 耗费更多服务器资源。\nSEO（搜索引擎优化）：搜索引擎通常会更青睐使用 HTTPS 协议的网站，因为 HTTPS 能够提供更高的安全性和用户隐私保护。使用 HTTPS 协议的网站在搜索结果中可能会被优先显示，从而对 SEO 产生影响。\n应用场景：\nHTTP 适用于对数据传输控制要求不高的应用场景，如浏览网页等；\nHTTPS 适用于需要保证数据传输安全性的应用场景，如银行网站、电商网站等。\nTLS 握手过程 TLS 握手需要做到：通信双方可以约定一个共同的加密方案（密钥），并且这个约定的过程（即 TLS 握手过程），即使被任何第三方窃听到，也无法解析出这个加密方案（密钥）。\n对称加密的特点是：约定一个共同的密钥，这个密钥可以用来加密数据，也可以用来解密数据，有点类似于上面提到的战场上的加密方案。\n非对称加密方案中，用户手握两把密钥，一把称为公钥，一把称为私钥，其中公/私钥都可以用来加密/解密数据，其特点为：用公钥加密后的数据，只有用私钥才能将其解开；用私钥加密后的数据，只有用公钥才能将其解开！\nTLS 握手过程：\n客户端向服务器发送 Client Hello 信息，告知自己想要建立一条 TLS 连接，并告知自己支持的加密算法。 服务器向客户端发送一个 Server Hello 的回应，并选择一个加密算法，同时给客户端发送自己的数字证书（包含服务器的公钥）。 客户端验证服务器发来的数字证书，验证通过后，在心里默默想出一个 pre-master 密钥（预主密钥），然后使用服务器的公钥，将预主密钥进行加密后，发送给服务器。 服务器用自己的私钥进行解密，得到预主密钥。 客户端和服务器都通过预主密钥，进行相同的计算后，得到后续通信时使用的对称加密密钥，称为 shared secret。 客户端和服务器端都分别用生成的 shared-secret 加密一段报文后，发送给对方，以验证对方能够成功收到信息并解密。 然后 TLS 就建立成功了，接下来双方都用这个 shared-secret 进行加密通信。\n总结一下，HTTPS 的加密过程中其实既用到了非对称加密也用到了对称加密，其中握手过程使用的是非对称加密，主要目的是双方可以安全的协商一个统一的密钥，而真正的数据传输过程则使用的是对称加密，正是使用刚才商量的这个密钥。\nHTTP 协议是哪一层的？作用呢？ HTTP（Hypertext Transfer Protocol）是应用层协议，是互联网上使用最广泛的协议之一。它基于客户端-服务器模型，通过在客户端和服务器之间传输文本数据来进行通信。HTTP 通常使用 TCP 作为传输层协议，也可以使用 TLS/SSL 进行加密。\n**HTTP 的作用是定义了客户端和服务器之间的通信方式，使得客户端可以向服务器请求资源，并且服务器可以向客户端发送响应结果。**HTTP 使用 URL（Uniform Resource Locator）来定位资源，通过请求方法（如 GET、POST、PUT、DELETE 等）来描述对资源的操作，通过请求头和响应头来传递附加信息，如编码格式、内容类型、Cookie 等。\nHTTP 协议主要作用包括：\n建立连接：客户端与服务器建立 TCP 连接，然后发送 HTTP 请求，服务器接收请求并处理。（三次握手建立连接） 发送请求：客户端发送 HTTP 请求到服务器，包括请求方法（GET、POST、PUT 等）、请求头（如 User-Agent、Accept 等）和请求正文（可选）等信息。 处理请求：服务器接收并解析 HTTP 请求，执行请求操作（如查询数据库等），并将处理结果返回给客户端。 返回响应：服务器返回 HTTP 响应，包括响应状态码（如 200 OK、404 Not Found 等）、响应头（如 Content-Type、Cache-Control 等）和响应正文（可选）等信息。 关闭连接：客户端接收到响应后，关闭 TCP 连接。（四次挥手关闭连接） HTTP 请求特点 HTTP 的主要特点包括以下几个方面：\n简单易用：HTTP 协议采用文本格式传输数据，易于人类阅读和编写，使用简单。 无状态：HTTP 是一种无状态协议，每次请求和响应之间相互独立，服务器不会保存任何客户端信息，客户端需要自行维护会话状态。 可扩展性：HTTP 允许通过扩展头部信息和请求方法等方式进行扩展，支持自定义数据传输格式和协议。 非连接型：HTTP 是一种非连接型协议，每个请求和响应之间相互独立，不存在长期的连接状态。 HTTP 协议是应用层协议中非常重要的一种，它的作用是为 Web 应用程序提供了标准的通信方式，使得客户端和服务器之间的交互变得更加简单、高效和灵活。\nHTTP 有哪些常见的状态码 状态码是服务器对客户端请求结果的反馈，根据状态码可以快速定位问题所在，进行相应的处理。\nHTTP（超文本传输协议）常见的状态码有以下几种：\n1xx（信息类状态码）：指示已经接收到请求，正在继续处理。\n2xx（成功状态码）：指示请求已经被接收、理解和接受。\n200 OK：请求已成功处理。 201 Created：请求已经被实现，而且有一个新的资源已经依据请求的需要而建立。 204 No Content：服务器已经成功处理了请求，但是没有返回任何实体内容。 3xx（重定向状态码）：需要进行附加操作以完成请求。\n301 Moved Permanently：请求的网页已永久移动到新位置。 302 Found：请求的网页已经临时移动到新位置。 304 Not Modified：客户端发送了一个带条件的请求，服务器端允许请求访问资源，但是请求未满足条件。 4xx（客户端错误状态码）：请求包含错误语法或不能被执行。\n400 Bad Request：请求报文存在语法错误。 401 Unauthorized：表示发送的请求需要有通过 HTTP 认证的认证信息。 403 Forbidden：表示对请求资源的访问被服务器拒绝。 404 Not Found：请求的资源不存在。 5xx（服务器错误状态码）：服务器在处理请求的过程中发生了错误。\n500 Internal Server Error：服务器遇到了一个未曾预料的状况，导致无法完成对请求的处理。 502 Bad Gateway：充当网关或代理的服务器，从远端服务器接收到了一个无效的请求。 503 Service Unavailable：服务器暂时处于超负载或正在停机维护，无法处理请求。 HTTP 常见字段有哪些？ Host 字段：客户端发送请求时，用来指定服务器的域名，有了 Host 字段，就可以将请求发往「同一台」服务器上的不同网站。\nConnection 字段\nConnection 字段最常用于客户端要求服务器使用「HTTP 长连接」机制，以便其他请求复用。\nHTTP 长连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。\nContent-Type 字段\nContent-Type 字段用于服务器回应时，告诉客户端，本次数据是什么格式。\nContent-Encoding 字段\nContent-Encoding 字段说明数据的压缩方法。表示服务器返回的数据使用了什么压缩格式\n常见的 HTTP 请求？ **HTTP（Hypertext Transfer Protocol）是用于在 Web 上传输数据的应用层协议。它定义了客户端和服务器之间进行通信的方式，包括发送请求并接收响应。**常见的 HTTP 请求方法（也称为 HTTP 动词）用于指定客户端想要执行的操作。以下是常见的 HTTP 请求方法：\nGET： **用于从服务器获取资源。**客户端发送 GET 请求以获取特定资源的数据。 POST： **用于向服务器提交数据。**客户端发送 POST 请求时，通常会附带数据，例如通过表单提交的用户输入。这个请求方法用于创建新的资源、提交表单数据等。 PUT： **用于更新服务器上的资源。**客户端发送 PUT 请求时，指定的数据将替换服务器上的相应资源。这个请求方法用于更新资源的全部内容。 PATCH： 与 PUT 类似，但用于部分更新资源。客户端发送 PATCH 请求时，只更新资源的一部分内容，而不是替换整个资源。 DELETE： **用于从服务器删除资源。**客户端发送 DELETE 请求以删除指定的资源。 OPTIONS： **获取服务器支持的通信选项。**客户端发送 OPTIONS 请求以获取服务器支持的请求方法、头部信息等。 这些 HTTP 请求方法用于不同的操作，允许客户端与服务器进行各种交互。每个请求方法都会在请求报文中明确指定，并告诉服务器客户端希望执行的操作。服务器在接收到请求后会根据请求方法和其他头部信息来处理请求，并返回相应的响应。\nHTTP 协议中 GET 和 POST 有什么区别？分别适用于什么场景？ 来自：编程导航官方\nHTTP 协议中 GET 和 POST 是两种常用的请求方法，它们的区别如下：\n参数传递方式不同 ：GET 请求参数是在 URL 中以键值对的形式传递的，例如：http://www.example.com/?key1=value1\u0026amp;key2=value2。 而 POST 请求参数是在请求体中以键值对的形式传递的。 参数传递大小不同 ：GET 请求参数有大小限制，因为 URL 长度有限制，不同的浏览器和服务器对 URL 长度的限制不同，一般为 2048 个字符。而 POST 请求参数没有大小限制，因为它们是以请求体的形式传递的。 安全性不同 ：GET 请求的参数是明文传输的，因为参数在 URL 中，如果涉及敏感信息（如密码），容易被窃取或暴露在浏览器历史记录、代理服务器日志等地方。而 POST 请求的参数在请求体中传输，相对安全一些，但是也需要注意参数加密和防止 CSRF 攻击等问题。 GET 和 POST 适用的场景不同：\n**GET 请求适用于获取数据，如浏览网页、搜索等。**因为 GET 请求参数以明文形式传输，容易被拦截和篡改，所以不适用于提交敏感信息的操作。 **POST 请求适用于提交数据，如登录、注册、发布内容等。**因为 POST 请求参数在请求体中传输，相对安全一些，可以提交敏感信息，但是需要注意参数加密和防止 CSRF 攻击等问题。 根据 RFC 规范，GET 的语义是从服务器获取指定的资源，这个资源可以是静态的文本、页面、图片视频等。GET 请求的参数位置一般是写在 URL 中，URL 规定只能支持 ASCII，所以 GET 请求的参数只允许 ASCII 字符 ，而且浏览器会对 URL 的长度有限制（HTTP 协议本身对 URL 长度并没有做任何规定）。\n根据 RFC 规范，POST 的语义是根据请求负荷（报文 body）对指定的资源做出处理，具体的处理方式视资源类型而不同。POST 请求携带数据的位置一般是写在报文 body 中，body 中的数据可以是任意格式的数据，只要客户端与服务端协商好即可，而且浏览器不会对 body 大小做限制。\nHTTP1.0 和 HTTP2.0 有什么区别？ HTTP 1.0 和 HTTP 2.0 是两个不同版本的超文本传输协议（HTTP），用于在客户端和服务器之间传输网络资源（例如网页、图片等）。它们有一些重要的区别，以下是其中一些主要区别：\n性能优化：\nHTTP 1.0： 在 HTTP 1.0 中，每个请求都需要使用一个单独的 TCP 连接。这会导致\u0026quot;队头阻塞\u0026quot;问题，即在一个请求等待响应时，后续的请求必须等待。 HTTP 2.0： HTTP 2.0 引入了多路复用，允许多个请求和响应在同一个 TCP 连接中同时传输。这解决了队头阻塞问题，提高了并发性能。 头部压缩：\nHTTP 1.0： 在每个请求和响应中，头部信息（例如 cookies、User-Agent 等）都会以明文形式传输，导致额外的数据传输和延迟。 HTTP 2.0： HTTP 2.0 使用了 HPACK 压缩算法，对头部信息进行压缩，减少了数据传输量，提高了性能。 二进制传输：\nHTTP 1.0： 数据在文本形式下传输，包括头部和正文，可能会导致解析和传输的开销。 HTTP 2.0： 数据以二进制格式传输，更紧凑且高效，减少了解析开销。 服务器推送：\nHTTP 1.0： 服务器无法主动向客户端推送资源，而必须等待客户端请求。 HTTP 2.0： HTTP 2.0 支持服务器推送，服务器可以在没有显式请求的情况下向客户端推送资源，提高了加载速度。 流量控制：\nHTTP 1.0： 缺乏有效的流量控制机制，容易出现过载和资源浪费。 HTTP 2.0： 引入了流量控制机制，允许接收方控制数据的传输速率，避免了过载问题。 优先级设置：\nHTTP 1.0： 请求无法明确设置优先级，可能导致关键请求被延迟处理。 HTTP 2.0： 支持请求和响应的优先级设置，确保关键资源得到更快的响应。 总体而言，HTTP 2.0 在性能、效率和功能方面相较于 HTTP 1.0 有着明显的改进。然而，需要注意的是，虽然大多数现代浏览器和服务器都支持 HTTP 2.0，但在某些情况下，如特定网络配置，一些优化可能并不明显。\nTCP 篇 什么是 TCP ？ TCP 是面向连接的、可靠的、基于字节流的传输层通信协议\n面向连接：一定是「一对一」才能连接，不能像 UDP 协议可以一个主机同时向多个主机发送消息，也就是一对多是无法做到的； 可靠的：无论的网络链路中出现了怎样的链路变化，TCP 都可以保证一个报文一定能够到达接收端； 字节流：用户消息通过 TCP 协议传输时，消息可能会被操作系统「分组」成多个的 TCP 报文，如果接收方的程序如果不知道「消息的边界」，是无法读出一个有效的用户消息的。并且 TCP 报文是「有序的」，当「前一个」TCP 报文没有收到的时候，即使它先收到了后面的 TCP 报文，那么也不能扔给应用层去处理，同时对「重复」的 TCP 报文会自动丢弃 TCP 头格式有哪些？ TCP 头部有哪些字段？\nTransmission Control Protocol (TCP) 是一种常用的网络传输协议，用于在计算机之间建立可靠的连接并进行数据传输。TCP 首部包含了多个字段，用于管理和控制数据传输。\nTCP 头部字段：\n序列号：在建立连接时由计算机生成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送一次数据，就「累加」一次该「数据字节数」的大小。用来解决网络包乱序问题。\n确认应答号：指下一次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。用来解决丢包的问题。\n控制位：\nACK：该位为 1 时，「确认应答」的字段变为有效，TCP 规定除了最初建立连接时的 SYN 包之外该位必须设置为 1 。 RST：该位为 1 时，表示 TCP 连接中出现异常必须强制断开连接。 SYN：该位为 1 时，表示希望建立连接，并在其「序列号」的字段进行序列号初始值的设定。 FIN：该位为 1 时，表示今后不会再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双方的主机之间就可以相互交换 FIN 位为 1 的 TCP 段 源端口号 (Source Port): 16 位，表示发送端口号。 目标端口号 (Destination Port): 16 位，表示接收端口号。 序列号 (Sequence Number): 32 位，用于标识发送的数据字节在整个数据流中的位置。 确认号 (Acknowledgment Number): 32 位，用于确认已经收到的数据字节序列号。 数据偏移 (Data Offset): 4 位，表示 TCP 首部的长度，以 32 位字（4 字节）为单位。由于这个字段的最小值是 5，所以 TCP 首部的最小长度是 20 字节。 保留 (Reserved): 6 位，保留供将来使用，目前应设置为 0。 控制位 (Flags): 6 位，用于指示 TCP 数据包的不同状态和控制选项，如下： URG (Urgent): 紧急指针是否有效。 ACK (Acknowledgment): 确认号是否有效。 PSH (Push): 表示接收方应该尽快将数据交给应用层，而不是等待缓冲区填满。 RST (Reset): 重置连接。 SYN (Synchronize): 同步序列号，用于建立连接。 FIN (Finish): 表示发送方已经发送完数据。 窗口大小 (Window Size): 16 位，指示发送方可以接收的字节数量，用于流量控制。 校验和 (Checksum): 16 位，用于检测首部和数据在传输过程中是否出现错误。 紧急指针 (Urgent Pointer): 16 位，仅在 URG 标志位设置时才有效，用于指示紧急数据的结束位置。 选项 (Options): 可变长度，提供一些额外的控制和管理选项，如最大报文段长度 (MSS)、时间戳等。 填充 (Padding): 用于确保首部长度为 32 位的整数倍，从而对齐数据。 TCP 和 UDP 协议的区别？ X TCP 和 UDP 协议的区别主要在如下几个方面：\n是否面向连接：UDP 在传送数据之前不需要先建立连接。而 TCP 提供面向连接的服务，在传送数据之前必须先建立连接，数据传送结束后要释放连接。 是否是可靠传输：远地主机在收到 UDP 报文后，不需要给出任何确认，并且不保证数据不丢失，不保证是否顺序到达。TCP 提供可靠的传输服务，TCP 在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制。通过 TCP 连接传输的数据，无差错、不丢失、不重复、并且按序到达 是否有状态：这个和上面的“是否可靠传输”相对应。TCP 传输是有状态的，这个有状态说的是 TCP 会去记录自己发送消息的状态比如消息是否发送了、是否被接收了等等。为此 ，TCP 需要维持复杂的连接状态表。而 UDP 是无状态服务，简单来说就是不管发出去之后的事情了（这很渣男！）。 传输方式：TCP 是基于字节流的传输方式，UDP 是基于数据报的传输方式。 首部开销：TCP 首部开销（20 ～ 60 字节）比 UDP 首部开销（8 字节）要大。 传输效率：由于使用 TCP 进行传输的时候多了连接、确认、重传等机制，所以 TCP 的传输效率要比 UDP 低很多。 连接数：TCP 连接个数有限制，UDP 没有连接数限制。 拥塞控制、流量控制 TCP 有拥塞控制和流量控制机制，保证数据传输的安全性。 UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。 TCP 和 UDP 协议的应用场景：\nUDP 一般用于即时通信，比如：语音、 视频、直播等等。这些场景对传输数据的准确性要求不是特别高，比如你看视频即使少个一两帧，实际给人的感觉区别也不大。\nTCP 用于对传输准确性要求特别高的场景，比如文件传输 FTP、发送和接收邮件、远程登录等等\nUDP 不提供复杂的控制机制，利用 IP 提供面向「无连接」的通信服务。\nUDP 协议真的非常简，头部只有 8 个字节（64 位）\n**TCP 传输控制协议，是面向连接的，基于字节流，**基于 TCP 发送数据的时候，如果没有按时到达接收端，会超时重传，因此可靠性高，但是效率低，适合用于对实时性要求不高，但是对准确性要求高的场景，比如邮件传输。\n**UDP 是用户数据报协议，是无连接的，基于数据报，**基于 UDP 发送数据的时候，目的端如果没有按时收到数据，会直接丢弃，因此可靠性较低，但是效率高，**适应于对实时性要求高，但是对准确性要求不高的场景，比如网络通话。**UDP 对所需资源也比较少，TCP 所需资源比较多。\n—— 顺便回忆一下操作系统的知识：协程是用户级的线程，是线程内部调度的基本单位，一个线程可以拥有多个协程。\nTCP 和 UDP 是计算机网络中两种常用的传输层协议，用于实现可靠传输和无连接传输。 TCP（Transmission Control Protocol）是一种面向连接的、可靠的传输协议。它通过三次握手四次挥手进行连接和断开链接，保证数据的可靠性、完整性和顺序性，具有较高的传输效率。TCP 协议适用于要求可靠传输的场景，如文件传输、电子邮件传输等。 TCP 协议的工作流程如下：\n客户端向服务器发送连接请求（SYN）。 服务器收到连接请求后，回复确认请求（SYN+ACK）。 客户端收到确认请求后，回复确认（ACK），完成连接。 数据传输完成后，客户端和服务器分别发送关闭连接请求（FIN）。 对方收到关闭请求后，回复确认（ACK）。 双方都收到对方的关闭请求和确认后，关闭连接。 UDP（User Datagram Protocol）是一种无连接的、不可靠的传输协议。它不需要建立连接和维护连接状态，具有较高的传输速度和实时性，但不保证数据的完整性和顺序性。UDP 协议适用于实时性要求高、数据量小、丢失数据不会影响结果的场景，如视频直播、语音通话等。 UDP 协议工作流程：\n客户端向服务器发送数据报。 服务器收到数据报后，直接处理数据并回复确认。 客户端收到确认后，继续发送下一个数据报。 如果数据报丢失或损坏，客户端不会重传，而是直接忽略。 TCP 三次握手四次挥手过程？为什么需要？ TCP（Transmission Control Protocol）是一种面向连接的协议，为了保证数据传输的可靠性，TCP 使用了三次握手和四次挥手的过程。\nACK (Acknowledgment): 确认号是否有效。 SYN (Synchronize): 同步序列号，用于建立连接。 FIN (Finish): 表示发送方已经发送完数据。 TCP 需要三次握手建立连接和四次挥手断开连接的原因如下：\n**三次握手建立连接：**三次握手用于建立一个可靠的连接，确保双方都准备好进行数据的传输。\n**第一次握手（SYN）：**客户端向服务端发送一个 SYN （同步）报文。此时客户端处于 SYN_SENT 状态，等待服务端确认。\n**第二次握手（SYN+ACK）：**服务端收到客户端的 SYN 报文后，给客户端回复一个 SYN+ACK（确认） 报文。此时服务端处于 SYN_RECV 状态。\n**第三次握手（ACK）：**客户端收到服务端的 SYN+ACK 报文后，向服务端发送一个 ACK 报文。此时客户端处于 ESTABLISHED 状态，服务端收到客户端的 ACK 报文后，也处于 ESTABLISHED 状态。连接建立成功。\n**为什么需要三次握手？**三次握手的目的是为了确认双方的收发能力和同步初始序列号，保证数据不丢失。\n四次挥手断开连接：\n**第一次挥手（FIN）：**客户端向服务端发送一个 FIN（结束） 报文。此时客户端处于 FIN_WAIT1 状态。\n**第二次挥手（ACK）**服务端收到客户端的 FIN 报文后，向客户端回复一个 ACK（确认） 报文。此时服务端处于 CLOSE_WAIT 状态。\n**第三次挥手（FIN）：**如果服务端也想关闭连接，就向客户端发送 FIN 报文。此时服务端处于 LAST_ACK 状态，等待客户端响应。\n**第四次挥手（ACK）：**客户端收到服务端的 FIN 报文后，向服务端发送一个 ACK 报文。此时客户端处于 TIME_WAIT 状态，等待 2MSL 过后关闭连接。服务端收到客户端的 ACK 报文后，也关闭连接。\n**为什么需要四次挥手？**四次挥手的目的是为了保证数据的完整性和可靠性。在关闭连接之前，双方需要确保所有数据都已经传输完毕，因此需要通过四次挥手的过程进行确认和处理。\n总结：三次握手的本质是确认通信双方收发数据的能力 ，四次挥手的目的是关闭一个连接。\nTCP 如何保证数据有序性和可靠性？ TCP（Transmission Control Protocol）通过一系列的机制来保证数据的有序性和可靠性，这使得数据能够按照发送的顺序到达目标。以下是 TCP 保证数据有序性和可靠性的主要机制：\n序列号和确认号： 每个 TCP 数据包都有一个唯一的序列号，用于标识发送端发送的数据字节在整个数据流中的位置。接收端使用确认号来告知发送端已成功接收到哪些数据字节，从而使得发送端可以追踪数据的传输情况。\n确认机制： TCP 使用确认机制来确保数据的可靠性。接收方收到数据后，会向发送方发送确认（ACK）报文，其中包含确认号，表示已经成功接收到指定序列号之前的数据。如果发送方一段时间内没有收到确认，它会假定数据丢失，并重新发送未确认的数据。\n超时与重传： 如果发送方发送了数据但没有及时收到确认，它会假设数据可能丢失，因此会启动超时计时器。一旦计时器超时，发送方会重新发送丢失的数据。这个机制确保了即使部分数据包丢失，数据也能最终被成功传输。\n流量控制： TCP 使用滑动窗口机制进行流量控制。接收方可以通过通知发送方自己的可用缓冲区大小来限制发送的速率，从而避免发送方发送过多数据导致接收方无法处理。\n拥塞控制： TCP 使用拥塞控制算法来避免网络拥塞。当网络拥塞时，数据包可能会丢失，因此发送方会减慢发送速率，以便适应网络的负载情况，从而降低拥塞程度。\n顺序交付： TCP 保证数据按照发送的顺序到达接收方。接收方使用序列号来重新排序接收到的数据，以确保数据按正确的顺序传递给应用层。\n连接管理： TCP 使用三次握手建立连接，并使用四次挥手终止连接。这些机制确保在数据传输之前和之后都有合适的握手和挥手过程，从而保证数据在正确的连接状态下传输。\n综合使用上述机制，TCP 能够在不可靠的网络环境中提供高度可靠的数据传输，确保数据的有序性和完整性。然而，需要注意的是，虽然 TCP 是一种可靠的协议，但也并不是绝对无故障的，某些情况下仍然可能出现数据丢失或乱序等问题。\nTCP 如何实现流量控制？ 参考：小林 Coding\nTCP 利用滑动窗口实现流量控制。流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。\n为什么需要流量控制? 这是因为双方在通信的时候，发送方的速率与接收方的速率是不一定相等，如果发送方的发送速率太快，会导致接收方处理不过来。如果接收方处理不过来的话，就只能把处理不过来的数据存在 接收缓冲区(Receiving Buffers) 里（失序的数据包也会被存放在缓存区里）。如果缓存区满了发送方还在狂发数据的话，接收方只能把收到的数据包丢掉。出现丢包问题的同时又疯狂浪费着珍贵的网络资源。因此，我们需要控制发送方的发送速率，让接收方与发送方处于一种动态平衡才好。\nTCP 和 IP 协议的区别 TCP（Transmission Control Protocol）和 IP（Internet Protocol）是两个互联网协议族中的不同成员，它们在互联网通信中扮演着不同的角色。\n以下是 TCP 和 IP 协议之间的主要区别：\n功能和作用：\nTCP（Transmission Control Protocol）： TCP 是一种面向连接、可靠的传输协议。它负责数据的分段、传输、序列化、确认和流量控制，以确保数据在网络中有序、完整地传输。 IP（Internet Protocol）： IP 是一种网络层协议，用于在网络中寻址和路由数据包。它主要负责数据包的分组、寻址和路由，以便将数据从源主机传输到目标主机。 层次和分工：\nTCP： TCP 是运行在传输层（第四层）的协议，位于应用层（第七层）和网络层（第三层）之间。它负责在应用程序之间建立可靠的连接，以实现数据传输的可靠性和有序性。 IP： IP 是运行在网络层（第三层）的协议，负责寻址和路由，将数据包从源主机发送到目标主机。IP 协议定义了如何将数据包传递到目标网络，并由路由器根据目标地址选择合适的路径进行转发。 连接性和可靠性：\nTCP： TCP 是面向连接的协议，意味着在数据传输之前，发送方和接收方需要建立连接。TCP 提供了可靠的数据传输机制，通过序列号、确认和重传来确保数据的完整性和有序性。 IP： IP 是无连接的协议，不维护连接状态。它主要关注数据包的路由和传输，但不保证数据包的可靠性或有序性。 头部信息：\nTCP： TCP 首部包含了用于管理数据传输的各种控制信息，如序列号、确认号、窗口大小等。 IP： IP 首部包含了源地址和目标地址等关于数据包的路由信息。 可靠性和效率的权衡：\nTCP： 由于 TCP 提供了可靠的传输，它会引入一些开销，如确认和重传机制。这使得 TCP 适用于需要确保数据完整性和顺序性的应用，如文件传输、电子邮件等。 IP： IP 更专注于尽快传递数据包，而不关心数据包的丢失或重复。这使得 IP 适用于需要更高传输效率的应用，如实时流媒体、VoIP 等。 综上所述，TCP 和 IP 是互联网协议族中的两个不同层次的协议，它们共同工作以实现可靠的数据传输和网络通信。TCP 提供了可靠的传输机制，而 IP 则负责路由和分组传输。\n滑动窗口和拥塞策略是什么？ 滑动窗口和拥塞策略都是与数据传输和网络通信相关的概念，用于在数据传输过程中进行流量控制和拥塞管理。它们在不同层次上影响数据的传输效率和网络的稳定性。\n滑动窗口（Sliding Window）： 滑动窗口是一种用于流量控制的机制，特别是在基于连接的协议（如 TCP）中。它允许发送方在未收到确认前发送一定数量的数据，而不需要等待每个数据包的确认。滑动窗口的大小是动态变化的，取决于网络的情况和接收方的能力。发送方通过调整窗口大小来控制发送的数据量，从而避免在网络中引入过多的未确认数据，同时充分利用网络的带宽。\n滑动窗口的主要目标是在保持网络链路的高效利用的同时，防止因发送太多未确认的数据导致接收方无法处理。发送方根据接收方的确认来调整窗口的大小，以确保数据的可靠传输和合理的流量控制。\n拥塞策略（Congestion Control）： 拥塞策略是一种管理网络中拥塞（即网络中的流量超出其处理能力）的方法。拥塞可能导致数据丢失、延迟增加和网络性能下降。拥塞策略旨在减轻拥塞，以保持网络的稳定性和性能。\n常见的拥塞策略包括：\n慢开始和拥塞避免： 这是 TCP 拥塞控制的基础。发送方在开始发送数据时，逐渐增加发送窗口的大小，以测试网络的容量。一旦网络出现拥塞，发送方将进入拥塞避免阶段，每个往返时间内只增加一个窗口大小，以避免引入过多的数据。\n快重传和快恢复： 如果接收方连续收到相同的序列号的数据报文（表明前一个数据包丢失），它可以触发快重传，发送一个重复确认。发送方在收到连续的重复确认后，可以不必等待超时就知道哪个数据包丢失，从而更快地进行重传。\n拥塞窗口调整： 当网络出现拥塞时，发送方会减小发送窗口的大小，从而降低发送速率，以适应网络的负载。\n随机早期检测（RED）： 一种队列管理算法，用于控制路由器的输入队列中的数据包数量，以防止拥塞。它基于队列长度调整数据包的丢弃率，从而在网络拥塞时避免进一步加剧拥塞。\n综合而言，滑动窗口和拥塞策略都是网络通信中的重要概念，用于在数据传输过程中确保数据的可靠性、有序性和网络的稳定性。\nTCP 的拥塞控制是怎么实现的？ 在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞。拥塞控制就是为了防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机，所有的路由器，以及与降低网络传输性能有关的所有因素。相反，流量控制往往是点对点通信量的控制，是个端到端的问题。流量控制所要做到的就是抑制发送端发送数据的速率，以便使接收端来得及接收。\nTCP 的拥塞控制\n为了进行拥塞控制，TCP 发送方要维持一个 拥塞窗口(cwnd) 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。\nTCP 的拥塞控制采用了四种算法，即 慢开始、 拥塞避免、快重传 和 快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理 AQM），以减少网络拥塞的发生。\n慢开始： 慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd 初始值为 1，每经过一个传播轮次，cwnd 加倍。 拥塞避免： 拥塞避免算法的思路是让拥塞窗口 cwnd 缓慢增大，即每经过一个往返时间 RTT 就把发送方的 cwnd 加 1. 快重传与快恢复： 在 TCP/IP 中，快速重传和恢复（fast retransmit and recovery，FRR）是一种拥塞控制算法，它能快速恢复丢失的数据包。没有 FRR，如果数据包丢失了，TCP 将会使用定时器来要求传输暂停。在暂停的这段时间内，没有新的或复制的数据包被发送。有了 FRR，如果接收机接收到一个不按顺序的数据段，它会立即给发送机发送一个重复确认。如果发送机接收到三个重复确认，它会假定确认件指出的数据段丢失了，并立即重传这些丢失的数据段。有了 FRR，就不会因为重传时要求的暂停被耽误。 当有单独的数据包丢失时，快速重传和恢复（FRR）能最有效地工作。当有多个数据信息包在某一段很短的时间内丢失时，它则不能很有效地工作 其他 抓包验证协议？ 抓包验证协议是一种通过监视网络通信流量来分析和验证网络协议的方法。抓包工具允许您捕获进出网络设备（如计算机、服务器、路由器等）的数据包，并分析这些数据包以了解通信过程、协议使用情况以及可能出现的问题。以下是抓包验证协议的一般步骤：\n选择抓包工具： 选择合适的网络抓包工具。一些常见的抓包工具包括 Wireshark、Tcpdump、Fiddler、Charles 等。\n安装和配置： 下载并安装所选工具，然后根据需要配置网络适配器或代理设置。确保工具能够在您所关心的网络环境中正常工作。\n选择捕获接口： 打开抓包工具，选择要捕获的网络接口。这可以是您的网络适配器、虚拟网络接口、特定的端口等。\n开始捕获： 启动捕获过程，抓包工具会开始监听选定的接口，并记录所有进出的数据包。\n执行协议操作： 执行涉及您关心的协议的操作，例如打开网页、发送请求、执行登录等。\n分析捕获结果： 停止捕获过程后，抓包工具会显示捕获的数据包列表。您可以通过过滤、排序和搜索来分析这些数据包。重点关注您所感兴趣的协议的通信过程，查看请求和响应、协议头部、数据内容等。\n验证协议： 基于捕获结果，验证协议是否按预期工作。检查协议规范是否得到遵循，检测潜在的错误、丢包、延迟等问题。\n排除问题： 如果发现了协议问题，分析数据包可能有助于确定问题的原因。您可以检查是否有错误的请求、响应状态码、头部信息错误等。\n优化性能： 如果您的目标是优化协议性能，抓包也可以帮助您识别瓶颈、延迟等问题，并采取相应的优化措施。\n请注意，抓包需要合法的网络权限，并且可能涉及敏感数据的传输。在使用抓包工具时，确保您遵循适当的法律和隐私规定，不要获取未经授权的数据。\n下面是操作系统部分啦 操作系统主要有哪些功能？ 从资源管理的角度来看，操作系统有 6 大功能：\n进程和线程的管理：进程的创建、撤销、阻塞、唤醒，进程间的通信等。 存储管理：内存的分配和管理、外存（磁盘等）的分配和管理等。 文件管理：文件的读、写、创建及删除等。 设备管理：完成设备（输入输出设备和外部存储设备等）的请求或释放，以及设备启动等功能。 网络管理：操作系统负责管理计算机网络的使用。网络是计算机系统中连接不同计算机的方式，操作系统需要管理计算机网络的配置、连接、通信和安全等，以提供高效可靠的网络服务。 安全管理：用户的身份认证、访问控制、文件加密等，以防止非法用户对系统资源的访问和操作。 为什么要使用多线程? 先从总体上来说：\n从计算机底层来说： 线程可以比作是轻量级的进程，是程序执行的最小单位,线程间的切换和调度的成本远远小于进程。另外，多核 CPU 时代意味着多个线程可以同时运行，这减少了线程上下文切换的开销。 从当代互联网发展趋势来说： 现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。 再深入到计算机底层来探讨：\n单核时代：在单核时代多线程主要是为了提高单进程利用 CPU 和 IO 系统的效率。 假设只运行了一个 Java 进程的情况，当我们请求 IO 的时候，如果 Java 进程中只有一个线程，此线程被 IO 阻塞则整个进程被阻塞。CPU 和 IO 设备只有一个在运行，那么可以简单地说系统整体效率只有 50%。当使用多线程的时候，一个线程被 IO 阻塞，其他线程还可以继续使用 CPU。从而提高了 Java 进程利用系统资源的整体效率。 多核时代: 多核时代多线程主要是为了提高进程利用多核 CPU 的能力。举个例子：假如我们要计算一个复杂的任务，我们只用一个线程的话，不论系统有几个 CPU 核心，都只会有一个 CPU 核心被利用到。而创建多个线程，这些线程可以被映射到底层多个 CPU 上执行，在任务中的多个线程没有资源竞争的情况下，任务执行的效率会有显著性的提高，约等于（单核时执行时间/CPU 核心数） 线程间的同步的方式有哪些？ 线程同步是两个或多个共享关键资源的线程的并发执行。应该同步线程以避免关键的资源使用冲突。\n下面是几种常见的线程同步的方式：\n互斥锁(Mutex)：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。比如 Java 中的 synchronized 关键词和各种 Lock 都是这种机制。 读写锁（Read-Write Lock）：允许多个线程同时读取共享资源，但只有一个线程可以对共享资源进行写操作。 信号量(Semaphore)：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量。 屏障（Barrier）：屏障是一种同步原语，用于等待多个线程到达某个点再一起继续执行。当一个线程到达屏障时，它会停止执行并等待其他线程到达屏障，直到所有线程都到达屏障后，它们才会一起继续执行。比如 Java 中的 CyclicBarrier 是这种机制。 事件(Event) :Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作 ","permalink":"https://lidengxm.github.io/posts/java/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%85%AB%E8%82%A1/","summary":"这里是计算机网络 基础 OSI 七层模型是什么？X OSI 七层模型 是国际标准化组织提出一个网络分层模型，它将网络通信划分为七个不同的层次，每个层次都负责不同的功能。 其大体结构以及每一层提供的功能如下图所示： 计算机网络各层协议主要如下： 应用层：HTTP、HTTPS、FTP、SMTP、POP3 等 表示","title":"计算机基础八股"},{"content":"框架面试题合集 spring 当涉及校招应届生的 Spring 框架相关面试题时，通常会着重考察基础知识和理解能力。以下是一些常见的 Spring 相关面试题，希望对你有所帮助：\n什么是 Spring 框架？\n解释 Spring 框架的概念、作用和特点。 Spring 框架的核心模块是什么？\n提及 Spring 框架的核心模块，如 Spring Core、Spring Context、Spring AOP 和 Spring Beans。 什么是 IoC（控制反转）和 DI（依赖注入）？\n解释 IoC 和 DI 的概念，以及它们在 Spring 中的实现。 Spring 中的 Bean 是什么？\n说明 Spring 中的 Bean 是如何定义和管理的。 如何在 Spring 中进行 Bean 的装配？\n解释通过 XML 配置文件、注解或 Java 配置类来实现 Bean 的装配。 Spring 的 AOP 是什么？\n介绍 Spring AOP 的概念和应用场景。 在 Spring AOP 中，切面、连接点和通知分别是什么？\n解释 AOP 中的这些概念，以及它们之间的关系。 Spring 中的事务管理是怎样实现的？\n解释 Spring 中的事务管理机制，并说明如何配置和使用事务。 Spring 中的 MVC 是什么？\n介绍 Spring MVC 的概念和工作原理。 Spring Boot 和 Spring 框架有什么区别？\n比较 Spring Boot 和传统 Spring 框架的特点和使用场景。 Spring Security 是做什么的？\n解释 Spring Security 的作用和目的。 什么是 Spring Bean 的作用域？\n解释 Spring Bean 的单例、原型、会话和请求等作用域。 如何解决 Spring 中的循环依赖问题？\n说明 Spring 是如何处理循环依赖的。 Spring 中的异常处理机制是什么？\n解释 Spring 中的异常处理方式，例如使用 @ControllerAdvice 和 @ExceptionHandler。 如何集成 Spring 和其他技术？\n举例说明如何集成 Spring 和数据库、消息队列、缓存等技术。 这些问题涵盖了 Spring 框架的一些核心概念和常见用法。在准备面试时，建议深入学习 Spring 框架的基础知识，并通过实践来加深理解。此外，还可以尝试解决一些与 Spring 相关的小项目或实际问题，以加强自己在面试中的表现。祝你面试顺利！\nspringmvc 当涉及校招应届生的 Spring MVC 框架相关面试题时，通常会重点考察基础知识和理解能力。以下是一些常见的 Spring MVC 相关面试题，希望对你有所帮助：\n什么是 Spring MVC 框架？\n解释 Spring MVC 框架的概念、作用和特点。 Spring MVC 的主要组件是什么？\n提及 Spring MVC 的主要组件，如 DispatcherServlet、HandlerMapping、Controller、ViewResolver 等。 Spring MVC 的工作原理是什么？\n说明客户端请求到达服务器并经过 Spring MVC 框架处理的流程。 Spring MVC 中的 DispatcherServlet 是什么？\n解释 DispatcherServlet 的作用和工作方式。 如何在 Spring MVC 中定义 Controller？\n介绍在 Spring MVC 中定义 Controller 的几种方式，如使用注解、实现接口等。 Spring MVC 中的请求映射是怎样实现的？\n解释请求映射的方式，如使用注解、XML 配置等。 Spring MVC 中的数据绑定是什么？\n解释数据绑定的概念，以及如何将请求参数绑定到方法参数或模型属性。 如何处理 Spring MVC 中的表单提交？\n说明如何在 Controller 中处理表单提交，并如何验证表单数据。 Spring MVC 中的视图解析器是什么？\n解释视图解析器的作用，以及如何配置和使用它。 什么是拦截器（Interceptor）？\n解释拦截器的概念和作用，以及如何在 Spring MVC 中配置和使用拦截器。 Spring MVC 的异常处理机制是什么？\n解释 Spring MVC 如何处理控制器中的异常，并如何自定义异常处理。 如何使用 RESTful 风格的 API 开发？\n说明如何在 Spring MVC 中设计和实现 RESTful 风格的 API。 Spring MVC 中的文件上传是怎样实现的？\n解释 Spring MVC 如何处理文件上传，并如何配置和使用文件上传功能。 如何进行 Spring MVC 的单元测试？\n说明如何编写和执行 Spring MVC 控制器的单元测试。 Spring MVC 和 Spring Boot 有什么区别？\n比较 Spring MVC 和 Spring Boot 的特点和使用场景。 这些问题涵盖了 Spring MVC 框架的一些核心概念和常见用法。在准备面试时，建议深入学习 Spring MVC 框架的基础知识，并通过实践来加深理解。还可以尝试解决一些与 Spring MVC 相关的小项目或实际问题，以加强自己在面试中的表现。祝你面试顺利！\nspringboot 当涉及校招应届生的 Spring Boot 框架相关面试题时，通常会考察基础知识和对 Spring Boot 的理解。以下是一些常见的 Spring Boot 相关面试题，希望对你有所帮助：\n什么是 Spring Boot？\n解释 Spring Boot 的概念、作用和特点。 Spring Boot 的主要优点是什么？\n介绍使用 Spring Boot 的好处，如简化配置、快速开发等。 Spring Boot 中的自动配置是什么？\n解释 Spring Boot 自动配置的原理和作用。 如何创建一个 Spring Boot 项目？\n说明如何使用 Spring Initializr 创建一个新的 Spring Boot 项目。 如何配置 Spring Boot 项目的属性？\n介绍如何使用 application.properties 或 application.yml 文件来配置项目属性。 如何在 Spring Boot 中定义一个 RESTful API？\n解释如何在 Spring Boot 中创建和暴露 RESTful 风格的 API。 Spring Boot 中的启动类是什么？\n解释 Spring Boot 项目中的启动类作用和特点。 Spring Boot 的热部署是怎样实现的？\n说明 Spring Boot 如何支持热部署和开发者热部署的区别。 如何使用 Spring Boot 进行数据访问？\n介绍如何使用 Spring Boot 和 Spring Data JPA 或其他数据访问技术来操作数据库。 Spring Boot 中的 Bean 是什么？\n解释 Spring Boot 中的 Bean 是如何定义和管理的。 如何在 Spring Boot 中处理异常？\n解释如何定义全局异常处理器来处理项目中的异常。 Spring Boot 中如何进行单元测试？\n说明如何编写和执行 Spring Boot 应用的单元测试。 如何在 Spring Boot 中使用缓存？\n解释如何配置和使用 Spring Boot 中的缓存功能。 Spring Boot 中的配置文件有哪些种类？\n介绍 application.properties 和 application.yml 之间的区别和使用场景。 如何集成其他技术到 Spring Boot 项目中？\n举例说明如何集成消息队列、NoSQL 数据库等技术到 Spring Boot 项目中。 这些问题涵盖了 Spring Boot 框架的一些核心概念和常见用法。在准备面试时，建议深入学习 Spring Boot 框架的基础知识，并通过实践来加深理解。还可以尝试解决一些与 Spring Boot 相关的小项目或实际问题，以加强自己在面试中的表现。祝你面试顺利！\nSpring Spring 是什么？特性？有哪些模块？ Spring是一个开源的Java应用程序框架，它提供了全面的基础设施，用于帮助开发人员构建高效、灵活和可维护的企业级Java应用程序。\nSpring的特性包括：\nIoC（控制反转）： Spring采用IoC容器来管理和组织对象的生命周期和依赖关系。通过IoC，对象的创建和依赖关系不再由代码硬编码决定，而是由Spring容器动态地管理。\nDI（依赖注入）： DI是IoC的一种实现方式，通过依赖注入，Spring容器会自动将依赖关系注入到需要它们的对象中，从而实现对象之间的松耦合。\nAOP（面向切面编程）： Spring支持AOP，它允许开发人员在不修改原有代码的情况下，将横切关注点（如日志、安全性等）从应用程序中提取出来，以增强代码的复用性和可维护性。\n事务管理： Spring提供了强大的事务管理功能，它支持声明式事务和编程式事务，能够轻松地管理数据库事务。\nMVC框架： Spring提供了一个灵活的MVC框架，用于构建Web应用程序。它支持将请求映射到控制器、视图渲染、数据绑定等功能。\n面向切面编程（AOP）： Spring支持AOP，允许开发人员将横切关注点（如日志、安全性等）从应用程序代码中提取出来，实现横向代码的重用。\nJDBC支持： Spring提供了JDBC模块，简化了JDBC的使用，提供了更便捷的数据库访问方式。\nSpring框架主要由以下几个模块组成：\nSpring Core（核心容器）： 提供了Spring框架的基本功能，包括IoC容器的实现、Bean的创建和管理、依赖注入等。\nSpring AOP（面向切面编程）： 提供了AOP的支持，允许开发人员将横切关注点从应用程序代码中解耦出来，以增强代码的复用性和可维护性。\nSpring JDBC（数据库访问）： 提供了简化JDBC操作的功能，使得开发人员可以更方便地访问数据库。\nSpring ORM（对象关系映射）： 提供了集成各种ORM框架（如Hibernate、JPA）的功能，简化了对象和数据库之间的映射。\nSpring Web（Web开发）： 提供了构建Web应用程序的支持，包括MVC框架、RESTful Web服务等。\nSpring Test（测试）： 提供了对Spring应用程序进行单元测试和集成测试的功能。\n总体来说，Spring框架为Java开发者提供了丰富的功能和工具，使得开发高效、可维护和可扩展的应用程序变得更加简单。\nSpring 的两大核心概念是什么？简单讲一下你对它们的理解 来自：编程导航官方、yes.\nSpring 框架的两大核心概念是控制反转（Inversion of Control，IoC）和面向切面编程（Aspect Oriented Programming，AOP）。\n控制反转IOC指的是将对象的创建和依赖注入由应用代码转移到了 Spring 容器中进行，即由 Spring 容器负责创建对象和管理它们之间的依赖关系。这样，应用代码只需要关注业务逻辑的实现，而不需要关注对象的创建和管理，降低了应用代码的复杂度，提高了代码的可重用性和可维护性。\n面向切面编程AOP是指将与业务逻辑无关的代码（如日志、安全、事务等）从业务逻辑中剥离出来，以便于统一管理和维护。通过 AOP，我们可以将这些与业务逻辑无关的横切关注点（Cross-cutting Concerns）定义为切面（Aspect），并将它们织入到业务逻辑中，从而实现了业务逻辑与横切关注点的解耦。并且可维护性也大大提高\n扩展：\n那AOP是如何实现的呢？ (见 面试题挑战 Day3 JDK 动态代理和 CGLIB 动态代理的区别是什么？) 如果你简历上写了设计模式，那么这两个核心概念中有涉及到什么设计模式可以讲讲吗？ 工厂模式：spring中使用了BeanFactory和ApplicationContext创建了Bean对象。 单例模式：在IOC中的对象默认都是单例的，可以通过配置文件修改。 代理模式：AOP就是基于动态代理的，如果对象实现了接口，使用JDK的动态代理，如果对象没有实现接口则使用CGLIB的动态代理。（这里可以暗示往这两个动态代理方面问，就又撞到前几天刷过的题了） Spring的 IOC 和 AOP 怎么理解 控制的是对象的创建权，管理权。反转的是权利，使用对象时，由主动new产生对象转换为由外部提供对象\nSpring容器负责创建、配置和管理 bean，也就是它管理着 bean 的生命周期，控制着 bean 的依赖注入\n控制对象⽣命周期的不再是引⽤它的对象，⽽是容器。对具体对象，以前是它控制其它对象，现在所有对象都被容器控制，所以这就叫控制反转\nAOP切面编程简单说，就是把⼀些业务逻辑中相同代码抽取到⼀个独⽴的模块中，让业务逻辑更加清爽。\n通过切面技术为业务主体增加额外的通知（Advice），从而对声明为“切点”（Pointcut）的代码块进行统一管理和装饰\nAOP是面向对象OOP的一种补充，OOP的核心单元是类class，AOP的核心单元是切面aspect。利用AOP可以对业务逻辑各个部分进行隔离，从而降低耦合度，提高程序的可重用性，也提高了效率\nSpringAOP主要想解决什么问题？\nSpring AOP主要解决的是横切关注点的问题，即在一个系统中，可能存在多个模块或组件都需要实现类似的功能，比如日志记录、权限校验、事务管理等等。\n如果每个模块都去实现这些功能，就会导致代码冗余，可维护性和可扩展性降低。\n而AOP则是基于动态代理的机制，在不修改原有代码的情况下，通过在代码执行前后插入增强代码的方式，可以减少系统中的重复代码，降低了模块间的耦合度。实现对横切关注点的统一处理，从而提高代码的复用性和可维护性。\nAOP的原理了解吗？\nSpring AOP 是基于动态代理的，它使用 JDK 的动态代理或 CGLIB 动态代理来为目标对象生成代理对象，进而实现切面功能。主要应⽤于处理⼀些具有横切性质的系统级服务，如⽇志收集、事务管理、权限校验、安全检查、缓存、对象池管理等。\n**JDK 动态代理：**通过目标对象实现的接口来为目标对象生成代理对象，利用反射机制实现方法的调用。 **CGLIB 动态代理：**通过目标对象的子类来为目标对象生成代理对象，利用字节码技术实现方法的调用。 实现 Spring AOP，需要定义切面（Aspect）和连接点（Join Point），并将切面织入到连接点处，生成代理对象。\nJoin Point：连接点指的是程序中进行方法调用的点。Spring AOP 可以对类的方法调用进行拦截，并在目标方法的前后添加额外的功能逻辑。\nAspect：切面是一系列横切逻辑的集合，可通过 Spring AOP 把这些横切逻辑模块化，然后把它们应用到多个对象当中。\nAdvice：通知是要执行的额外逻辑，Spring AOP 包含如下五种类型的 Advice：\n前置通知（Before Advice）：在目标方法调用前，执行通知代码。 后置通知（After Advice）：在目标方法调用后，执行通知代码。 环绕通知（Around Advice）：包裹目标方法，在目标方法调用的前后，分别执行通知代码。 异常通知（After Throwing Advice）：捕获目标方法执行的异常，并在异常抛出时，执行通知代码。 返回通知（After Returning Advice）：在目标方法执行后，正常返回后，执行通知代码。 Pointcut：定义切入点，指定哪些类的哪些方法需要被拦截，可以使用注解方式或者切入表达式的方式。\n切面织入（Weaving）：将切面与目标对象结合生成代理对象，在代理对象方法执行前后或抛出异常时，执行 Advice 中的操作。\n总结，Spring AOP 就是把各种通知写成切面的方式，通过拦截指定的方法实现各种额外的操作。而切面是通过代理实现的，使用 JDK 动态代理或者 CGLIB 动态代理将通知织入目标对象的方法当中。\n有哪些优点？\nIOC 解决了依赖问题。在传统的 Java 编程中，对象之间相互依赖，难以维护和扩展。使用 IOC 将对象的创建、销毁、依赖关系的维护等交给 Spring 容器来管理，使开发者只需要专注于业务逻辑的实现，而不用考虑对象的生命周期管理和依赖关系注入。\nAOP 解决了代码复用问题。在传统的 Java 编程中，业务逻辑和非业务逻辑耦合在一起，难以复用。使用 AOP 可以将某个横切关注点（如日志记录、事务处理、安全检查等）抽象出来，然后以切面的方式添加到业务逻辑中，避免代码的复杂和重复。\nAOP使用场景有哪些？ 记录操作日志，用来在方法调用前、后或异常发生时记录日志，方便跟踪和监控系统的运行情况。\n将获取用户名、获取请求方式、访问结果、模块结果、登录IP、操作时间这些通过方法抽取成切面方法\n权限校验，在某个方法前加上AOP定义的切面，确保具备相应权限的用户才能执行方法\n事务管理，实现事务管理，将事务的开始、提交和回滚操作织入到需要事务支持的方法中\n切片类\n实现@Compenont注解，被Spring管理 实现@Aspect注解，标明是切面类 AOP（面向切面编程）是一种编程范式，用于在程序运行时动态地将横切关注点（如日志记录、安全性检查、事务管理等）与主要业务逻辑（核心关注点）分离开来。AOP可以在不修改原有代码的情况下，通过横切关注点的织入来实现对系统的功能增强和重用。\n以下是一些常见的 AOP 使用场景：\n日志记录： AOP 可以用来在方法调用前、后或异常发生时记录日志，方便跟踪和监控系统的运行情况。\n事务管理： AOP 可以实现事务管理，将事务的开始、提交和回滚操作织入到需要事务支持的方法中，保证数据的一致性和完整性。\n安全性检查： AOP 可以用于在方法执行前进行权限验证和安全性检查，确保只有具备相应权限的用户能够执行特定操作。\n性能监控： AOP 可以用于在方法调用前后记录方法执行时间，以及对方法性能进行监控和优化。\n异常处理： AOP 可以用于在方法执行发生异常时进行异常处理和统一的错误处理。\n缓存管理： AOP 可以用于在方法调用前先查询缓存，如果缓存中存在相应数据，则直接返回缓存数据，减少对数据库的访问。\n日志审计： AOP 可以用于在方法调用前后进行日志审计，记录用户的操作行为，以便后续审计和追踪。\n国际化和本地化： AOP 可以用于在方法调用前根据用户的语言设置进行国际化和本地化处理，显示相应的语言资源。\n权限控制： AOP 可以用于在方法调用前对用户的权限进行校验，确保用户有权执行特定操作。\n你的项目中有没有用过AOP？ 用过，在API开发平台中的用户登录校验用到了\n进行了权限校验，比如修改接口上线接口就是只有管理员身份可以操作的，一般用户无法操作\n如何实现的呢？\n@Autowired 和 @Resource 区别 @Autowired是Spring提供的注解，用于实现自动装配。它根据类型进行依赖注入，如果存在多个匹配的bean，则根据名称进行匹配，也可以结合 @Qualifier 注解来指定具体的bean。 @Resource是Java标准库中的注解，也可用于实现自动装配。它根据名称进行依赖注入，如果存在多个匹配的bean，则根据类型进行匹配。可以通过 name 属性显式指定要注入的bean的名称。 主要区别：\n来源不同：@Autowired是Spring的注解，@Resource是Java标准库的注解。 用法不同：@Autowired可用于字段、构造方法、Setter方法和方法参数，@Resource主要用于字段和方法参数。 依赖注入方式不同：@Autowired根据类型进行注入，@Resource根据名称进行注入。 Spring中框架常用的注解 X Spring注解及其功能：（配置和依赖注入）\n@Component： 将类标记为Spring组件，让Spring容器自动扫描并将其实例化为Bean。（@Controller、@Service、@Repository） @Autowired： 自动注入Bean的依赖，通过类型匹配进行注入。 @Qualifier： 和@Autowired一起使用，指定要注入的具体Bean名称。 @Resource： 和@Autowired类似，但是通过名称匹配进行注入。 @Scope：设置作用域 @Value： 注入简单类型的值或SpEL表达式。 @ComponentScan： 配置Spring组件扫描的基础包。 @Configuration： 声明当前类是一个配置类，相当于Spring的XML配置文件。 @Bean： 声明一个Bean对象，用于替代XML配置中的标签。 @PostConstruct： 在Bean初始化完成后执行指定方法。 还有就是与AOP相关做增强的注解如@Aspect、@Before、@After、@Around、@Pointcut\nSpringMVC中常见的注解：（请求和响应）\n@RequestMapping： **将HTTP请求映射到处理方法上，用于定义Web应用程序的控制器。**可以定义在类上和方法上。 @RestController： 组合了@Controller和@ResponseBody，用于定义RESTful Web服务的控制器。所有方法返回JSON响应格式 @PathVariable： 将URL中的路径参数{/user/{id}}映射到方法参数上。 **@RequestParam：**指定请求参数的名称； @RequestBody： 将请求体内容映射到方法参数上。可以将接收的JSON转换为Java对象 @ResponseBody： 注解实现将controller方法返回对象转化为json 对象响应给客户端 @RequestHeader：获取指定的请求头数据 SpringBoot中常见注解：\n@SpringBootConfiguration：组合了@Configuiration注解，实现配置文件的功能\n@EnableAutoConfiguration：打开自动配置的功能，也可以关闭某个自动配置的选项\n@ComponentScan：Spring扫描组件包的路径\n@PathVariable： 这个注解用于将URL中的路径参数映射到方法的参数上。它通常用于处理RESTful风格的URL，将URL中的一部分作为方法的参数，使得控制器方法能够访问和使用这些参数。例如：\n@GetMapping(\u0026#34;/users/{userId}\u0026#34;) public ResponseEntity\u0026lt;User\u0026gt; getUser(@PathVariable Long userId) { // ... } @RequestParam： 这个注解用于将HTTP请求的查询参数映射到方法的参数上。查询参数通常是在URL中通过?符号传递的参数，这个注解使得你可以在控制器方法中轻松地获取和使用这些参数。例如： @GetMapping(\u0026#34;/search\u0026#34;) public ResponseEntity\u0026lt;List\u0026lt;Product\u0026gt;\u0026gt; searchProducts(@RequestParam String keyword) { // ... } @RequestBody： 这个注解用于将HTTP请求的请求体内容映射到方法的参数上。通常在处理POST、PUT等请求时使用，它可以将请求体中的JSON、XML等数据直接映射到方法参数的对象上。例如： @PostMapping(\u0026#34;/create\u0026#34;) public ResponseEntity\u0026lt;Product\u0026gt; createProduct(@RequestBody Product product) { // ... } @ResponseBody： 这个注解用于将方法的返回值直接写入HTTP响应体中，通常在处理JSON、XML等响应时使用。它告诉Spring框架将方法的返回值转换为响应体内容，并发送给客户端。例如： @GetMapping(\u0026#34;/user/{userId}\u0026#34;) @ResponseBody public User getUser(@PathVariable Long userId) { // ... return user; } 这些注解帮助你在控制器中更清晰地处理请求和响应，同时可以方便地进行参数映射、数据转换以及响应的构建。通过使用这些注解，你可以更轻松地构建出功能强大且易于维护的Web应用程序。\nSpring的循环依赖是什么？如何解决？ Spring循环依赖（Circular Dependency）是指在Spring应用程序中，两个或多个Bean之间相互依赖，形成了一个循环的引用关系。\n这种情况下，Spring容器无法确定应该首先实例化哪个Bean，因为它们互相依赖于对方的实例。\n比如A依赖于B，B依赖于A\n循环依赖在Spring中允许存在，Spring框架依据三级缓存已经解决了大部分的循环依赖\n一级缓存：单例池，缓存已经经历了完整的生命周期，已经初始化完成的bean对象 二级缓存：缓存早期的bean对象（生命周期还没走完） 三级缓存：缓存的是ObjectFactory，表示对象工厂，用来创建某个对象的 具体流程\n第一，先实例A对象，同时会创建ObjectFactory对象存入三级缓存 singletonFactories 第二，A在初始化的时候需要B对象，然后进行B对象的创建 第三，B实例化完成，也会创建ObjectFactory对象存入三级缓存 singletonFactories 第四，B需要注入A，通过三级缓存中获取ObjectFactory来生成一个A的对象 同时存入二级缓存，这个是有两种情况，一个是可能是A的普通对象，另外 一个是A的代理对象，都可以让ObjectFactory来生产对应的对象，这也是三 级缓存的关键 第五，B通过从通过二级缓存earlySingletonObjects 获得到A的对象后可以正 常注入，B创建成功，存入一级缓存singletonObjects 第六，回到A对象初始化，因为B对象已经创建完成，则可以直接注入B，A 创建成功存入一级缓存singletonObjects 第七，二级缓存中的临时对象A清除 构造方法出现了循环依赖怎么解决？\n由于bean的生命周期中构造函数是第一个执行的，spring框架并不能解决构 造函数的的依赖注入，在循环依赖的其中一个Bean上添加@Lazy注解，什么时候需要对象再进行 bean对象的创建\n循环依赖通常出现在以下情况下：\n构造函数循环依赖：当Bean A的构造函数参数中需要Bean B，而Bean B的构造函数参数中需要Bean A，就会出现循环依赖。 属性循环依赖：Bean A中的属性依赖于Bean B，而Bean B中的属性又依赖于Bean A。 解决Spring循环依赖的方法有几种：\n使用Setter注入：将循环依赖的Bean之间的依赖关系放在setter方法上，而不是构造函数或属性中。这样Spring容器可以先实例化Bean，然后再注入依赖，避免了构造函数循环依赖。\n使用@Lazy注解：在循环依赖的其中一个Bean上添加@Lazy注解，延迟初始化Bean，从而避免在实例化时出现循环依赖。但这可能会导致性能问题，因为Bean在第一次使用时才会被实例化。\n使用代理对象：Spring提供了基于接口的代理机制，可以通过使用接口和代理来解决循环依赖问题。在配置类中，使用@Bean注解声明Bean时，可以添加@Scope(proxyMode = ScopedProxyMode.INTERFACES)来创建接口代理，或者使用@Scope(proxyMode = ScopedProxyMode.TARGET_CLASS)来创建基于CGLIB的代理。\n优化设计：重新设计应用程序结构，尽量减少循环依赖的存在。这可能需要对类的职责和关系进行调整。\n使用构造函数注入和工厂方法：将循环依赖的Bean的依赖通过构造函数注入，并使用工厂方法创建Bean实例。这样可以确保Bean在被完全构造之前不会被暴露给外部。\n请注意，虽然有多种解决循环依赖的方法，但选择哪种方法取决于应用程序的具体情况和需求。最好的实践是尽量避免循环依赖，保持应用程序的设计简单和清晰。\nSpring的三级缓存 在Spring框架中，Bean的创建过程涉及到三级缓存（Three-Level Cache）的概念，用于管理Bean的创建与依赖解析。这三级缓存包括singleton对象的缓存、early singleton对象的缓存以及singleton工厂的缓存。\n一级缓存（singleton对象的缓存）： 这是Spring容器中最常见的缓存级别，**用于存储已经创建的singleton对象。**当你通过Spring容器获取一个singleton对象时，容器会首先查找一级缓存中是否已经存在该对象的实例。如果存在，则直接返回已有实例，否则将会创建新的对象并存储在一级缓存中。\n二级缓存（early singleton对象的缓存）： 二级缓存是在一级缓存之后的一个缓存层。当Spring创建一个singleton对象时，它会在创建过程中首先将对象实例放入二级缓存中。这是为了解决循环依赖的问题。如果在创建过程中发现其他Bean需要引用该对象，Spring可以通过二级缓存来提前暴露对象，从而避免循环依赖问题。一旦对象完全创建并初始化，它就会从二级缓存中移除并放入一级缓存中。\n三级缓存（singleton工厂的缓存）： 三级缓存存储的是用于创建singleton对象的ObjectFactory。在Bean的创建过程中，如果发现需要解决循环依赖，Spring会将正在创建的Bean的ObjectFactory存储在三级缓存中。这允许Spring在适当的时候通过工厂创建对象实例，从而在循环依赖的情况下完成对象的创建和初始化。\n需要注意的是，三级缓存不是公开的API，而是Spring框架内部使用的机制。对于大多数开发者来说，理解一级缓存和二级缓存的概念更加重要，因为它们与Bean的生命周期和依赖解析紧密相关。理解这些缓存级别有助于更好地管理Bean的创建和依赖关系，避免潜在的问题，例如循环依赖。\nSpring用到了哪些设计模式？ 单例模式：Spring 的 Bean 默认是单例模式，通过 Spring 容器管理 Bean 的生命周期，保证每个 Bean 只被创建一次，并在整个应用程序中重用。\n工厂模式：Spring 使用工厂模式通过 BeanFactory 和 ApplicationContext 创建并管理 Bean 对象。\n代理模式：Spring AOP 基于动态代理技术，使用代理模式实现切面编程，提供了对 AOP 编程的支持。\n观察者模式：Spring 中的事件机制基于观察者模式，通过 ApplicationEventPublisher 发布事件，由 ApplicationListener 监听事件，实现了对象间的松耦合。\n策略模式：Spring 中的 HandlerInterceptor 和 HandlerExecutionChain 使用了策略模式，允许开发者自定义处理器拦截器，按照一定顺序执行。\n责任链模式：Spring 中的过滤器和拦截器使用了责任链模式，多个过滤器和拦截器按照一定顺序执行，每个过滤器和拦截器可以拦截请求或者响应并做出相应的处理。\n模板方法模式（Template Method）： 在Spring的JdbcTemplate中，使用模板方法模式封装了JDBC操作，将公共的操作抽象成模板方法，具体实现由子类提供。\n装饰器模式（Decorator）： 在Spring中，使用装饰器模式来实现对Bean的增强，如使用@Async来异步执行方法。\n模型-视图-控制器（MVC）模式： Spring的Web模块采用了MVC模式，将应用程序分成模型、视图和控制器三个组件，实现了业务逻辑和界面的分离。\n委派模式（Delegate）： Spring中的DispatcherServlet采用了委派模式，根据URL将请求分发给不同的处理器来处理。\n适配器模式（Adapter）： 在Spring中，通过HandlerAdapter接口实现了适配器模式，将不同类型的处理器适配到统一的处理器接口上。\nBeanFactory 和 FactoryBean有什么区别 BeanFactory 和 FactoryBean 是 Spring 框架中两个关键的接口，它们在创建和管理 Bean 方面有着不同的作用和用途。\nBeanFactory：\nBeanFactory 是 Spring IoC 容器的核心接口**，用于管理和提供 Bean 实例。** BeanFactory 负责管理 Bean 的生命周期、依赖注入、对象实例化和配置。 在 Spring 中，有多种实现 BeanFactory 接口的类，其中最常用的是 DefaultListableBeanFactory，它是 Spring 容器的默认实现。 BeanFactory 是一种轻量级容器，按需加载和初始化 Bean，只有在使用时才实例化 Bean。 FactoryBean：\nFactoryBean 是一个特殊的 Bean 接口，它允许定义更复杂的 Bean 创建逻辑。 实现 FactoryBean 接口的类是一个工厂 Bean，用于生成其他的 Bean 实例。 通过实现 FactoryBean 接口，你可以自定义 Bean 实例的创建过程，允许更高级的逻辑和配置。 当你在 Spring 容器中注册一个实现了 FactoryBean 接口的类时，实际上并不会直接将该类的实例作为 Bean 放入容器中。相反，容器会调用 FactoryBean 接口的方法，根据你的逻辑来生成最终的 Bean 实例。 总结：\nBeanFactory 是 Spring IoC 容器的基础接口，用于管理 Bean 的声明周期和依赖注入。 FactoryBean 是一个特殊的 Bean 接口，用于定义更复杂的 Bean 创建逻辑，并且允许你自定义 Bean 实例的生成过程。 当你需要在 Spring 容器中使用特定逻辑生成 Bean 实例时，可以实现 FactoryBean 接口来实现自定义的 Bean 创建逻辑。 如果有两个相同名字的bean会怎么样 如果有两个相同名字的Bean，Spring默认的处理方式是抛出异常。\n因为根据名称进行装配时，Spring要求唯一匹配的Bean，否则会报错。\n如果确实存在需要区分的情况，可以使用@Qualifier注解指定具体的Bean名称，或者使用@Autowired和@Qualifier一起使用。\n另外，也可以使用@Primary注解来指定某个Bean为首选Bean，当存在多个匹配的Bean时，会优先选择标记为@Primary的Bean。\nSpring Bean 的生命周期 BeanDefinition对象\n**Spring Bean 的生命周期是指一个 Bean 实例从被创建到被销毁的整个过程。**Spring 容器负责管理 Bean 的生命周期，确保 Bean 在需要时正确创建、初始化和销毁。\nSpring Bean 的生命周期包括以下阶段：\n首先，通过BeanDefinition对象获取bean的定义信息 实例化bean： Spring 容器根据配置信息创建 Bean 的实例。（1 bean的创建） 依赖注入： Spring 容器将 Bean 的依赖注入到 Bean 实例中，即设置 Bean 的属性值和引用。 处理Aware接口： 如果 Bean 实现了 BeanNameAware 、BeanFactoryAware 和ApplicationContextAware接口，Spring 容器会回调相应的方法，以传递 Bean 的名称和 BeanFactory。 前置初始化（Initialization）： 如果 Bean 实现了 InitializingBean 接口，Spring 容器会调用其 afterPropertiesSet() 方法，完成 Bean 的前置初始化工作。 自定义初始化方法（Initialization）： 可以通过 @PostConstruct 注解或来定义自定义的初始化方法。 后置初始化（Initialization）： 如果 Bean 定义了 BeanPostProcessor 接口的实现类，Spring 容器会在初始化过程中调用它们的 postProcessBeforeInitialization() 方法，允许对 Bean 进行额外的处理。（2-6初始化赋值bean）（可以在此阶段使用AOP对对象进行增强，基于动态代理的方式） Bean 使用： Bean 可以被正常使用，执行它们的业务逻辑。 销毁（Destruction）： 如果 Bean 实现了 DisposableBean 接口，Spring 容器在销毁 Bean 之前会调用它的 destroy() 方法。 **默认情况下，Spring 容器中的 Bean 是单例的。**也就是说，对于同一个 Bean 定义，Spring 容器只会创建一个实例，并在后续的请求中重复使用这个实例。这种单例模式可以确保多个对象之间共享同一个实例，从而节省资源和提高性能。\nSpring 容器中的单例 Bean 在容器启动阶段被创建，并在整个容器的生命周期中保持活跃状态。无论是通过 XML 配置文件定义 Bean，还是通过注解声明 Bean，其默认的作用域（scope）都是单例的。\nSpring容器初始化阶段starter？ 在 Spring 容器启动阶段，会执行一系列的操作来初始化和准备容器，以及创建和配置所有的 Bean 实例。\n以下是 Spring 容器启动阶段会进行的主要操作：\n加载配置文件： Spring 容器会根据配置文件（通常是 XML 配置文件或基于注解的配置类）加载应用程序的配置信息。\n创建容器： Spring 容器会根据加载的配置文件创建相应类型的容器。常见的容器类型包括 ApplicationContext 和 BeanFactory。\n解析配置： 容器会解析配置文件中定义的 Bean 和其他配置信息，建立内部的数据结构来管理这些配置。\n实例化 Bean： 容器会根据配置信息创建 Bean 实例。根据配置的作用域，可能会创建单例 Bean 或原型（prototype）Bean。\n属性注入： 容器会通过依赖注入（DI）将属性值注入到 Bean 实例中，满足依赖关系。\nBean 生命周期回调： 容器会调用 Bean 生命周期的回调方法，比如调用 @PostConstruct 方法和 Bean 实现的 InitializingBean 接口的 afterPropertiesSet() 方法。\n处理 Aware 接口： 如果 Bean 实现了 Aware 接口，容器会调用相应的回调方法，比如 BeanNameAware、ApplicationContextAware 等。\n处理后置处理器（PostProcessor）： 容器会调用注册的 Bean 后置处理器，允许开发人员在 Bean 实例化之前或之后进行一些自定义的处理。\n初始化单例 Bean： 对于单例 Bean，容器会在启动阶段初始化并提前创建这些 Bean。\n完成启动： 容器完成上述所有操作后，即完成了启动阶段，此时容器已经准备好了所有的 Bean 实例，应用程序可以开始运行了。\n总体来说，Spring 容器启动阶段的主要任务是读取配置信息、创建和配置 Bean 实例，并在需要时进行依赖注入和生命周期回调。一旦启动阶段完成，容器就进入了运行时阶段，提供服务并管理应用程序中的 Bean 实例。\nSpring框架中的bean是线程安全的吗 答案：不是线程安全的\n当多用户同时请求一个服务时，容器会给每一个请求分配一个线程，这时多个线程会并发执行该请求对应的业务逻辑（成员方法），如果该处理逻辑中有对该单例状态的修改（体现为该单例的成员属性），则必须考虑线程同步问题。\nSpring框架并没有对单例bean进行任何多线程的封装处理。关于单例bean的线程安全和并发问题需要开发者自行去搞定。\n比如：我们通常在项目中使用的Spring bean都是不可变的状态(比如Service类和DAO类)，所以在某种程度上说Spring的单例bean是线程安全的。\n如果你的bean有多种状态的话（比如 View Model对象），就需要自行保证线程安全。最浅显的解决办法就是将多态bean的作用由“singleton”变更为“prototype”。\nSpring的事务你在项目中用到没有，怎么用的? Spring的事务常用的使用方式是通过@Transactional注解来实现。在项目中使用事务时，通常需要按照以下步骤进行配置：\n在Spring的配置文件中开启事务管理，可以使用tx:annotation-driven标签或者@Bean注解方式。\n在需要进行事务管理的方法或类上添加@Transactional注解。\n可以通过@Transactional注解的属性来配置事务的传播行为、隔离级别、回滚规则等。\n在方法内部进行数据库操作，当方法执行结束时，如果抛出异常，则事务会进行回滚；否则，事务会进行提交。\nSpring事务管理有几种方式 主要理解编程式和声明式管理方式\nSpring中的事务本质是通过AOP功能，对方法前后进行拦截，在执行方法之前开启事务，在执行完目标方法之后根据执行情况提交或者回滚事务\nSpring框架提供了多种方式来管理事务，以满足不同的应用场景和需求。以下是Spring事务管理的几种常见方式：\n编程式事务管理： 这种方式通过编写代码来显式地管理事务。开发人员需要在代码中手动开始、提交或回滚事务，使用Spring提供的TransactionTemplate或PlatformTransactionManager接口来实现。这种方式较为灵活，适用于在代码中需要精确控制事务的场景。\n声明式事务管理（基于XML配置）： 这种方式通过在XML配置文件中声明事务管理的规则。开发人员可以通过XML配置来定义事务的传播行为、隔离级别、回滚规则等。然后，在需要应用事务的方法上，使用XML配置中定义的事务管理器进行事务管理。这种方式较为简单，适用于事务管理的规则相对固定的场景。\n声明式事务管理（基于注解）： 这种方式通过在方法或类上使用注解来声明事务管理的规则。开发人员可以使用@Transactional注解来标记需要应用事务的方法，Spring会根据注解的配置来管理事务。这种方式简化了事务配置，使代码更加清晰，并且更容易与业务逻辑整合。\n事务模板（TransactionTemplate）： 事务模板是Spring提供的编程式事务管理的方式之一。通过TransactionTemplate，开发人员可以在代码中精确控制事务的开始、提交和回滚，并在事务中执行需要的操作。\n注解驱动事务管理： Spring支持使用@Transactional注解来声明事务管理的规则。开发人员可以将@Transactional注解应用在方法或类上，来标记需要应用事务的方法。这样，Spring会根据注解的配置来管理事务，实现声明式事务管理。\nAOP 面向切面编程： Spring的事务管理底层实现就是使用了AOP。通过AOP，Spring可以在方法执行前后织入事务处理的逻辑，实现事务的开启、提交和回滚等操作。\n不同的事务管理方式适用于不同的场景，开发人员可以根据具体的需求选择合适的事务管理方式。\n声明式事务和编程式事务有什么区别 声明式事务和编程式事务是两种不同的事务管理方式，它们在事务管理的实现方式和使用方式上有一些区别。\n实现方式：\n编程式事务：编程式事务是通过编写代码来实现的。需要在代码中显式地开始、提交和回滚事务，使用Spring提供的编程式事务管理类（如TransactionTemplate）或接口（如PlatformTransactionManager）来实现。对业务代码有侵入性，项目中很少使用。\n声明式事务：声明式事务是**通过注解或XML配置来实现的。**只需要在方法或类上添加相应的注解或配置即可，无需手动编写事务管理的代码。Spring框架会根据注解或配置来管理事务。\n使用方式：\n编程式事务：编程式事务需要代码中显式地管理事务，需要手动调用事务的开始、提交和回滚等方法。这种方式相对繁琐，需要更多的代码来处理事务。\n声明式事务：声明式事务只需要在应用事务的方法或类上添加相应的注解，Spring框架会根据注解的配置自动管理事务。这样可以使代码更加清晰，并且减少了手动处理事务的代码量。\n灵活性：\n编程式事务：编程式事务相对较为固定，事务管理的代码通常在业务逻辑代码中分散，难以集中管理。对于需要频繁修改事务规则的情况，使用编程式事务可能不够灵活。但能够实现精准事务控制。\n声明式事务：声明式事务更加灵活，可以根据不同的需求，使用不同的事务管理器和事务配置。\nSpring的事务注解 在 Spring 框架中，事务管理可以通过使用 @Transactional 注解来实现。@Transactional 注解可以应用在类或方法上，用于声明事务的属性和行为。它提供了一种简单、声明性的方式来管理方法或类的事务行为。\n以下是常用的 @Transactional 注解的属性：\npropagation（传播行为）： 指定事务的传播行为，默认值是 Propagation.REQUIRED。传播行为定义了方法调用时如何使用现有的事务，比如是否创建新的事务、加入已有的事务等。\nisolation（隔离级别）： 指定事务的隔离级别，默认值是 Isolation.DEFAULT。隔离级别定义了多个事务之间的数据隔离程度，包括读取未提交数据、读取已提交数据、可重复读、串行化等级别。\nreadOnly： 指定事务是否为只读事务，默认值是 false。如果设置为 true，则表示该事务只读取数据，不做数据修改操作。\ntimeout： 指定事务的超时时间，默认值是 -1，表示没有超时限制。如果设置了超时时间，在指定的时间内事务没有完成，将自动回滚。\nrollbackFor 和 noRollbackFor： 分别指定事务中抛出哪些异常会触发回滚操作，以及哪些异常不会触发回滚。\ntransactionManager： 指定使用的事务管理器的名称。当应用中存在多个事务管理器时，可以通过该属性指定要使用的事务管理器。\nvalue 和 qualifier： 这两个属性用于指定事务的名称。value 属性是 @AliasFor 注解，可以替代 qualifier 属性来指定事务的名称。\n示例使用：\n@Service public class MyService { @Transactional(propagation = Propagation.REQUIRED, isolation = Isolation.DEFAULT, readOnly = false, timeout = -1, rollbackFor = {Exception.class}) public void myTransactionalMethod() { // 事务方法的业务逻辑 } } 在上面的例子中，myTransactionalMethod() 方法会被声明为一个事务方法，当调用该方法时，Spring 会根据 @Transactional 注解的属性来管理该方法的事务行为。\nSpring事务的传播机制？ 讲一下Spring事务的传播机制？\nSpring 事务的传播机制决定了事务如何在方法调用之间传播。\nSpring 支持下面的事务传播机制：propagation\nPROPAGATION_REQUIRED：**如果当前存在事务，则加入当前事务，如果当前不存在事务，则新建一个事务。**required PROPAGATION_SUPPORTS：如果当前存在事务，则加入当前事务，**如果当前不存在事务，则以非事务的方式执行。**supports PROPAGATION_MANDATORY：如果当前存在事务，则加入当前事务，**如果当前不存在事务，则抛出异常。**mandatory PROPAGATION_REQUIRES_NEW：当前存在事务则挂起当前事务，并新建一个事务执行。requires_new PROPAGATION_NOT_SUPPORTED：以非事务的方式执行，如果当前存在事务，则挂起事务。not_supported PROPAGATION_NEVER：**以非事务的方式执行，如果当前存在事务，则抛出异常。**never PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务中执行，如果当前不存在事务，则新建一个事务。nested 有时候在一个大的事务中，需要执行一些小的业务操作，这些小的业务操作可以单独成功或失败，不影响大的事务，这属于哪种事务传播机制？\n大事务中执行小业务操作不影响大事务的情况，可以使用 PROPAGATION_REQUIRES_NEW 事务传播机制。在这种传播机制下，一个新的独立事务将会被开启，它不依赖于外部事务。即使内部事务失败回滚，外部事务也不受影响，而且内部事务可以独立地提交或回滚，不会影响到外部事务的结果。\n如果当前存在事务，则使用当前事务，如果当前不存在事务，则无事务执行，这属于哪种事务传播机制？\nPROPAGATION_NOT_SUPPORTED，即如果当前存在事务，则加入当前事务；如果当前不存在事务，则以无事务的方式执行方法。这种传播行为会在方法执行时，暂时暂停已有的事务（如果有的话），然后以非事务方式执行当前方法，从而实现无事务的效果。这在某些情况下很有用，特别是当你想要在不影响现有事务的情况下执行一段逻辑时。\nSpring中事务失效的场景有哪些？ 本题考察对spring框架的深入理解、复杂业务的编码经验\n失效场景：\n异常捕获处理 抛出检查异常 非public方法导致的异常 异常捕获处理\n事务通知只有捉到了目标抛出的异常，才能进行后续的回滚处理，如果目标自己处理掉异常，事务通知无法知悉。\n即如果在使用Spring事务的方法中，如果对可能出现的异常进行自己处理（e.printStackTrace），Spring是发觉不到出现异常的，事务也就失效了。\n解决方法：在catch块中添加throw new RuntimeException(e)抛出\n抛出检查异常\nSpring 默认只会回滚非检查异常（RuntimeException），像FileNotFoundException就不会被检测到 解决方法：配置rollbackFor属性为Exception\n非public方法导致的异常\nSpring为方法创建代理、添加事务通知、前提条件都是该方法是public为前提。\n解决：方法改为public\nSpring给我们提供了很多扩展点，这些有了解吗？ 不太清楚扩展点指的什么\n作者补充：\nSpring框架提供了许多扩展点，使得开发者可以根据需求定制和扩展Spring的功能。以下是一些常用的扩展点：\nBeanFactoryPostProcessor：允许在Spring容器实例化bean之前修改bean的定义。常用于修改bean属性或改变bean的作用域。 BeanPostProcessor：可以在bean实例化、配置以及初始化之后对其进行额外处理。常用于代理bean、修改bean属性等。 PropertySource：用于定义不同的属性源，如文件、数据库等，以便在Spring应用中使用。 ImportSelector和ImportBeanDefinitionRegistrar：用于根据条件动态注册bean定义，实现配置类的模块化。 Spring MVC中的HandlerInterceptor：用于拦截处理请求，可以在请求处理前、处理中和处理后执行特定逻辑。 Spring MVC中的ControllerAdvice：用于全局处理控制器的异常、数据绑定和数据校验。 Spring Boot的自动配置：通过创建自定义的自动配置类，可以实现对框架和第三方库的自动配置。 自定义注解：创建自定义注解，用于实现特定功能或约定，如权限控制、日志记录等。 动态代理有哪两种？如何实现？ 在 Java 中，动态代理有两种主要的实现方式：\n基于接口的动态代理（JDK 动态代理）： JDK 动态代理是 Java 标准库提供的一种动态代理实现方式。它基于接口，要求被代理的类必须实现至少一个接口。JDK 动态代理使用 java.lang.reflect.Proxy 类来创建代理对象。代理对象实现了被代理接口，并将方法调用转发给一个实现了 InvocationHandler 接口的处理器对象。处理器对象中实现了对原始方法的增强逻辑。\nJDK 动态代理的步骤：\n定义一个接口（或一组接口）； 创建一个实现 InvocationHandler 接口的处理器对象； 使用 Proxy.newProxyInstance() 方法创建代理对象，并指定接口、处理器对象以及类加载器。 基于类的动态代理（CGLIB 动态代理）： CGLIB（Code Generation Library）是一个开源的第三方库，它可以在运行时动态生成类的字节码。CGLIB 动态代理不要求被代理的类实现接口，它直接生成被代理类的子类，并重写其中的方法来实现代理。CGLIB 动态代理使用 net.sf.cglib.proxy.Enhancer 类来创建代理对象。\nCGLIB 动态代理的步骤：\n创建一个 Enhancer 对象； 设置被代理类作为父类； 设置 MethodInterceptor 对象，用于实现对原始方法的增强逻辑； 使用 Enhancer.create() 方法创建代理对象。 需要注意的是，JDK 动态代理只能代理实现了接口的类，而 CGLIB 动态代理可以代理普通类。由于 JDK 动态代理基于接口，因此在某些情况下只能使用 CGLIB 动态代理。\n选择使用哪种动态代理方式取决于被代理类的类型和需求。如果被代理类实现了接口，优先考虑使用 JDK 动态代理。如果被代理类没有实现接口，可以考虑使用 CGLIB 动态代理。\n1、JDK动态代理具体实现原理：\n通过实现 InvocationHandler 接口创建自己的调用处理器；\n通过为Proxy类指定 ClassLoader 对象和一组interface来创建动态代理；\n通过反射机制获取动态代理类的构造函数，其唯一参数类型就是调用处理器接口类型；\n通过构造函数创建动态代理类实例，构造时调用处理器对象作为参数参入；\nJDK动态代理是面向接口的代理模式，如果被代理目标没有接口那么Spring也无能为力，Spring通过Java的反射机制生产被代理接口的新的匿名实现类，重写了其中AOP的增强方法。\n2、CGLib动态代理：\n利用ASM开源包，对代理对象类的class文件加载进来，通过修改其字节码生成子类来处理。\n3、两者对比：\nJDK动态代理是面向接口的。\nCGLib动态代理是通过字节码底层继承要代理类来实现，因此如果被代理类被final关键字所修饰，会失败。\n如果要被代理的对象是个实现类，那么Spring会使用JDK动态代理来完成操作（Spirng默认采用JDK动态代理实现机制）；\n**如果要被代理的对象不是个实现类，那么Spring会强制使用CGLib来实现动态代理。\n代理模式和适配器模式有什么区别？ 代理模式主要是去加强一个类的方法。适配器模式是接口转换成一个想要的接口（这个问题被面试说回答的不好）\n作者补充：\n代理模式和适配器模式是两种常用的设计模式，它们的区别主要体现在以下几个方面：\n作用不同：代理模式是为了控制对对象的访问，而适配器模式是为了解决接口不匹配的问题。 解决问题的角度不同：代理模式是从外部控制访问，保护目标对象，而适配器模式是从内部改变对象接口，让其能够适配客户端的要求。 实现方式不同：代理模式通常使用面向对象的继承或者组合方式实现，而适配器模式则通常使用对象组合方式实现。 适用场景不同：代理模式适用于需要对对象进行控制和保护的情况，例如远程代理、虚拟代理等。适配器模式适用于需要将一个类的接口转换成客户端期望的另一个接口的情况，例如旧系统的升级改造、不兼容接口的统一等。 SpringMVC SpringMVC 框架\nSpringMVC 是一个基于 Java 的实现了 MVC 设计模式的请求驱动类型的轻量级 Web 框架，通过把 Model，View，Controller 分离，将 Web 层进行职责解耦，把复杂的 Web 应用分成逻辑清晰的几部分，简化开发，减少出错，方便组内开发人员之间的配合。 简而言之，SpringMVC 就是将我们原来开发在 Servlet 中的代码拆分了，一部分由 SpringMVC 完成，一部分由我们自己完成。\nSpringMVC 主要组件\n前端控制器 DispatcherServlet：接收请求、响应结果，相当于转发器，有了 DispatcherServlet 就减少了其它组件之间的耦合度。\n处理器映射器 HandlerMapping：根据请求的 URL 来查找 Handler。\n处理器适配器 HandlerAdapter：负责执行 Handler。\n处理器 Handler：处理业务逻辑的 Java 类（我们自己写的 Controller 类）。\n视图解析器 ViewResolver：进行视图的解析，根据视图逻辑名将 ModelAndView 解析成真正的视图（view） 。\n视图 View：View 是一个接口， 它的实现类支持不同的视图类型，如 jsp，freemarker， pdf 等。\nSpring和Spring MVC的区别 Spring 和 Spring MVC 是两个相关但不同的模块。\nSpring： Spring 是一个综合性的企业级开发框架，提供了大量的功能和特性，用于简化企业级应用程序的开发。它的核心是 IoC（控制反转）和 AOP（面向切面编程）容器。Spring 提供了一系列的模块，包括但不限于：\nSpring Core：提供 IoC 容器和依赖注入功能，负责管理对象的创建和生命周期。 Spring AOP：实现面向切面编程，用于在程序运行时动态地添加横切关注点。 Spring ORM：集成了各种 ORM 框架，如 Hibernate、JPA，用于数据库访问。 Spring JDBC：提供对 JDBC 的封装，简化数据库访问。 Spring Transactions：提供声明式事务管理。 Spring Security：提供安全认证和授权功能等。 Spring MVC： Spring MVC 是 Spring 框架的一个模块，用于构建基于 MVC（Model-View-Controller）模式的 Web 应用程序。它是 Spring 框架的一部分，用于处理 Web 请求、响应和视图渲染等。Spring MVC 提供了一个灵活的、基于注解的控制器模型，允许开发人员通过注解来定义请求处理方法。Spring MVC 的核心组件包括：\nDispatcherServlet：前端控制器，用于拦截所有的请求，并将其分发给相应的处理器。 HandlerMapping：用于将请求映射到相应的处理器。 HandlerAdapter：用于执行处理器方法，并处理方法返回的结果。 ViewResolver：用于将逻辑视图名解析为实际的视图。 总结：Spring 是一个综合性的企业级开发框架，提供了依赖注入、面向切面编程、数据库访问、事务管理、安全认证等功能。而 Spring MVC 则是 Spring 框架中用于构建 Web 应用程序的一个模块，它基于 MVC 模式，用于处理 Web 请求、响应和视图渲染。 Spring MVC 通常作为 Spring 框架的一部分使用，用于构建 Web 层，使得开发 Web 应用程序更加方便和高效。\nSpring、SpringMVC、SpringBoot 三者之间是什么关系？ Spring、SpringMVC、SpringBoot 是三个独立的框架，它们之间的关系是：\nSpring 是一个 Java 的轻量级应用框架，提供了基于 IoC 和 AOP 的支持，用于构建企业级应用。Spring 有多个模块，包括 Spring Core、Spring Context、Spring JDBC、Spring Web 等，每个模块提供了不同的功能。 SpringMVC 是 Spring 框架的一部分，是基于 MVC 设计模式的 Web 框架，用于构建 Web 应用程序。它提供了控制器、视图解析器、数据绑定、异常处理等功能，使得开发 Web 应用变得更加简单。SpringMVC 还支持 RESTful 架构。 SpringBoot 是基于 Spring 框架的一个开发框架，用于快速构建独立的、生产级别的 Spring 应用程序。它通过自动配置和约定优于配置的方式，简化了 Spring 应用程序的配置和开发过程。SpringBoot 集成了很多常用的第三方库和工具，例如 Spring Data、Spring Security、Thymeleaf、Logback 等，可以极大地提高开发效率。 因此，SpringBoot 可以看作是在 Spring 的基础上，通过自动配置和约定优于配置的方式，提供了更加简单、快速的开发体验。而 SpringMVC 则是 Spring 框架中用于构建 Web 应用程序的模块。\nSpring MVC执行流程 X 视图版本（JSP）\n用户发送请求到前端控制器（DispatcherServlet）。这是一个调度中心 前端控制器 （ DispatcherServlet ） 收到请求调用处理器映射器 （HandlerMapping），去查找处理器（Handler）。 处理器映射器（HandlerMapping）找到具体的处理器(可以根据 xml 配置、注解进行查找)，生成处理器对象及处理器拦截器(如果有则生成)一并返回给前端控制器 DispatcherServlet。 前端控制器（DispatcherServlet）调用处理器适配器（HandlerAdapter）。 处理器适配器（HandlerAdapter）去调用自定义的处理器类（Controller）。 自定义的处理器类（Controller）将得到的参数进行处理并返回结果给处理器适配器（HandlerAdapter）。 处理器适配器 （ HandlerAdapter ）将得到的结果返回给前端控制器 （DispatcherServlet）。 前端控制器（DispatcherServlet ）将 ModelAndView 传给视图解析器 （ViewReslover）。 视图解析器（ViewReslover）将得到的参数从逻辑视图转换为物理视图并返回给前端控制器（DispatcherServlet）。 前端控制器（DispatcherServlet）调用物理视图进行渲染并返回。 前端控制器（DispatcherServlet）将渲染后的结果返回。 这是之前有JSP视图的流程，现在基本都是前后端分离开发的，并没有视图这些，一般都是handler中使用Response直接结果返回。\n前后端开发，接口开发\n用户发送请求到前端控制器（DispatcherServlet）。 前端控制器 （ DispatcherServlet ） 收到请求调用处理器映射器 （HandlerMapping），去查找处理器（Handler）。 处理器映射器（HandlerMapping）找到具体的处理器(可以根据 xml 配置、注解进行查找)，生成处理器对象及处理器拦截器(如果有则生成)一并返回给 DispatcherServlet。 前端控制器（DispatcherServlet）调用处理器适配器（HandlerAdapter）。 处理器适配器（HandlerAdapter）去调用自定义的处理器类（Controller）。 方法上加上了@RequestBody注解 通过HttpMessageConvert来返回结果转换为JSON并响应 SpringBoot SpringBoot比Spring相比的优点 SpringBoot的优点：\nSpring Boot 可以快速创建独⽴的 Spring 应⽤程序。 Spring Boot 内嵌了如 Tomcat，Jetty 和 Undertow 这样的容器，也就是说可以直接跑起来， ⽤不着再做部署⼯作了。 Spring Boot ⽆需再像 Spring ⼀样使⽤⼀堆繁琐的 xml ⽂件配置。 Spring Boot 可以⾃动配置(核⼼)Spring。SpringBoot 将原有的 XML 配置改为 Java 配置，将 bean 注⼊改为使⽤注解注⼊的⽅式(@Autowire)，并将多个 xml、properties 配置浓缩在⼀个 appliaction.yml 配置⽂件中。 Spring Boot 提供了⼀些现有的功能，如量度⼯具，表单数据验证以及⼀些外部配置这样的⼀些 第三⽅功能。 Spring Boot 可以快速整合常⽤依赖（开发库，例如 spring-webmvc、jackson-json、 validation-api 和 tomcat 等），提供的 POM 可以简化 Maven 的配置。当我们引⼊核⼼依赖 时，SpringBoot 会⾃引⼊其他依赖。 Spring Boot 是在 Spring 框架基础上构建的一种简化配置、快速开发的框架，它具有许多优点相对于传统的 Spring 框架，以下是一些 Spring Boot 相对于 Spring 的优点：\n简化配置： Spring Boot 可以通过约定大于配置的方式，自动配置大部分应用程序所需的配置，减少了繁琐的 XML 配置，使得应用的配置更加简单。\n快速开发： Spring Boot 提供了一系列的 Starter 和自动配置，使得开发者可以快速搭建一个可用的应用，并专注于业务逻辑的开发，提高开发效率。\n集成方便： Spring Boot 内置了许多常用的第三方库和框架的 Starter，可以方便地集成数据库、消息队列、安全认证、监控等功能，减少了集成的复杂性。\n嵌入式 Web 服务器： Spring Boot 默认使用嵌入式的 Web 服务器（如 Tomcat、Jetty），不需要部署在外部容器中，简化了部署和运行的过程。\n自动化配置： Spring Boot 可以根据类路径上的依赖和配置，自动配置 Bean 和各种组件，大大减少了手动配置的工作。\n单一 Jar 包： Spring Boot 可以将应用及其依赖打包成一个可执行的 Jar 文件，方便部署和运行，也便于容器化部署。\n健康监测： Spring Boot 提供了健康检查和监控功能，可以方便地查看应用的状态和性能指标。\n生态系统： Spring Boot 是 Spring 生态系统中的一部分，可以与其他 Spring 项目无缝集成，如 Spring Cloud 用于构建微服务架构。\n总的来说，Spring Boot 是一个非常便捷的框架，它简化了 Spring 应用的开发和配置，提高了开发效率，让开发者可以更专注于业务逻辑的实现。同时，Spring Boot 的自动化配置和快速集成功能也为构建现代化的应用提供了很多便利。\nSpring 和 Spring Boot区别 Spring 和 Spring Boot 是两个相关但不同的概念：\nSpring： Spring 是一个综合性的企业级开发框架，提供了大量的功能和特性，用于简化企业级应用程序的开发。Spring 的核心是 IoC（控制反转）和 AOP（面向切面编程）容器。它提供了依赖注入、面向切面编程、事务管理、数据库访问、安全认证等众多功能。\nSpring Boot： Spring Boot 是在 Spring 框架基础上构建的一种简化配置、快速开发的框架。它借助于自动配置和约定大于配置的原则，使得开发者可以快速搭建可用的应用程序，并专注于业务逻辑的开发。Spring Boot 提供了一系列 Starter 和自动配置，简化了应用的配置和依赖管理。\n总结：\nSpring 是一个完整的企业级开发框架，提供了众多功能和模块，适用于各种规模的企业应用开发。 Spring Boot 是在 Spring 框架基础上的扩展，目标是简化 Spring 应用的开发和配置，提高开发效率。 Spring Boot 提供了自动配置、Starter 和约定大于配置等特性，使得开发者能够更轻松地构建现代化的应用。 Spring Boot 是 Spring 生态系统的一部分，可以与其他 Spring 项目（如 Spring Cloud）无缝集成，用于构建分布式和微服务架构。 SpringBoot的启动流程 Spring Boot 的启动流程可以简要概括为以下几个步骤：\n加载 Spring Boot 应用配置： 当应用启动时，Spring Boot 会读取默认的配置文件（例如 application.properties 或 application.yml）以及自定义的配置文件，并将配置信息加载到 Spring 的环境中。\n创建 Spring Boot 应用上下文（ApplicationContext）： Spring Boot 会根据加载的配置信息创建应用上下文，应用上下文是 Spring 中的核心容器，用于管理和组织 Bean 对象。\n扫描和注册 Bean： Spring Boot 会自动扫描指定包及其子包，查找带有注解（如 @Component、@Service、@Controller 等）的类，并将这些类注册为 Bean。\n执行 SpringApplicationRunListeners： Spring Boot 在应用启动的过程中会执行一系列的监听器（SpringApplicationRunListener），用于在不同阶段执行自定义的操作。\n执行 ApplicationRunner 和 CommandLineRunner： Spring Boot 提供了两个接口 ApplicationRunner 和 CommandLineRunner，可以让开发者在 Spring Boot 启动完成后执行一些初始化操作。\n启动 Web 服务器： 如果应用是一个 Web 应用，Spring Boot 会根据配置选择合适的 Web 服务器（如 Tomcat、Jetty），并将应用部署到 Web 服务器中。\n应用启动完成： 当所有初始化和配置工作完成后，Spring Boot 应用就启动完成了，可以响应外部请求。\n在整个启动流程中，Spring Boot 通过自动配置和约定大于配置的原则，大大简化了应用的配置和启动过程，让开发者可以更加专注于业务逻辑的开发，提高开发效率。\nSpring Boot的Starter机制了解吗？ Spring Boot Starter 是 Spring Boot 中用于快速集成和配置特定功能的一种机制。它将相关的依赖项、配置和代码封装在一个 Maven 项目中，以提供特定功能的“启动器”。\nSpring Boot Starter 的主要特点和作用包括：\n简化配置： Starter 封装了一组相关的依赖项和配置，可以一次性引入一个 Starter，从而避免手动添加多个依赖项和配置的繁琐过程。\n约定大于配置： Starter 遵循约定大于配置的原则，提供了默认的配置和依赖项，使得开发者只需要关注自定义配置，而不必过多地关注底层的依赖项。\n功能模块化： Starter 将功能模块化，每个 Starter 提供特定的功能。例如，Spring Boot 提供了多个 Starter，如 spring-boot-starter-web 用于启动 Web 应用，spring-boot-starter-data-jpa 用于启动 JPA 数据访问。\n易于扩展： 开发者可以自定义和编写自己的 Starter，将自定义的功能封装为 Starter，以供项目中使用。\nSpring Boot Starter 的命名约定为 spring-boot-starter-*，其中 * 表示特定的功能模块。当引入一个 Starter 时，Spring Boot 会自动将相关的依赖项和配置添加到项目中。\n例如，如果需要启动一个 Web 应用，只需在 pom.xml 文件中添加以下依赖：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Spring Boot 会自动将与 Web 相关的依赖项和配置添加到项目中，使得开发者可以快速搭建一个可用的 Web 应用。\n通过使用 Starter 机制，Spring Boot 大大简化了应用的配置和依赖管理，让开发者更加专注于业务开发，提高了开发效率。同时，Starter 也促进了模块化的应用设计，使得应用更加易于维护和扩展。\nSpringBoot的配置文件 Spring Boot 支持多种类型的配置文件，用于配置应用程序的属性和参数。常见的配置文件类型有：\napplication.properties： 这是最常用的配置文件类型，采用键值对的形式，用于配置应用程序的属性。在 Spring Boot 应用的 src/main/resources 目录下，可以创建一个名为 application.properties 的文件，并在其中设置属性值。\n示例：\n# 数据库连接配置 spring.datasource.url=jdbc:mysql://localhost:3306/mydb spring.datasource.username=root spring.datasource.password=secret # Web 服务器端口 server.port=8080 application.yml： 这是另一种常见的配置文件类型，采用 YAML（YAML Ain\u0026rsquo;t Markup Language）格式，使用缩进来表示层次关系，可读性较好。在 Spring Boot 应用的 src/main/resources 目录下，可以创建一个名为 application.yml 的文件，并在其中设置属性值。\n示例：\n# 数据库连接配置 spring: datasource: url: jdbc:mysql://localhost:3306/mydb username: root password: secret # Web 服务器端口 server: port: 8080 application-{profile}.properties 或 application-{profile}.yml： 这是针对不同环境的配置文件，可以根据不同的运行环境使用不同的配置。{profile} 是环境的标识，例如 dev、prod 等。\n示例：\napplication-dev.properties：\n# 开发环境数据库配置 spring.datasource.url=jdbc:mysql://dev-server:3306/mydb spring.datasource.username=dev-user spring.datasource.password=dev-password application-prod.properties：\n# 生产环境数据库配置 spring.datasource.url=jdbc:mysql://prod-server:3306/mydb spring.datasource.username=prod-user spring.datasource.password=prod-password bootstrap.properties 或 bootstrap.yml： 这是用于 Spring Cloud 配置的特殊配置文件。在 Spring Boot 应用中，如果引入了 Spring Cloud 的相关依赖，可以使用 bootstrap.properties 或 bootstrap.yml 来配置应用程序的属性。\n这些配置文件可以在 Spring Boot 应用的 src/main/resources 目录下创建，并根据需要设置不同的属性值。Spring Boot 会自动加载这些配置文件，并将其中的属性值注入到应用程序中。同时，可以使用多个配置文件来实现不同环境下的配置管理，使得应用程序更具灵活性和可维护性。\nSpring Boot 的自动配置原理 X Spring Boot 的自动装配（Auto-configuration）是指在应用启动时，Spring Boot 根据项目的依赖和配置，自动完成一系列的配置工作，以简化开发者的配置过程。\n在Spring Boot项目中的引导类上有一个注解@SpringBootApplication，这个 注解是对三个注解进行了封装，分别是：\n@SpringBootConfiguration @EnableAutoConfiguration @ComponentScan 其中@EnableAutoConfiguration是实现自动配置的核心注解，该注解通过@Import注解导入对应的配置选择器。关键是内部就是读取了该项目引用jar包的类路径上的 META-INF/spring.factories 文件中所配置的类的全类名\n在这些配置类中所定义的Bean会根据条件注解所指定的条件来决定是否需要 将其导入到Spring容器中。\n一般条件判断会有像 @ConditionalOnClass 这样的注解，判断是否有对应的 class文件，如果有则加载该类，把这个配置类的所有的Bean放入spring容器 中使用。\nSpring Boot 的自动装配基于 Spring 框架的 @Configuration 注解和条件化配置机制。当启动一个 Spring Boot 应用时，Spring Boot 会扫描应用的 classpath，查找指定的依赖和配置，然后根据条件和优先级自动装配相应的组件。\n自动装配的优先级如下：\n用户配置优先： 如果开发者在项目中手动定义了配置（@Configuration），则用户配置优先于自动配置。\n条件化配置： Spring Boot 使用条件化配置来确定是否需要自动装配某个组件。如果满足特定的条件，才会执行自动配置。\n类路径上的 META-INF/spring.factories 文件： Spring Boot 在 classpath 上搜索 META-INF/spring.factories 文件，该文件列出了要执行的自动配置类。\nSpring Boot 中的自动装配类通常带有 @ConditionalOnXXX 注解，用于定义装配的条件，例如 @ConditionalOnClass 表示当特定的类存在时才进行自动装配，@ConditionalOnProperty 表示当指定属性存在时才进行自动装配等。\n例如，Spring Boot 提供了许多 Starter，每个 Starter 都包含了一组相关的依赖和配置，用于快速集成某个功能。例如，spring-boot-starter-web Starter 包含了与 Web 相关的依赖和配置，可以快速构建一个 Web 应用程序。\n总结：\nSpring Boot 的自动装配是通过条件化配置和 classpath 上的 META-INF/spring.factories 文件来实现的。它可以根据项目的依赖和配置，自动装配所需的组件和配置，简化了开发者的配置工作，提高了开发效率。\nMyBatis MyBatis的执行流程 mybatis-config.xml，加载映射文件\n①读取MyBatis配置文件：mybatis-config.xml加载运行环境和映射文件\n②构造会话工厂SqlSessionFactory，一个项目只需要一个，单例的，一般由 spring进行管理\n③会话工厂创建SqlSession对象，这里面就含了执行SQL语句的所有方法\n④操作数据库的接口，Executor执行器，同时负责查询缓存的维护\n⑤Executor接口的执行方法中有一个MappedStatement类型的参数，封装了 映射信息 ⑥输入参数映射\n⑦输出结果映射\nMyBatis 如何实现延迟加载？X 延迟加载：需要用到数据才进行加载，不需要用到数据时就不加载数据\nMyBatis是否支持延迟加载？\nMybatis支持一对一关联对象和一对多关联集合对象的延迟加载\n在Mybatis配置文件中，可以配置是否启用延迟加载 lazyLoadingEnabled=true|false，默认是关闭的\nMyBatis 实现延迟加载的方式有两种：\n延迟加载配置方式：在 MyBatis 的配置文件中，通过配置 来指定延迟加载的触发方法。默认是关闭的。\n动态代理方式：当需要延迟加载某个属性时，MyBatis 会生成一个动态代理，拦截对该属性的访问，然后使用真正的查询操作去加载该属性的值。\n延迟加载的原理\n延迟加载在底层主要使用的CGLIB动态代理完成的\n使用CGLIB创建目标对象的代理对象，这里的目标对象就是开启了 延迟加载的mapper 当调用目标方法时，进入拦截器invoke方法，如果发现目标方法是null 值，再执行sql查询 获取数据以后，调用set方法设置属性值，再继续查询目标方法，就能查到数据 MyBatis的#和$区别 在 MyBatis 中，# 和 $ 都是用于在 SQL 中插入动态值的占位符，但它们在处理动态值时有一些重要的区别。\n# 占位符：\n# 占位符会将参数值当作一个预编译的参数，并使用 JDBC 的 PreparedStatement 来处理。这样可以防止 SQL 注入攻击。 使用 # 占位符时，MyBatis 会自动将参数值转义，因此不需要手动处理特殊字符。 # 占位符一般用于处理参数值，如条件查询等。 示例：\n\u0026lt;!-- 使用 # 占位符 --\u0026gt; SELECT * FROM users WHERE id = #{userId} $ 占位符：\n$ 占位符将参数值直接拼接到 SQL 中，不会进行预编译处理，存在 SQL 注入的风险。 使用 $ 占位符时，需要手动处理特殊字符的转义，以防止 SQL 注入攻击。 $ 占位符一般用于处理 SQL 片段，如动态表名或动态列名，而不是参数值。 示例：\n\u0026lt;!-- 使用 $ 占位符 --\u0026gt; SELECT * FROM ${tableName} WHERE id = ${userId} 总结：\n使用 # 占位符时，参数值会被预编译处理，安全性较高，适用于处理参数值。 使用 $ 占位符时，参数值会直接拼接到 SQL 中，慎用，可能存在 SQL 注入的风险，适用于处理 SQL 片段。 MyBatis 的一级缓存和二级缓存 X MyBatis 的多级缓存机制是指在 MyBatis 中存在多个级别的缓存，用于提高查询性能并减少数据库访问次数。\n本地缓存，基于PrepetualCache，本质上是一个HashMap\n一级缓存：基于 PerpetualCache 的 HashMap 本地缓存，其存储作 用域为 Session，当Session进行flush或close之后，该Session中的所有Cache 就将清空，默认打开一级缓存\n二级缓存：基于namespace和mapper的作用域起作用的，不是依赖于SQL session，默认也是采用 PerpetualCache，HashMap 存储。\n如果想要开启二级缓存需要在全局配置文件和映射文件中开启配置才行。\nMyBatis的二级缓存什么时候清理缓存中的数据？\n当某一个作用域(一级缓存 Session/二级缓存Namespaces)的进行了新增、修 改、删除操作后，默认该作用域下所有 select 中的缓存将被 clear。\n一级缓存（本地缓存）： 一级缓存是 MyBatis 默认开启的缓存，它是在同一个 SqlSession 内部的缓存。 当在同一个 SqlSession 中执行相同的查询语句时，MyBatis 会将查询结果缓存起来，下次再执行相同的查询语句时，直接从缓存中获取结果，避免了重复查询数据库。 一级缓存是与 SqlSession 绑定的，当 SqlSession 被关闭或提交时，缓存也会被清空。 默认情况下，一级缓存是开启的，如果需要关闭一级缓存，可以在配置文件中将 localCacheScope 设置为 STATEMENT。 二级缓存（全局缓存）： 二级缓存是 MyBatis 的全局缓存，它可以被多个 SqlSession 共享。 二级缓存是将查询结果缓存到一个共享的缓存区域中，当多个 SqlSession 执行相同的查询语句时，可以直接从缓存中获取结果，避免了重复查询数据库。 二级缓存的生命周期与整个应用程序的生命周期相同，需要配置相应的缓存实现器，如 Ehcache、Redis 等。 默认情况下，二级缓存是关闭的，如果需要开启二级缓存，可以在配置文件中将 \u0026lt;setting\u0026gt; 标签的 cacheEnabled 设置为 true。在映射文件中使用\u0026lt;cache/\u0026gt;生效缓存 MyBatis 如何防止SQL注入 MyBatis 提供了一些机制来防止 SQL 注入攻击，主要包括以下几点：\n使用参数化查询： 在执行 SQL 时，使用参数化查询（Prepared Statement）而不是直接将参数拼接到 SQL 语句中。参数化查询可以让数据库系统将查询语句和参数分开处理，从而避免了 SQL 注入攻击。在 MyBatis 中，我们使用 #{} 占位符来表示参数化查询。\n示例：\n\u0026lt;!-- 使用参数化查询，避免 SQL 注入 --\u0026gt; SELECT * FROM users WHERE username = #{username} AND password = #{password} 使用动态 SQL 和条件判断： 在需要拼接动态 SQL 的情况下，使用 MyBatis 提供的动态 SQL 标签（如 \u0026lt;if\u0026gt;、\u0026lt;where\u0026gt;、\u0026lt;choose\u0026gt; 等）来进行条件判断和拼接，而不是手动拼接 SQL 字符串。这样可以确保拼接的 SQL 是合法的，从而避免 SQL 注入。\n示例：\n\u0026lt;!-- 使用动态 SQL 和条件判断，避免 SQL 注入 --\u0026gt; \u0026lt;select id=\u0026#34;getUserByUsernameAndPassword\u0026#34; resultType=\u0026#34;User\u0026#34;\u0026gt; SELECT * FROM users \u0026lt;where\u0026gt; \u0026lt;if test=\u0026#34;username != null and password != null\u0026#34;\u0026gt; AND username = #{username} AND password = #{password} \u0026lt;/if\u0026gt; \u0026lt;/where\u0026gt; \u0026lt;/select\u0026gt; 过滤特殊字符： 在输入参数传递到 SQL 之前，对参数值进行特殊字符的过滤和转义，以确保参数值不包含恶意的 SQL 注入代码。\n限制输入长度： 对于输入的字符串参数，可以对其长度进行限制，避免超长输入导致的 SQL 注入攻击。\n使用 MyBatis 提供的参数检查： MyBatis 提供了一些有用的函数，如 Ognl 和 OGNL，用于在条件判断时对参数值进行合法性检查。\n虽然 MyBatis 采取了上述措施来防止 SQL 注入攻击，但在实际开发中，仍需谨慎对待用户输入，严格校验输入参数，以确保应用程序的安全性。除了 MyBatis 本身的防御措施，还可以结合其他安全框架和数据库权限控制，从多个层面来保护应用程序的安全。\nMyBatis运用了哪些常见的设计模式？ MyBatis 运用了一些常见的设计模式来实现其核心功能和提供灵活性。以下是 MyBatis 中常见的设计模式：\n工厂模式（Factory Pattern）： MyBatis 使用工厂模式来创建 SqlSessionFactory 和 SqlSession。SqlSessionFactory 是用于创建 SqlSession 的工厂类，而 SqlSession 是与数据库交互的主要接口。工厂模式将对象的创建和使用分离，使得系统更加灵活，易于扩展和维护。\n装饰器模式（Decorator Pattern）： MyBatis 使用装饰器模式来扩展和定制功能。通过装饰器模式，可以在不修改原有代码的情况下，动态地增加功能和行为。例如，MyBatis 的 Mapper 接口在运行时通过动态代理来生成实现类，从而实现了 Mapper 接口的具体实现。\n代理模式（Proxy Pattern）： MyBatis 使用动态代理实现了 Mapper 接口的具体实现。Mapper 接口的方法调用会被动态代理拦截，然后委托给相应的 SqlSession 进行数据库操作。代理模式可以实现对目标对象的控制，而不需要修改目标对象的代码。\n模板方法模式（Template Method Pattern）： MyBatis 中的 SqlSession 和 Statement 都采用了模板方法模式。模板方法模式将算法的框架定义在父类中，将具体实现延迟到子类中。SqlSession 和 Statement 提供了一系列的模板方法，子类可以通过继承并实现这些方法来完成具体的数据库操作。\n观察者模式（Observer Pattern）： MyBatis 使用了观察者模式来通知 Mapper 方法的调用，触发相应的数据库操作。在 Mapper 方法调用时，MyBatis 会触发相应的事件，通知注册的观察者进行处理。\n享元模式（Flyweight Pattern）： MyBatis 使用了享元模式来缓存复用 Statement 对象。Statement 对象是预编译的 SQL 语句的抽象表示，MyBatis 通过享元模式将 Statement 对象缓存起来，避免了重复创建。\n以上是 MyBatis 中常见的设计模式，这些设计模式使得 MyBatis 可以实现高度灵活的 SQL 映射和数据库操作，并提供了强大的扩展性和可维护性。\nJDBC连接数据库的步骤吗？ 使用JDBC连接数据库的步骤如下：\n加载数据库驱动程序：使用Class.forName()方法加载对应的数据库驱动程序，例如：Class.forName(\u0026ldquo;com.mysql.jdbc.Driver\u0026rdquo;); 建立数据库连接：使用DriverManager.getConnection()方法建立与数据库的连接，需要指定数据库的URL、用户名和密码，例如：Connection conn = DriverManager.getConnection(\u0026ldquo;jdbc:mysql://localhost/mydatabase\u0026rdquo;, \u0026ldquo;username\u0026rdquo;, \u0026ldquo;password\u0026rdquo;); 创建Statement对象：使用Connection对象的createStatement()方法创建一个Statement对象，用于执行SQL语句，例如：Statement stmt = conn.createStatement(); 执行SQL语句：使用Statement对象的executeQuery()或executeUpdate()方法执行SQL语句，例如：ResultSet rs = stmt.executeQuery(\u0026ldquo;SELECT * FROM mytable\u0026rdquo;); 处理查询结果：如果执行的是查询语句，需要使用ResultSet对象来处理查询结果，例如：while (rs.next()) { String name = rs.getString(\u0026ldquo;name\u0026rdquo;); int age = rs.getInt(\u0026ldquo;age\u0026rdquo;); } 关闭数据库连接：在程序结束时，需要使用Connection对象的close()方法关闭数据库连接，例如：conn.close(); 如何实现统一异常处理 在 Spring 框架中，可以通过自定义异常处理器来实现统一异常处理。Spring 提供了一个异常处理接口 HandlerExceptionResolver，可以通过实现该接口来自定义全局异常处理器。\n统一异常处理的步骤如下：\n创建一个自定义的异常类，该异常类继承自 Exception 或其子类。这个异常类可以用于表示你应用程序中可能发生的特定异常情况。\n实现 HandlerExceptionResolver 接口，该接口包含一个方法 resolveException()，在该方法中进行异常处理逻辑。\n在实现的 resolveException() 方法中，根据不同的异常类型进行相应的处理。你可以将异常信息记录日志、返回自定义的错误页面、返回 JSON 格式的错误信息等。\n注册自定义的异常处理器，在 Spring 配置文件（如 applicationContext.xml 或 Spring Boot 的配置类）中配置该处理器。\n示例：\npublic class MyCustomException extends RuntimeException { // 自定义异常类 // 可以添加自定义的异常信息和构造方法 } public class MyExceptionHandler implements HandlerExceptionResolver { @Override public ModelAndView resolveException(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) { ModelAndView mav = new ModelAndView(); if (ex instanceof MyCustomException) { // 处理自定义异常 MyCustomException mav.addObject(\u0026#34;errorMsg\u0026#34;, \u0026#34;发生了自定义异常\u0026#34;); mav.setViewName(\u0026#34;error-page\u0026#34;); } else { // 处理其他异常 mav.addObject(\u0026#34;errorMsg\u0026#34;, \u0026#34;发生了其他异常\u0026#34;); mav.setViewName(\u0026#34;error-page\u0026#34;); } return mav; } } 在 Spring 配置文件中配置自定义异常处理器：\n\u0026lt;bean id=\u0026#34;myExceptionHandler\u0026#34; class=\u0026#34;com.example.MyExceptionHandler\u0026#34; /\u0026gt; 这样，当应用程序中抛出 MyCustomException 或其他异常时，都会被自定义的异常处理器拦截并进行相应的处理。通过这种方式，你可以实现统一的异常处理，使应用程序的异常响应更加规范和友好。\n","permalink":"https://lidengxm.github.io/posts/java/%E6%A1%86%E6%9E%B6%E5%85%AB%E8%82%A1/","summary":"框架面试题合集 spring 当涉及校招应届生的 Spring 框架相关面试题时，通常会着重考察基础知识和理解能力。以下是一些常见的 Spring 相关面试题，希望对你有所帮助： 什么是 Spring 框架？ 解释 Spring 框架的概念、作用和特点。 Spring 框架的核心模块是什么？ 提及 Spring 框架的核心模块，如 Spring Core、Spring Context、Spring","title":"框架八股"},{"content":"Redis 是什么？使用场景有哪些？ Redis 是什么？\n**Redis（Remote Dictionary Server）是一个开源的内存数据存储系统，它被广泛用作缓存、消息队列和数据库。**Redis 支持多种数据结构，包括字符串、列表、集合、有序集合、哈希等，并提供了丰富的操作命令，非常适合作为中间件使用\n怎么使用？\n首先要连接到服务器上的 Redis，然后通过使用 Redis 的客户端操作 Redis\n使用 Redis 可以分为以下几个方面：\n安装和启动： 首先，需要在服务器上安装 Redis，并通过启动 Redis 服务器来开始使用它。 连接到 Redis： 客户端通过连接到 Redis 服务器来使用其功能。可以使用 Redis 提供的命令行客户端或者在多种编程语言中使用相应的 Redis 客户端库。 数据操作： 一旦连接到 Redis，你可以使用各种命令对数据进行读取、写入、更新和删除操作。例如，使用SET命令存储一个键值对，使用GET命令获取键对应的值。 数据结构： Redis 支持多种数据结构，如字符串、列表、集合、有序集合和哈希等。你可以根据实际需求选择适合的数据结构来存储和管理数据。 持久化： Redis 提供了两种持久化方式，分别是 RDB（Redis Database）快照和 AOF（Append-only File）日志。通过持久化，可以将数据保存在硬盘上，以防止数据丢失。 发布与订阅： Redis 支持发布与订阅模式，允许多个客户端订阅特定的频道并接收发布到这些频道的消息。 事务： Redis 支持简单的事务，可以通过 MULTI、EXEC、WATCH 等命令来实现一组命令的原子性操作。 性能优化： Redis 的性能主要来自于数据存储在内存中，因此它非常快速。你可以通过一些性能优化策略如数据分片、合理设置过期时间等进一步提升性能。 使用场景？\n根据简历上的业务进行回答 解决可能会出现的缓存问题：缓存三兄弟 缓存预热 定时任务 Scheduler Redis 数据结构存储对象、做排行榜 分布式锁 set nx ex、Redisson 消息队列\u0026hellip; 主要常用的业务场景有：\n对热点数据的缓存；因为 Redis 支持多种数据类型，数据存储在内存中，访问速度块，所以 Redis 很适合用来存储热点数据； 限时类业务的实现；可以使用 expire 命令设置 key 的生存时间，到时间后自动删除 key。例如使用在验证码验证、优惠活动等业务场景； 计数器的实现；因为 incrby 命令可以实现原子性的递增，所以可以运用于高并发的秒杀活动、分布式序列号的生成。例如限制一个手机号发多少条短信、一个接口一分钟限制多少请求、一个接口一天限制调用多少次等业务场景。 排行榜的实现；借助 Sorted Set 进行热点数据的排序。例如：下单量最多的用户排行榜，最热门的帖子（回复最多）等业务场景； 分布式锁实现；可以利用 Redis 的 setnx 命令进行。 队列机制实现；Redis 提供了 list push 和 list pop 这样的命令，所以能够很方便的执行队列操作。 Redis 实现分布式 Session 的好处？ 基于 cookie 和 session 实现登录中，session 每个 tomcat 中都有一份属于自己的 session，多台 Tomcat 并不共享 session 存储空间，当请求切换到不同 tomcat 服务时导致数据丢失的问题。\n早期方案通过 session 数据拷贝可以同步每个 Tomcat 上的 session\nTomcat 拷贝 session 数据的问题：\n浪费空间 可能会出现延迟 所以 session 的实现应该具有共享存储、key-value 结构=\u0026gt;Redis 实现分布式 session\n为什么要共享 Session？\n防止多个后端服务器的数据存储不一致，导致用户服务时出现未登录的情况\nRedis 如何实现分布式 session？\n使用 Redis 实现分布式 Session 需要将 Session 数据存储在 Redis 中，并在多个 Tomcat 实例之间共享这些数据。可以使用 Spring Session 和 Spring Boot 自动配置来实现这一目标，并在代码中访问 Session 数据。\n！Redis 的常用数据结构有哪些？应用场景？ Redis 的常用数据结构有字符串 String、哈希 hash、列表 list、集合 set、有序集合 sorted set\n应用场景：\n字符串，最简单也最常用的数据存储类型，主要用于缓存、计数器、分布式锁等。 哈希，hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象,value 是一个无序字典，类似于 HashMap 结构。主要用于存储对象、用户信息等结构化数据。 列表，存储有序的字符串列表，可以在头部或尾部添加或移除元素。list 常用作消息队列、任务队列等。 集合，set 存储的元素无序，且不能重复。常用于存储共同好友、共同关注、标签等不重复的数据。 有序集合，sorted set 每个元素关联一个分数（score），用于排序。常用于做排行榜、范围查询。 位图，利用 String 结构存储的 BitMap，常用于用户签到、状态记录等 地理坐标，GEO 用来存储地理信息，常用于地理位置范围查询和距离计算 HyperLogLog（基数估计），用于统计一个数据集合中的独立元素个数，比如统计网站的每日独立访客数量。 ！如何保证数据库和缓存的一致性？ 双写一致性：当修改了数据库的数据的同时也要更新缓存的数据，缓存和数据库的数据要保持一致\n延时双删：指在删除缓存数据的同时，延迟一段时间再次检查数据库，以确保数据的一致性。\n不能保证强一致性 延时双删是一种解决数据库与缓存一致性的方法。它的基本原理是在删除缓存中的数据时，不是立即删除，而是等待一段时间后再删除。在这段时间内，如果数据库中的数据被修改，则可以通过缓存的数据来进行恢复。\n具体实现方式如下：\n在删除缓存中的数据时，设置一个删除标记，表示该数据已经被删除，但并未立即删除缓存中的数据。 在数据库中增加一个删除状态字段，表示该数据是否已经被删除。 当数据库中的数据被修改时，同时修改删除状态字段，表示该数据已经被删除。 在删除标记的时间期限过后，再删除缓存中的数据。 为什么要双删？\n答案：因为无论是先查询数据库更新缓存，还是先更新缓存再查询数据库，都会有可能出现脏数据\n使用读写锁：读写锁允许多个线程同时读取数据，但只允许一个线程进行写入操作。这种锁机制可以有效地处理并发读取和写入操作，从而保证数据的一致性。\n性能不高，适用于需要强一致性的业务场景下 使用 Redisson 实现的读写锁，在查询缓存的时候添加共享锁，可以保证读读不互斥，读写互斥。当更新缓存的时候，添加排他锁，排它锁读读，读写都互斥，这样可以保证在更新缓存的时候不会有其他线程读取数据，避免了脏数据。\n当需要从数据库中读取数据时，首先尝试获取读锁。如果没有写操作，多个线程可以同时持有读锁，从而实现并发读取。 如果需要更新数据，先尝试获取写锁。获取写锁后，其他线程无法进行读取或写入操作，直到写锁被释放。 在写入数据库之前，更新缓存数据。由于获取了写锁，确保其他线程无法读取或写入，保证了数据的一致性。 更新数据库后，释放写锁。这样其他线程就可以继续获取读锁进行读取操作。 在读取数据时，首先尝试获取读锁。如果没有写操作，可以并发地读取数据。 注意：读和写方法上需要加同一把锁\n排他锁底层也是 Redis 的set nx命令，同一时刻只能有有一个线程拿到该锁\n读写双写（Read-Through and Write-Through）： 在读取缓存数据之前，先尝试从缓存中读取数据。如果缓存中没有数据或数据已过期，再从数据库读取数据，并将数据存储到缓存中。这样可以确保缓存中的数据与数据库中的数据保持一致。 更新时同步（Write-Through with Cache Invalidation）： 在更新数据库数据时，先更新数据库，然后再同步更新缓存，或者直接使缓存数据失效。这样，下一次读取该数据时，会重新从数据库中读取最新数据，并存储到缓存中。 数据过期策略（Expiration Policy）： 设置缓存数据的过期时间，确保缓存中的数据不会过时太久。过期时间应根据业务需求和数据更新频率来设置，以平衡缓存的性能和一致性。 缓存穿透处理（Cache Miss Handling）： 当缓存中没有某个数据时，如果直接请求数据库，而数据库中也没有该数据，可能会导致缓存穿透。为了避免这种情况，可以在缓存中设置一个\u0026quot;空值\u0026quot;标记，表示数据库中没有该数据，从而避免频繁访问数据库。 使用消息队列： 在更新数据库数据时，先将更新操作写入消息队列，再由消费者服务从队列中读取操作，并依次更新数据库和缓存。这样可以确保数据库和缓存的一致性，同时提高系统的可伸缩性和性能。 事务性操作： 对于一些需要同时更新数据库和缓存的复杂操作，可以使用数据库的事务机制来保证操作的原子性，同时在事务中更新缓存数据。 ！缓存穿透、缓存雪崩、缓存击穿？ 缓存穿透是什么？如何解决？ 缓存穿透：\n客户端请求的数据在 MySQL 中不存在，MySQL 查询不到也不会写入缓存，这些请求都会打到数据库，大量无效且不存在的数据可能会压垮数据库。 解决方案：\n缓存空值对象：查询返回的数据为空也建立缓存，值为空，并且设置一个较短的 TTL\n优点：客户端在请求 Redis 缓存时一定会命中缓存，如果数据不存在就会命中空值，实现简单 缺点：可能会占额外的内存，因为要将缓存中不存在的数据也建立缓存，但可以通过设置较短的 TTL 优化 布隆过滤器：用于检测一个元素是否在一个集合中，将所有可能的数据都存入一个 bitmap 中（经过 Hash 计算），如果要查询的数据不存在就直接过滤掉，不会再打到数据库\n优点：无需多余的 Key 不占内存 缺点：实现复杂，可能会存在误判可能，即有的数据不存在也可能判断存在 注意：缓存预热时要把布隆过滤器先预热了。\n误判率：数组越小误判率就越大，数组越大误判率就越小，但数组过大占用空间也大\n缓存雪崩是设么？如何解决？\n缓存雪崩是指在同一时段大量的缓存key同时失效或者Redis服务宕机，导致大量请求打到数据库，带给数据库巨大压力。\n解决方法：\n给不同的 Key 的 TTL 添加随机值，降低大量 Key 同时失效的概率\n利用 Redis 集群提高服务的可用性\n给缓存业务添加业务降级限流策略，降级可作为系统的保底策略，适用于缓存穿透、缓存雪崩、缓存击穿\n给业务加多级缓存，nginx 缓存+redis 缓存+其他缓存（ehcache 等）\n缓存击穿是设么？如何解决？\n缓存击穿是指**某个热点 key（访问量很高）**突然过期了，大量请求瞬间在此时打到数据库，导致数据库压力骤增瞬间把数据库压垮\n与缓存雪崩不同，缓存雪崩是大量 key 同时失效，而缓存击穿是某个 key 突然失效 解决方法：\n加互斥锁（分布式锁）：客户端来查询缓存发现缓存未命中时，加互斥锁去查询数据库并更新缓存。 其他线程尝试加互斥锁失败，休眠等待，再重试，直到前面线程释放锁之后才有机会获得锁 强一致性、性能较差 逻辑过期：因为设置 TTL 引起的过期 key 导致的缓存击穿，所以不设置 TTL 而在存入缓存时添加一个字段用来表示是否过期，此过期非真正过期 查询缓存时发现缓存已过期，返回旧数据，并开启一个异步线程去查询数据库更新缓存 其他线程查询缓存发现缓存已过期，尝试加锁，加锁失败，都会返回旧数据 高可用、性能优 缓存预热？原理？如何实现？ 缓存预热指在系统启动或者缓存过期时将数据提前加载到缓存中，减少加载数据时的延迟。\n优点：\n加快数据访问速度：通过将缓存中的数据加载到内存中，可以减少数据的访问时间，提高系统的性能 减少数据库访问次数：通过缓存预热，可以减少对数据库的访问次数，降低数据库的负载，提高数据库的性能。 提高用户体验：通过缓存预热，可以加快用户对数据的访问速度，提高用户的体验。 缺点：\n缓存预热需要将数据加载到内存中，这会增加系统的负载，可能会影响系统的性能 如果缓存中的数据与数据库中的数据不一致，可能会导致数据的不一致性问题 缓存中的数据可能会过期，需要及时更新缓存中的数据，以保证数据的一致性 如何实现缓存预热？\n定时任务实现缓存预热\n主类上加上@EnableScheduling 注解 写定时任务（加@Scheduled 注解），设置任务自动执行（cron 表达式），并加分布式锁保证集群下同一时刻只会执行一次任务 定时任务执行原理 Spring Scheduler 是 Spring 框架提供的一种定时任务调度工具，可以定期执行预定的任务。其执行原理是通过创建一个线程池来管理定时任务的执行线程，根据设定的时间表触发任务执行。每当定时任务触发时，Spring Scheduler 就会创建一个新的线程执行对应的任务逻辑。\n分布式锁的作用 使用分布式锁来保证首页加载任务时同一时刻只能有一个实例在执行，其他实例需要等待锁的释放。当一个实例获得锁后，开始执行首页加载任务，并在任务完成后释放锁，其他实例竞争获取锁并执行任务。\n这样可以保证只有一台机器能执行定时任务，避免多个机器重复写缓存，浪费资源。\n分布式锁？Redis 和 Redisson 实现？ 分布式锁是什么? 分布式锁可以保证在多 JVM 线程下锁监视器对所有锁的可见性，也就保证了集群下锁的互斥性与可见性\n分布式锁是一种用于在分布式系统中实现互斥访问的机制。在多个节点同时访问共享资源时，分布式锁可以确保只有一个节点能够获得锁，从而避免并发访问引发的数据冲突和竞争条件问题。\n作用：\n保护共享资源：在分布式系统中，多个节点可能同时访问共享资源，通过使用分布式锁，可以确保同一时间只有一个节点能够访问共享资源，避免冲突和数据不一致。 避免竞争条件：通过对关键操作加锁，分布式锁可以防止多个节点同时执行某个操作，避免竞争条件的出现，确保操作的顺序执行。 Redis 如何实现分布式锁？ Redis 实现分布式锁思路；\n利用 String 的set nx ex（SET if Not eXists）命令获取锁，并设置过期时间，保存线程标示。 释放锁时先判断线程标示是否与自己一致，一致则删除锁，删除对应的 keydel key，通过检查 key 是否存在来判断锁是否被成功释放。 锁判断和释放的流程必须在 Lua 脚本中执行，Lua 脚本可以在一个脚本中编写多条 Redis 命令，确保多条命令执行时的原子性。\nRedis 实现分布式锁的特点：\n利用set nx保证互斥性 利用set ex保证 Redis 发生故障时仍能及时释放锁，避免死锁问题，提高程序安全性。但要注意set nx ex一条命令执行，否则可能会出现数据不一致问题 利用 Redis 集群保证高可用和高并发特性 Redis 基于 setnx 实现的分布式锁存在的问题\n不可重入，同一个线程不能重复获得锁 不可重试，线程尝试获取锁失败就返回 false，不会重试 超时释放，锁超时释放虽然也可以避免死锁，但是在业务复杂的场景下仍可能出现问题 主从一致性，如果 Redis 提供了主从集群，主从同步存在延迟，当主宕机时，如果从机同步了主机中的锁数据，则会出现锁不一致性。 Redisson 实现分布式锁的原理 Redisson 实现分布式锁底层也是基于 Redis 的set nx ex命令和lua脚本，但在他的基础上做了增强。\nRedisson 加锁，设置过期时间等操作都是基于 lua 脚本执行，可以保证代码逻辑的一致性。\nRedisson 实现分布式锁流程：\n引入 Redisson 依赖 创建 RedissonConfig 配置类 在业务层注入 RedissonClient 客户端 Redisson 实现分布式锁如何合理的控制锁的有效时长？\n在 Redisson 的分布式锁中，提供一个看门狗（Watch Dog）机制，一个线程获得锁之后，在任务执行完之前，看门狗会给持有锁的线程无限续期，默认是每 10 秒续期一次\nRedisson 实现分布式锁是否可重入？\n可重入。\nRedisson 加锁的时候会记住该线程的标示，使用 hash 结构来存储线程信息和重入次数，如果第二次再尝试获取锁的时候能再次获得到锁，实现锁的重入。\nRedisson 实现的分布式锁是基于同一个线程的基础上的。\nRedisson 实现的分布式锁能保证主从一致性吗\n可以。\nRedisson 中实现的红锁（RedLock），不能再一个 Redis 实例上创建锁，应该是在多个 Redis 实例上创建锁（n / 2 + 1），避免在一个 Redis 实例上加锁。\n缺点：实现复杂、性能差\nRedisson 是一个基于 Redis 的 Java 驻留内存数据网格（In-Memory Data Grid），它提供了对分布式锁的支持。Redisson 可以通过 Redis 的原子操作来实现分布式锁，如 SETNX、EXPIRE 等命令。\n使用 Redisson 实现分布式锁具有以下优点：\n简单易用：Redisson API 提供了直观且友好的接口，可以方便地进行分布式锁的操作。 高性能：Redisson 作为一个专为高并发和高性能设计的分布式锁库，能够有效利用 Redis 的内存存储和响应快速的特性。 可靠性：Redisson 提供了多种策略来确保分布式锁的安全性，如重试和自动续租等机制。 以下是使用 Redisson 实现分布式锁的基本步骤：\n通过 Redisson 客户端实例化一个分布式锁对象。 使用该分布式锁对象调用lock()方法尝试获得锁。如果获取成功，可以继续执行临界区代码，否则会阻塞等待锁资源的释放。 执行完临界区代码后，通过调用unlock()方法释放锁，让其他节点可以获取到锁资源。 使用 Redisson 可以方便地实现基于 Redis 的分布式锁，同时它还提供了其他丰富的功能和特性，如可重入锁、公平锁、读写锁等。\n需要注意的是，在使用任何分布式锁的实现方式时，请谨慎处理异常情况、超时和释放锁的操作，以保证锁的正确释放和防止死锁的发生。\nRedis 中数据 key 的过期淘汰策略？ 假如 Redis 的 key 过期之后，会立即删除吗？=\u0026gt;key 的过期淘汰策略\n惰性删除：使用时发现过期就删除 定期删除：每隔一段时间随机删除过期 的 key 在 Redis 中，有以下几种常见的 key 过期淘汰策略：惰性删除+定期删除\n惰性删除（Eviction by Access）：当尝试获取一个 key 的值时，Redis 会先检查该 key 是否过期，如果过期则删除该 key。\n优点：可以避免定时删除带来的额外开销\n缺点：可能导致过期的 key 一直未被访问，从而一直占用内存，永远不会删除。\n定期删除（Eviction by Approximated Random Sampling）：**Redis 每隔一段时间，会随机选择一些 key，并检查它们是否过期，如果过期则删除。**这种策略通过随机方式，分散了删除操作的压力，减少了每次删除带来的开销。\n定期删除有两种模式：\nSLOW 模式：定时任务执行，执行频率默认为 10hz，每次不超过 25ms，可以通过修改配置文件 redis.conf 的 hz 选项来调整整个次数\nFAST 模式：执行频率不固定，但两次间隔不低于 2ms，每次耗时不超过 1ms\n优点：可以通过限制删除操作执行时长和频率来减少删除操作对 CPU 的影响。另外定期删除，也能有效释放过期键占用的内存\n缺点：难以确定删除操作执行的时长和频率\nRedis 的过期策略：惰性删除与定期删除配合使用\n其他 key 的过期删除策略：\n定时删除（Eviction by Timeout）：**当一个 key 设置了过期时间后，会在过期时间到达时被自动删除。**Redis 会通过定期扫描 key 的过期时间，并删除过期的 key。 内存淘汰策略：**当 Redis 中的内存达到指定阈值时，需要释放部分内存。**Redis 提供了不同的内存淘汰策略来决定删除哪些 key 来腾出内存空间。常见的内存淘汰策略包括 LFU（最少使用频率）、LRU（最近最少使用）和 Random（随机选择）等。 需要注意的是，Redis 的过期淘汰策略是近似策略，即不保证所有过期键都会立即被删除。具体的淘汰行为由 Redis 的配置参数决定，例如 maxmemory、maxmemory-policy 等。\n常见的内存淘汰策略包括 LFU（最少使用频率）、LRU（最近最少使用）和 Random（随机选择）\nRedis 的内存淘汰策略 内存淘汰策略：当 Redis 中的内存不够用时，此时向 Redis 中添加新的 Key，那么 Redis 就会按照一种规则将内存中的数据删掉，这种策略就是内存淘汰策略。\n**Redis 提供了以下几种常见的内存淘汰策略：**使用时要根据具体业务场景使用。\n策略 介绍 作用 VOLATILE-LRU LRU（最近最少使用）算法 主要用于临时数据的缓存，保证较新的数据留在缓存中 VOLATILE-LFU LFU（最少使用频率）算法 优先删除访问频率较低的数据，保留访问频率较高的数据。 VOLATILE-TTL 对象是设置了 TTL 的 key 剩余 TTL 值越小越先淘汰 NOEVICTION（默认） 不进行任何操作，对写入操作返回错误 ALLKEYS-LRU 在所有的 KEY 中使用 LRU 算法 用于缓存中的所有数据都是有价值的场景 ALLKEYS-LFU 在所有的 KEY 中使用 LFU 算法 优先删除访问频率较低的数据，保留访问频率较高的数据 ALLKEYS-RANDOM 随机选择要删除的 key 一般不用 这些淘汰策略可以通过配置文件或者通过CONFIG SET命令进行设置。例如，可以通过以下命令将内存淘汰策略设置为volatile-lru：\nCONFIG SET maxmemory-policy volatile-lru LRU：最近最少使用。用当前时间减去最后一次访问时间，这个值越大则淘汰优先级越高。\nLFU：最少使用频率。统计每个 key 的访问频率，优先淘汰使用频率最少的 Key\n数据库中有 1000 万数据，Redis 只能存储 20 万数据，如何保证 Redis 中的数据都是热点数据？\n使用ALLKEYS-LRU策略，最近最少使用的数据淘汰，留下来的都是经常访问的热点数据\nRedis 的内存用完了会怎么样？\n主要看数据淘汰策略是什么，如果是默认的配置（NOEVICTION），会直接报错\nRedis 如何实现秒杀逻辑？ 秒杀业务的优化思路是什么？\n先利用 Redis 完成库存余量、一人一单判断，完成抢单业务 再将下单业务放入阻塞队列，利用独立线程异步下单 Redis lua 脚本实现库存预验，讲一下逻辑 这个功能完全可以用代码实现，你为什么采用这个方式实现？目的？ 使用 Redis Lua 脚本实现库存预验的目的是为了在高并发环境下提供更快速和可靠的库存预验功能。Redis 是一个高性能的内存数据库，而 Lua 是一种嵌入式脚本语言，与 Redis 的服务器进行紧密集成。\n使用 Redis Lua 脚本可以在 Redis 服务器端以原子方式执行多个操作，确保不会被其他请求中断，并且减少了网络开销和协调开销。\n库存预验通常有以下逻辑：\n获取当前商品的库存数量（从 Redis 中获取）。 检查商品库存是否足够进行购买。比较库存数量和购买数量。 如果库存足够，减少商品库存数量（使用 Redis 的 DECRBY 或 HINCRBY 命令）。 返回库存预验结果给客户端。 使用 Redis Lua 脚本可以将这些步骤封装到一个原子操作中，从而确保在执行期间不会有其他并发操作修改库存数量。这样可以避免因为高并发请求导致的库存错误和数据不一致性问题。\n总结起来，使用 Redis Lua 脚本实现库存预验的目的是为了提供高性能、高并发和原子操作的库存预验功能，从而保证库存的准确性和可靠性。\nLua 脚本是什么？有什么作用？一定能保证原子性？ Lua 脚本是在一个脚本中编写多条 Redis 命令，确保多条命令执行时的原子性。\nRedis 实现分布式锁时最后判断锁和释放锁的逻辑就需要保证原子性，Lua 脚本就可以很好的解决，防止由于并发条件下锁再次释放错的情况发生。\nLua 脚本是 Redis 中的一种脚本语言，通过 Lua 脚本，可以在 Redis 服务器端执行复杂的业务逻辑。以下是 Lua 脚本在 Redis 中的作用：\n原子性操作：**Lua 脚本可以通过 Redis 的 EVAL 命令一次性发送给服务器执行，保证了脚本的原子性操作。**这在某些需要多个 Redis 命令才能完成的操作中非常有用，比如复杂的业务逻辑，减少了网络开销和多次交互。 事务支持：**通过 Lua 脚本，可以将多个 Redis 命令组合在一起形成一个原子性的事务，确保事务中的所有命令要么全部执行成功，要么全部回滚。**这样可以保证一系列操作的一致性，避免并发操作带来的数据不一致性问题。 原生操作：**Lua 脚本可以直接操作 Redis 的数据结构和命令，不需要额外的接口调用。**这使得编写复杂的数据操作变得更加方便和高效，还可以利用 Lua 的丰富特性和函数库进行更灵活的数据处理。 复杂计算和业务逻辑：Lua 脚本可以根据业务需求，编写复杂的计算逻辑，如数学计算、字符串处理、逻辑判断等。这样可以减轻应用服务器的负载，将一部分计算逻辑移到 Redis 服务器端执行，提高系统性能和响应速度。 lua 脚本一定能保证原子性吗？不一定\n在单线程环境下，Lua 脚本默认是原子执行的。因为 Lua 使用的是解释执行的方式，一个完整的 Lua 脚本会被连续地执行，直到执行结束，期间不会被中断。\n在多线程环境下，如果多个线程同时执行 Lua 脚本，那么 Lua 脚本的原子性就取决于具体的线程调度和同步机制。如果在多线程环境中需要保证 Lua 脚本的原子性，可以使用互斥锁或其他线程同步机制来控制对 Lua 脚本的访问。\n在分布式环境下，如果多个节点同时执行 Lua 脚本，那么 Lua 脚本的原子性取决于底层分布式系统的支持和实现。一些分布式系统（如 Redis）提供了事务和乐观锁等机制来保证对 Lua 脚本的原子性操作。\n需要注意的是，Lua 脚本的原子性只能保证脚本中的多个操作在执行过程中不会被中断，但无法完全保证数据的一致性和并发安全。在并发环境中，仍然需要考虑并发写操作、资源竞争、数据同步等问题，以确保数据的正确性和一致性。\n布隆过滤器了解吗？ **布隆过滤器（Bloom Filter）是一种空间效率很高的概率型数据结构，主要用于判断一个元素是否属于一个集合中。**它可以快速地判断一个元素是否在集合中，但是对于元素属于集合的判断可能会出现一定的误判。\n布隆过滤器本质上是一个位数组（bit array）和一系列哈希函数构成的数据结构。它的原理如下：\n初始化：创建一个长度为 m 的位数组，将所有的位都初始化为 0。 添加元素：将要添加的元素分别经过 k 个不同的哈希函数，生成 k 个哈希值，并将位数组对应位置的位设为 1。 判断元素是否存在：将要查找的元素经过 k 个哈希函数，生成 k 个哈希值。判断位数组对应位置的位是否都为 1，如果都为 1，则认为该元素可能存在于集合中；如果有任何一个位为 0，则认为该元素一定不存在于集合中。 减少误判概率的方法有两种：\n增加位数组的长度（m）：增加位数组的长度可以降低冲突的概率，从而减小误判的可能性。但是，这会增加布隆过滤器的空间占用。 增加哈希函数的数量（k）：增加哈希函数的数量可以使得元素落在不同的位上的概率更均匀，从而减小冲突的概率。但是，增加哈希函数的数量会增加计算的开销。 由于其高效的空间利用率和快速的查询速度，布隆过滤器在很多场景下都有用武之地。例如，在网络缓存中，可以用来判断请求的内容是否已经缓存；在数据库中，可以用于去重，避免插入重复的数据；在搜索引擎中，可以用于过滤掉不可能存在的文档等等。\nRedis 中的 String 数据结构底层 Redis 中的任意数据类型的键和值都会被封装成一个 RedisObject，即 Redis 对象。会在 Redis 对象中主要由三个字段：当前数据结构的类型、编码方式和指向实际数据的指针。\nString 类型，首先会在 Redis 对象中指明他的类型为 OBJ_STRING。\nString 类型有三种编码方式：\n1、其最基本的编码方式是RAW编码，基于简单动态字符串 SDS来实现，存储上限为 512mb。他就是在内存中申请一个 SDS，然后让 Redis 对象的指针指向 SDS 即可。\n2、当要存储的 SDS 的长度小于 44 字节，就会使用EMBSTR编码，此时 Redis 对象的头部和 SDS 是在一个连续的内存空间中，申请内存的时候只需要一次分配，效率更高。为什么是 44 个字节呢？因为 Redis 对象数据结构中其头部有 16 个字节。然后 SDS 中头部 3 个字节，尾巴结束字符一个字节，加在一起共 20 个字节，20 个字节加上 44 个字节就是 64 个字节，因为 redis 内存分配会以 2 的 n 次方进行分配，64 个字节不会产生内存碎片。\n3、如果要存储的字符串是一个整数，且大小不超过 8 位二进制能表示的范围。就采用int的编码方式，直接让 redis 对象的指针位设置为这个整数\nRedis 的数据类型 Sorted Set（zset）以及底层实现机制？ zset 的功能和 sets 类似，但是它在集合内的元素是有顺序，不能重复的。所以适合做排行榜之类的功能。\n它底层实现机制的实现方式有两种，分别为 ziplist（压缩列表） 或者 skiplist（跳跃表）\n它们的区别为：\n底层使用的数据结构实现不同：ziplist 编码的有序集合对象使用压缩列表作为底层实现，而 skiplist 编码的有序集合对象使用 zset 结构作为底层实现。 底层集合元素保存的方式不同；ziplist 中的每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员，第二个节点保存元素的分值。并且压缩列表内的集合元素按分值从小到大的顺序进行排列，小的放置在靠近表头的位置，大的放置在靠近表尾的位置。skiplist 的一个 zset 结构同时包含一个字典和一个跳跃表。字典的键保存元素的值，字典的值则保存元素的分值；跳跃表节点的 object 属性保存元素的成员，跳跃表节点的 score 属性保存元素的分值。 当有序集合对象保存的元素数量小于 128 个，并且保存的所有元素长度都小于 64 字节时，对象使用 ziplist 编码。否则使用 skiplist 编码。 讲一下 Redis 的 Zset Zset 类型的底层数据结构是由压缩列表或跳表实现的：\n如果有序集合的元素个数小于128个，并且每个元素的值小于64字节时，Redis 会使用压缩列表作为 Zset 类型的底层数据结构； 如果有序集合的元素不满足上面的条件，Redis 会使用跳表作为 Zset 类型的底层数据结构； 在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。\nZSet 的范围查询时间复杂度是多少？ Redis 的 ZSet 的范围查询命令 ZRANGE 的时间复杂度是 O(log(N)+M)，其中 N 是有序集合的元素数量，M 是返回的元素数量。\n讲一下跳表 链表在查找元素的时候，因为需要逐一查找，所以查询效率非常低，时间复杂度是 O(N)，于是就出现了跳表。跳表是在链表基础上改进过来的，实现了一种「多层」的有序链表，这样的好处是能快读定位数据。\n那跳表长什么样呢？我这里举个例子，下图展示了一个层级为 3 的跳表。\n图中头节点有 L0~L2 三个头指针，分别指向了不同层级的节点，然后每个层级的节点都通过指针连接起来：\nL0 层级共有 5 个节点，分别是节点 1、2、3、4、5； L1 层级共有 3 个节点，分别是节点 2、3、5； L2 层级只有 1 个节点，也就是节点 3 。 如果我们要在链表中查找节点 4 这个元素，只能从头开始遍历链表，需要查找 4 次，而使用了跳表后，只需要查找 2 次就能定位到节点 4，因为可以在头节点直接从 L2 层级跳到节点 3，然后再往前遍历找到节点 4。\n可以看到，这个查找过程就是在多个层级上跳来跳去，最后定位到元素。当数据量很大时，跳表的查找复杂度就是 O(logN)。\n跳表是怎么设置层高的？ 跳表在创建节点时候，会生成范围为[0-1]的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数。\nRedis 实现排行榜功能？详细说 Redis 实现排行榜是 Redis 中一个很常见的场景，主要使用的是 ZSet 进行实现，下面是为什么选用 ZSet：\n有序性：排行榜肯定需要实现一个排序的功能，在 Redis 中有序的数据结构有 List 和 ZSet； 支持分数操作：ZSet 可以对集合中的元素进行增删改查操作，十分贴合排行榜中用户分数动态变化的场景，而 List 并不能针对分数进行操作，只有其中的 value 进行操作； 支持范围查询：zrange 命令可以按照分数进行范围查询，如排行榜中的 Top10 需求就可通过该特性进行实现； 支持去重：由于 ZSet 属于 Set 的特殊数据结构，因此同样拥有 Set 不可重复的特性，对于排行榜中不可出现重复项的需求也十分贴合，而 List 只能手动去重。 因此选择 ZSet 实现排行榜相对于 List 实现会更合适和高效。\n以学生成绩排行为例，下面是使用 Redis 命令实现\n# 添加示例数据 ZADD scores 90 \u0026#34;张三\u0026#34; ZADD scores 85 \u0026#34;李四\u0026#34; ZADD scores 95 \u0026#34;王五\u0026#34; ZADD scores 92 \u0026#34;赵六\u0026#34; # 查询排名前3的学生信息 ZRANGE scores 0 2 WITHSCORES # 查询排名前3的打印 1) \u0026#34;王五\u0026#34; 2) \u0026#34;95\u0026#34; 3) \u0026#34;赵六\u0026#34; 4) \u0026#34;92\u0026#34; 5) \u0026#34;张三\u0026#34; 6) \u0026#34;90\u0026#34; # 删除学生“李四”的成绩信息 ZREM scores \u0026#34;李四\u0026#34; 下面是 SpringBoot 整合 Redis 进行实现\n// 添加学生成绩 public void addScore(String name, int score) { redisTemplate.opsForZSet().add(\u0026#34;scores\u0026#34;, name, score); } // 查询排名前N的学生成绩 public List\u0026lt;Map.Entry\u0026lt;String, Double\u0026gt;\u0026gt; getTopScores(int n) { return redisTemplate.opsForZSet().reverseRangeWithScores(\u0026#34;scores\u0026#34;, 0, n - 1) .stream() .map(tuple -\u0026gt; new AbstractMap.SimpleEntry\u0026lt;\u0026gt;(tuple.getValue(), tuple.getScore())) .collect(Collectors.toList()); } // 删除某个学生的成绩 public void removeScore(String name) { redisTemplate.opsForZSet().remove(\u0026#34;scores\u0026#34;, name); } 要实现排行榜功能，可以使用 Redis 的有序集合（Sorted Set）数据结构。以下是一种具体的实现方式：\n将用户的得分作为有序集合的分值，用户 ID 作为有序集合的成员。\nZADD leaderboard \u0026lt;score\u0026gt; \u0026lt;member\u0026gt; 获取排行榜前 N 名用户：\nZREVRANGE leaderboard 0 \u0026lt;N-1\u0026gt; WITHSCORES 获取用户的排名：\nZREVRANK leaderboard \u0026lt;member\u0026gt; 获取用户的得分：\nZSCORE leaderboard \u0026lt;member\u0026gt; 增加用户的得分：\nZINCRBY leaderboard \u0026lt;increment\u0026gt; \u0026lt;member\u0026gt; 移除用户：\nZREM leaderboard \u0026lt;member\u0026gt; 注意事项：\n在使用有序集合的时候，分值需要为一个可比较的类型，通常使用浮点数或整数作为分值。 排行榜的排序是根据分值从高到低进行的。 如果多个用户的分值相同，可以根据需要执行额外的判断规则来确定用户的排名。 为了保证数据一致性，可以对 Redis 中的有序集合进行定期的持久化操作，比如使用 RDB 快照或者 AOF 日志等方式来进行数据的持久化。 需要根据具体需求来调整和扩展实现方式，例如可以添加时间维度来计算带有时间窗口的排行榜，或者添加其他的用户属性等。\nRedis 是单线程？ Redis 的单线程模型？ 在 Redis 6.0 以前，Redis 的核心网络模型选择用单线程来实现。\n对于一个 DB 来说，CPU 通常不会是瓶颈，因为大多数请求不会是 CPU 密集型的，而是 I/O 密集型。具体到 Redis 的话，如果不考虑 RDB/AOF 等持久化方案，Redis 是完全的纯内存操作，执行速度是非常快的，因此这部分操作通常不会是性能瓶颈，Redis 真正的性能瓶颈在于网络 I/O，也就是客户端和服务端之间的网络传输延迟，因此 Redis 选择了单线程的 I/O 多路复用来实现它的核心网络模型。\n实际上更加具体的选择单线程的原因如下：\n避免过多的上下文切换开销：如果是单线程则可以规避进程内频繁的线程切换开销，因为程序始终运行在进程中单个线程内，没有多线程切换的场景。 避免同步机制的开销：如果 Redis 选择多线程模型，又因为 Redis 是一个数据库，那么势必涉及到底层数据同步的问题，则必然会引入某些同步机制，比如锁，而我们知道 Redis 不仅仅提供了简单的 key-value 数据结构，还有 list、set 和 hash 等等其他丰富的数据结构，而不同的数据结构对同步访问的加锁粒度又不尽相同，可能会导致在操作数据过程中带来很多加锁解锁的开销，增加程序复杂度的同时还会降低性能。 简单可维护：如果 Redis 使用多线程模式，那么所有的底层数据结构都必须实现成线程安全的，这无疑又使得 Redis 的实现变得更加复杂。 总而言之，Redis 选择单线程可以说是多方博弈之后的一种权衡：在保证足够的性能表现之下，使用单线程保持代码的简单和可维护性。\nIO 多路复用是什么？ **IO 多路复用是指内核一旦发现进程指定的一个或者多个 IO 条件准备读取，它就通知该进程。**IO 多路复用适用如下场合：\n当客户处理多个描述字时（一般是交互式输入和网络套接口），必须使用 I/O 复用。\n当一个客户同时处理多个套接口时，而这种情况是可能的，但很少出现。\n如果一个 TCP 服务器既要处理监听套接口，又要处理已连接套接口，一般也要用到 I/O 复用。\n如果一个服务器即要处理 TCP，又要处理 UDP，一般要使用 I/O 复用。\n如果一个服务器要处理多个服务或多个协议，一般要使用 I/O 复用。\n与多进程和多线程技术相比，I/O 多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。\nRedis 中其中一个操作阻塞会对其他操作有影响吗？ 是的，Redis 在默认情况下是单线程的，并且所有的操作都是按顺序执行的。如果有一个耗时的操作阻塞了其他操作，那么后面的操作将会受到影响。\n当有一个读写操作阻塞了其他操作时，这个问题被称为\u0026quot;一个人坏了一锅粥\u0026quot;的情况。**由于 Redis 是单线程的，当一个操作被阻塞时，后续的操作将无法得到执行，导致其他操作也被阻塞。**这会导致服务器性能下降，并且可能引起系统响应时间延长。\n为了避免这种情况，有几个方法可以考虑：\n使用 Redis 集群：通过将数据分布在多个节点上，每个节点处理自己的子集数据，从而提高整体性能和并发性能。 合理设计数据结构：使用合适的数据结构和算法来减少操作的复杂度和耗时。 使用 Redis 的 pipeline 或者批量操作命令：将多个操作批量发送给 Redis 服务器，以减少网络通信开销。 使用 Redis 的异步命令：将一些耗时的操作转为异步执行，不阻塞主线程。 同时，需要根据实际场景评估系统的性能和负载，做好容量规划和资源管理，以提高系统的并发处理能力。\nRedis 6.0 之后为何引入了多线程？6.0 之前为什么不使用多线程？ Redis 6.0 之后，引入多线程主要是为了解决 Redis 在处理大规模数据时，单线程的性能瓶颈问题，以提高 Redis 的处理能力和效率。\n在 Redis 6.0 之前，Redis 只采用单线程的方式进行处理，这是因为 Redis 的核心是一个基于内存的键值对存储系统，它的瓶颈主要在于 CPU 的计算能力，而不是 I/O 操作的速度。\n在单线程模式下，Redis 可以使用简单的事件驱动模型，来实现高效的网络通信和事件处理，避免了线程切换和上下文切换带来的开销，同时也避免了多线程之间的锁竞争问题。因此，在 6.0 之前，Redis 一直采用单线程模式运行。\nRedis 6.0 之前为什么不使用多线程，主要有以下几个原因：\nRedis 单线程模型相对简单，容易维护和调试，代码逻辑也比较清晰。 Redis 的主要瓶颈在于 CPU，而不是 I/O，因此采用多线程模型并不能显著提高性能。 Redis 是一个内存型数据库，它的性能主要受到 CPU 和内存带宽的限制。采用多线程模型会增加线程之间的竞争和锁等开销，反而可能降低 Redis 的性能。 但是随着 Redis 的应用场景不断扩大和升级，Redis 也面临着越来越大规模、越来越高并发的挑战，单线程模式已经不能满足这些需求了。因此，在 Redis 6.0 中引入了多线程技术，以利用多核 CPU 的计算能力，提高 Redis 的性能和处理能力。\nRedis 6.0 引入多线程后，采用了多种技术手段来实现多线程操作的安全和稳定性，如使用锁和原子操作来保证数据一致性和线程安全。\n需要注意的是：\n**Redis 6.0 中的多线程并不是完全替代了单线程模型，而是在其基础上引入了多线程支持，通过将一些负载耗时的操作（如 I/O 操作）交给后台线程处理，从而提高 Redis 的性能。**同时，在多线程模式下，Redis 仍然保留了所有的单线程模式特性，如 ACID 事务等。 **Redis 6.0 中多线程的使用是可选的，并且可以通过配置文件进行启用或禁用，**以便在不同的应用场景下选择最适合的运行模式。 Redis 事务的 ACID？ Redis 能实现 ACID 属性吗？\nRedis 的事务可以保证原子性吗？为什么？\nRedis 的事务可以保证一致性吗？为什么？\nRedis 的事务可以保证隔离性吗？为什么？\nRedis 的事务可以保证持久性吗？为什么？\nRedis 中的事务是否支持回滚？\nRedis 并不严格遵守 ACID（原子性、一致性、隔离性和持久性）属性。Redis 是一个内存数据库，主要注重高性能和可扩展性，而不是严格遵守事务的特性。\n**Redis 的事务可以保证原子性。**在一个 Redis 事务中的一系列操作会被一次性地执行，要么全部执行成功，要么全部执行失败。这是通过 MULTI、EXEC 和 DISCARD 命令来实现的。在 EXEC 执行前，所有的操作会被放入一个队列中，EXEC 只会按顺序执行队列中的命令，期间不会被其他客户端的命令中断。\n**Redis 的事务不能保证一致性。**事务内的命令在执行过程中可能会被其他客户端的命令所修改，导致事务最终的结果与预期不符。Redis 在执行事务期间不会对命令进行隔离，而是按照先到先得的原则执行命令。\n**Redis 的事务不能保证隔离性。**多个客户端的命令可以并发地访问和修改 Redis 的数据，而无法保证事务的隔离性。在一个事务执行过程中，如果有其他客户端对同样的数据进行修改，那么事务提交后的结果可能不是预期的。\n**Redis 的事务不能保证持久性。**Redis 默认将数据存储在内存中，虽然可以通过持久化机制（RDB 和 AOF）将数据写入磁盘，但事务提交的数据只会保存在内存中，并没有立即写入磁盘。如果在事务提交后发生了服务器宕机或其他故障，可能会导致部分或全部事务数据的丢失。\n**Redis 中的事务是支持回滚的。**在执行事务过程中，如果某个命令执行出错，事务中的其他命令不会受到影响，且发生错误命令之后的命令也不会被执行。可以通过执行 DISCARD 命令来取消事务并清空队列，或者通过执行 EXEC 命令来提交事务。\nRedis 为什么快？ 来自:Starry、编程导航官方\n1、纯内存操作\nRedis 是基于内存的数据存储系统，绝大部分请求是纯粹的内存操作。\n2、单线程操作，避免了频繁的上下文切换\nRedis 的单线程操作是指，Redis 使用一个主线程来处理所有的客户端请求和数据操作，不会创建新的线程来处理请求。这种单线程模型的优点是可以避免多线程并发访问共享数据时的竞争和死锁问题，从而提高了 Redis 的性能和稳定性。此外，由于 Redis 的内存访问速度非常快，因此单线程处理请求也能够保证足够的性能。\n3、采用了非阻塞 I/O 多路复用机制\n为了实现单线程模型，Redis 使用了 IO 多路复用技术。IO 多路复用是指操作系统提供的一种 IO 模型，可以让一个进程同时监听多个 IO 事件（如读写事件），并在有事件发生时通知进程，从而实现并发处理 IO 事件。\n具体来说，在 Redis 中，客户端的请求是由一个单线程来处理的，而 IO 操作却是通过 epoll 多路复用技术实现的。\nRedis 单线程情况下，内核会一直监听 socket 上的连接请求或者数据请求，一旦有请求到达就交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。\nselect/epoll 提供了基于事件的回调机制，即针对不同事件的发生，调用相应的事件处理器。所以 Redis 一直在处理事件，提升 Redis 的响应性能。\n4、精简高效的数据结构\nRedis 内置了多种高效的数据结构，如哈希表、跳表等，这些数据结构的实现非常精简高效，减少了 Redis 对内存和 CPU 的占用，从而提高了 Redis 的性能。\n5、持久化策略\nRedis 支持多种持久化策略，如 RDB（快照）和 AOF（追加式文件）等，这些策略可以将内存中的数据保存到硬盘中，以保证数据的持久性和安全性。同时，Redis 可以将数据以压缩的方式存储在硬盘中，减少了硬盘的占用，提高了数据的读写速度。\nRedis 挂了如何补救？ 当 Redis 挂了（即 Redis 服务不可用），需要采取一系列措施来进行补救和恢复服务。下面是应对 Redis 挂掉的一些常见措施：\n监控和报警： 在 Redis 运行时，需要设置监控系统来实时监测 Redis 的状态和性能指标。一旦 Redis 挂掉，监控系统能够及时发现并触发报警，通知相关人员进行处理。\n查找原因： 一旦发现 Redis 挂了，需要立即查找造成 Redis 挂掉的原因。可以查看 Redis 日志和系统日志，以及监控系统的报警信息，来了解 Redis 挂掉的具体情况和异常现象。\n重启 Redis： 如果 Redis 挂掉是由于某个临时问题导致的，比如内存不足、连接数过大等，可以尝试通过重启 Redis 来恢复服务。在重启前最好备份好数据，确保数据不会丢失。\n恢复数据： 如果 Redis 挂掉后数据有损坏或丢失，可**以通过 Redis 的持久化机制（RDB 快照或 AOF 日志）来进行数据恢复。**根据情况，可以选择从 RDB 快照文件或 AOF 日志文件恢复数据。\n故障转移： 在分布式环境中，可以通过主从复制或哨兵模式来实现 Redis 的高可用性。如果 Redis 挂掉，其他备用的 Redis 节点可以接管服务，从而实现故障转移，减少服务中断时间。\n分析和优化： 一旦 Redis 服务恢复正常，需要对 Redis 进行性能分析和优化，找出造成 Redis 挂掉的根本原因，以防止类似问题再次发生。可能需要调整 Redis 的配置、增加硬件资源、优化代码等。\n灾备和冗余： 为了应对 Redis 挂掉的风险，可以采取灾备和冗余措施。可以设置多个 Redis 节点进行数据备份和冗余，保证在某个节点挂掉时，其他节点能够继续提供服务。\n自动化恢复： 对于重要的 Redis 服务，可以考虑实现自动化的恢复机制，比如自动进行数据备份和恢复、自动进行故障转移等，以降低人为操作的风险。\n总的来说，解决 Redis 挂掉的问题需要综合考虑监控、预防、备份和恢复等方面的措施。在高可用性和容错性要求较高的场景中，可以结合使用 Redis 的集群模式、哨兵模式或使用高可用的 Redis 代理（如 Redis Sentinel 或 Redis Cluster）等技术，以确保系统持续稳定地运行。\nRedis 的持久化 如何实现 Redis 的持久化？ Redis 提供了两种持久化方式，分别是 RDB（Redis DataBase）和 AOF（Append Only File）。\nRDB 持久化：（Redis Database Backup file）\nRDB 持久化是将 Redis 在某个时间点的数据保存到磁盘上的一个快照文件。该文件是一个二进制文件，包含了 Redis 在某个时刻的所有数据和状态。RDB 持久化适用于数据的备份和灾难恢复。\n实现步骤：\n配置 Redis 的redis.conf文件，启用 RDB 持久化功能：save 900 1表示在 900 秒（15 分钟）内，如果至少有 1 个 key 发生变化，则触发保存。 在需要手动触发 RDB 持久化时，可以使用SAVE或BGSAVE命令。SAVE命令会阻塞 Redis 服务器，直到 RDB 持久化完成；BGSAVE命令会创建一个子进程来进行持久化，避免主进程受到影响。 AOF 持久化：（Append Only File）\nAOF 持久化是将 Redis 的写操作追加到一个只能追加的日志文件中，也就是将每个写操作以追加的方式写入到 AOF 文件。该文件保存了所有能够还原服务器状态的写命令，因此 AOF 持久化适用于数据的完整性和恢复性。\n实现步骤：\n配置 Redis 的redis.conf文件，启用 AOF 持久化功能：appendonly yes。（AOF 默认是关闭的） Redis 在每次执行写操作（增删改）时，会将相应的写命令追加到 AOF 文件中。 AOF 文件会随着写操作的增加而不断增大，因此可以设置 AOF 文件的重写机制，使用BGREWRITEAOF命令来对 AOF 文件进行重写，去除冗余命令。用最少的命令达到相同的结果 set name lili set age 20 set k v bgrewriteaof //等同于使用 mset name 可以选择使用 RDB 持久化、AOF 持久化或者两者同时使用，这取决于你的需求和配置。使用 RDB 持久化可以节省磁盘空间，但可能会有数据丢失的风险；使用 AOF 持久化可以保证数据的完整性，但可能会增加磁盘的 IO 负载。\n需要注意的是，持久化功能会对 Redis 的性能产生一定的影响，因为数据持久化需要频繁地进行 IO 操作。\nRedis 的 RDB 和 AOF 是什么？各自的优缺点是什么？ Redis 的 RDB（Redis DataBase）和 AOF（Append Only File）是两种持久化方式，用于将 Redis 的数据保存到磁盘上，以便在 Redis 重启或崩溃后可以恢复数据。\nRDB（Redis DataBase）：RDB 持久化是将 Redis 在某个时间点的数据保存到磁盘上的一个快照文件。该文件是一个二进制文件，包含了 Redis 在某个时刻的所有数据和状态。RDB 持久化适用于数据的备份和灾难恢复。\nRDB 持久化通过设置不同的策略来触发数据的保存，比如可以设置在一段时间内有多少个写操作发生时触发保存。 执行 RDB 持久化时，Redis 主进程会 fork 出一个子进程，由子进程负责将数据写入到磁盘，这样可以避免阻塞主进程。 AOF（Append Only File）：AOF 持久化是将 Redis 的写操作追加到一个只能追加的日志文件中，也就是将每个写操作以追加的方式写入到 AOF 文件。该文件保存了所有能够还原服务器状态的写命令，通过重放这些写命令，可以还原数据到 Redis 重启前的状态。\nAOF 持久化通过将写命令以文本方式追加到 AOF 文件中，可以保证数据的完整性，但会带来更高的 IO 负载。 AOF 文件会不断增长，为了避免 AOF 文件过大，Redis 提供了 AOF 重写功能，可以通过BGREWRITEAOF命令将 AOF 文件重写为一份更小的文件，去除了一些重复或不必要的写命令。 RDB 与 AOF 区别：\nRDB 是将整个数据集保存为二进制文件，是一种全量持久化方式，适用于数据备份和灾难恢复。而 AOF 是将写操作追加到日志文件中，是一种增量持久化方式，适用于数据的完整性和恢复性。\n磁盘空间大小：\nRDB 适合在数据集比较大时进行备份操作，生成一个非常紧凑、经过压缩的数据文件。 AOF 文件记录了 Redis 执行的所有操作命令，可以确保数据不丢失，AOF 文件更大，占用磁盘空间更多 宕机恢复速度：\nRDB 机制在 Redis 重启时比 AOF 机制更快地将 Redis 恢复到内存中，因为 AOF 机制要重新执行 AOF 文件中的所有操作命令。 Redis 宕机恢复的速度较快，但可能会丢失最后一次 RDB 持久化后的数据。 数据丢失问题：\nRDB 机制可能会出现数据丢失，因为数据是周期性地进行备份，一旦 Redis 出现问题并且上一次备份之后还没有进行过数据变更，那么这部分数据将会丢失。 AOF 机制比 RDB 机制更加可靠，因为 AOF 文件记录了 Redis 执行的所有操作命令，可以确保数据不丢失。 备份操作安全：\nRDB 机制会造成一定的 IO 压力，当数据集比较大时，进行备份操作可能会阻塞 Redis 服务器进程。 AOF 机制在恢复大数据集时更加稳健，因为 AOF 文件记录了数据的操作过程，可以确保每一次操作都被正确地执行。 可以选择同时使用 RDB 和 AOF，即 RDB 作为数据备份，AOF 作为数据恢复。也可以根据实际需求只使用其中一种持久化方式。\n综上所述，RDB 适合用于数据集较大、备份、恢复数据和迁移数据等场景，AOF 适合用于数据可靠性要求高、数据恢复稳健等场景。\nRDB 的执行原理？ bgsave 开始时会 fork 主进程得到子进程，子进程共享主进程的内存数据。完成 fork 后读取内存数据并写入 RDB 文件。\n子进程是异步操作\nfork 采用的是 copy-on-write 技术：\n当主进程执行读操作时，访问共享内存 当主进程执行写操作时，则会拷贝一份数据，执行写操作 如果主线程和子线程同时读写呢？\n主线程的写操作：如果主线程在写操作时修改了共享内存的数据，此时会触发 copy-on-write 机制，会为子进程分配独立的内存页，并将原始的数据复制一份到子进程的内存中进行操作。这样，主线程的写操作不会影响子进程的数据。 主线程的读操作：如果主线程在读操作时，不会触发 copy-on-write 机制，而是继续共享内存数据。因为读操作不会修改数据，所以不需要创建新的内存页。 子线程的写操作：由于子进程是异步执行的，所以在主进程 fork 的时候，子进程会复制一份主进程的内存数据，之后就独立于主进程，可以进行自己的写操作，不会影响主进程的数据 触发保存： 当配置了 RDB 持久化并且满足一定条件（比如在一定时间内有一定数量的写操作）时，Redis 会自动触发 RDB 持久化。此时，Redis 会将数据集快照保存到磁盘上的一个 RDB 文件中。 生成数据快照： 在进行 RDB 持久化时，Redis 会生成一个数据集的快照。这个快照包含了当前内存中的所有数据，包括键、值、过期时间等信息。 写入临时文件： Redis 首先会创建一个临时的 RDB 文件，将数据快照写入这个临时文件中。 替换原文件： 一旦临时文件写入完成，Redis 会用新生成的 RDB 文件替换掉之前的旧 RDB 文件。这个过程通常是原子的，以确保在替换时不会丢失数据。 持久化完成： 一旦 RDB 文件替换完成，持久化过程就算完成了。此时，Redis 的数据已经被持久化到了磁盘上的 RDB 文件中。 需要注意的是，RDB 持久化是一个点对点的持久化方式，它会在一定时间间隔内生成完整的数据快照。由于 RDB 文件只包含了一份数据的快照，它适合用于备份和灾难恢复。但也有一些潜在的问题，比如如果在持久化间隔之间发生了故障，可能会丢失数据。因此，很多情况下，Redis 会与 AOF（Append-Only File）日志一起使用，以提供更可靠的持久化和恢复机制。\nRedis 单点吞吐量有多少？ 单点 TPS 达到 8 万/秒，QPS 达到 10 万/秒。TPS 和 QPS 的意思：\n**QPS：应用系统每秒钟最大能接受的用户访问量。**每秒钟处理完请求的次数，注意这里是处理完，具体是指发出请求到服务器处理完成功返回结果。可以理解在 Server 中有个 counter，每处理一个请求加 1，1s 后 counter=QPS。 **TPS：每秒钟最大能处理的请求数。**每秒钟处理完的事务次数，一个应用系统 1s 能完成多少事务处理，一个事务在分布式处理中，可能会对应多个请求，对于衡量单个接口服务的处理能力，用 QPS 比较合理。 渐进式 rehash 实现过程？ Redis 使用渐进式 rehash（渐进式哈希）来进行数据迁移。\n渐进式 rehash 将原有的哈希槽分为两个部分：源哈希槽和目标哈希槽。在 rehash 过程中，Redis 将逐步将源哈希槽中的键值对迁移到目标哈希槽中。\n以下是渐进式 rehash 的大致步骤：\nRedis 创建一个与当前哈希槽数量相同的目标哈希槽，并将每个目标哈希槽都设置为空哈希表。\nRedis 将哈希表的 rehashidx 属性设置为 0，表示开始从第一个源哈希槽开始迁移。\nRedis 在每个事件循环（event loop）中，会尝试迁移一小部分的键值对，以避免长时间阻塞。默认每次迁移不超过 1ms，避免对性能造成较大影响。\n每次迁移时，Redis 会将源哈希槽中的某些键值对迁移到对应的目标哈希槽中。此时，对这些键值对的读取和写入操作会同时在源哈希槽和目标哈希槽进行，直到所有对源哈希槽的键值对迁移完成。\n迁移完成后，Redis 将释放源哈希槽并将目标哈希槽设置为新的源哈希槽。并将 rehashidx 属性更新为下一个源哈希槽的索引，再次进行迁移操作。\n当所有的源哈希槽都被迁移到目标哈希槽后，Redis 将完成整个渐进式 rehash 过程，此时哈希槽的数量也会变为目标哈希槽的数量。\n渐进式 rehash 的好处是可以在数据迁移过程中保持 Redis 正常运行，尽可能地减少对系统性能的影响，并且保证在迁移过程中的数据访问的一致性。在 rehash 过程中，Redis 通过并行处理读写操作和渐进迁移，确保能够同时处理新旧哈希槽的操作，保持系统的可用性。\n哈希表是怎么扩容的？ 为了避免 rehash 在数据迁移过程中，因拷贝数据的耗时，影响 Redis 性能的情况，所以 Redis 采用了渐进式 rehash，也就是将数据的迁移的工作不再是一次性迁移完成，而是分多次迁移。\n渐进式 rehash 步骤如下：\n给「哈希表 2」 分配空间； 在 rehash 进行期间，每次哈希表元素进行新增、删除、查找或者更新操作时，Redis 除了会执行对应的操作之外，还会顺序将「哈希表 1 」中索引位置上的所有 key-value 迁移到「哈希表 2」 上； 随着处理客户端发起的哈希表操作请求数量越多，最终在某个时间点会把「哈希表 1 」的所有 key-value 迁移到「哈希表 2」，从而完成 rehash 操作。 这样就巧妙地把一次性大量数据迁移工作的开销，分摊到了多次处理请求的过程中，避免了一次性 rehash 的耗时操作。\n在进行渐进式 rehash 的过程中，会有两个哈希表，所以在渐进式 rehash 进行期间，哈希表元素的删除、查找、更新等操作都会在这两个哈希表进行。\n哈希表扩容的时候，有读请求怎么查？ 查找一个 key 的值的话，先会在「哈希表 1」 里面进行查找，如果没找到，就会继续到哈希表 2 里面进行找到。\nRedis 热点 Key 问题 Redis 大 key 如何解决？ 对大 Key 进行拆分。例如将含有数万成员的一个 HASH Key 拆分为多个 HASH Key，并确保每个 Key 的成员数量在合理范围。在 Redis 集群架构中，拆分大 Key 能对数据分片间的内存平衡起到显著作用。 对大 Key 进行清理。将不适用 Redis 能力的数据存至其它存储，并在 Redis 中删除此类数据。注意，要使用异步删除。 监控 Redis 的内存水位。可以通过监控系统设置合理的 Redis 内存报警阈值进行提醒，例如 Redis 内存使用率超过 70%、Redis 的内存在 1 小时内增长率超过 20%等。 对过期数据进行定期清。堆积大量过期数据会造成大 Key 的产生，例如在 HASH 数据类型中以增量的形式不断写入大量数据而忽略了数据的时效性。可以通过定时任务的方式对失效数据进行清理。 什么是热 key？ 通常以其接收到的 Key 被请求频率来判定，例如：\nQPS 集中在特定的 Key：Redis 实例的总 QPS（每秒查询率）为 10,000，而其中一个 Key 的每秒访问量达到了 7,000。 带宽使用率集中在特定的 Key：对一个拥有上千个成员且总大小为 1 MB 的 HASH Key 每秒发送大量的HGETALL操作请求。 CPU 使用时间占比集中在特定的 Key：对一个拥有数万个成员的 Key（ZSET 类型）每秒发送大量的ZRANGE操作请求。 如何解决热 key 问题？ 在 Redis 集群架构中对热 Key 进行复制。在 Redis 集群架构中，由于热 Key 的迁移粒度问题，无法将请求分散至其他数据分片，导致单个数据分片的压力无法下降。此时，可以将对应热 Key 进行复制并迁移至其他数据分片，例如将热 Key foo 复制出 3 个内容完全一样的 Key 并名为 foo2、foo3、foo4，将这三个 Key 迁移到其他数据分片来解决单个数据分片的热 Key 压力。 使用读写分离架构。如果热 Key 的产生来自于读请求，您可以将实例改造成读写分离架构来降低每个数据分片的读请求压力，甚至可以不断地增加从节点。但是读写分离架构在增加业务代码复杂度的同时，也会增加 Redis 集群架构复杂度。不仅要为多个从节点提供转发层（如 Proxy，LVS 等）来实现负载均衡，还要考虑从节点数量显著增加后带来故障率增加的问题。Redis 集群架构变更会为监控、运维、故障处理带来了更大的挑战。 ","permalink":"https://lidengxm.github.io/posts/java/redis%E5%85%AB%E8%82%A1/","summary":"Redis 是什么？使用场景有哪些？ Redis 是什么？ **Redis（Remote Dictionary Server）是一个开源的内存数据存储系统，它被广泛用作缓存、消息队列和数据库。**Redis 支持多种数据结构，包括字符串、列表、集合、有序集合、哈希等，并提供了丰富的操作命令，非常适合作为中间件使用 怎么使用？ 首先","title":"Redis八股"},{"content":"","permalink":"https://lidengxm.github.io/posts/tech/bbb/","summary":"","title":"Bbb"},{"content":"1 2 23 24 dsk jdsj\ndssssss\nds ds\nd d d s d\ns\nd\nd\nd dd\nd ds\n","permalink":"https://lidengxm.github.io/posts/tech/aaa/","summary":"1 2 23 24 dsk jdsj dssssss ds ds d d d s d s d d d dd d ds","title":"Aaa"},{"content":"Java 基础高频考点 抽象类和接口的区别？应用场景，怎么理解的 重载与重写的区别 HashMap 扩容原理、底层数据结构 OOP 是什么？ Java 语言基础 Java 语言的特点 ⾯向对象（封装，继承，多态） 平台⽆关性，平台⽆关性的具体表现在于，J a v a 是“⼀次编写，到处运⾏（Wr i t e O n c e，R u n a n y W h e r e）”的语⾔，因此采⽤ J a v a 语⾔编写的程序具有很好的可移植性，⽽保证这⼀点的正是 J a v a 的虚拟机机制。在引⼊虚拟机之后，J a v a 语⾔在不同的平台上运⾏不需要重新编译。 ⽀持多线程。C + + 语⾔没有内置的多线程机制，因此必须调⽤操作系统的多线程功能来进⾏多线程程序设计，⽽ J a v a 语⾔却提供了多线程⽀持； 编译与解释并存 JVM、JRE 和 JDK 有什么区别 J V M：Java 虚拟机，J a v a 程序运⾏在 J a v a 虚拟机上。针对不同系统的实现（Wi n d o w s，L i n u x，m a c O S）不同的 J V M，因此 J a v a 语⾔可以实现跨平台。\nJ R E： **J a v a 运⾏时环境。**它是运⾏已编译 J a v a 程序所需的所有内容的集合，包括 J a v a 虚拟机（J V M），J a v a 类库，J a v a 命令和其他的⼀些基础构件。但是，它不能⽤于创建新程序。\nJ D K : Java 开发工具包，它是功能⻬全的 J a v a S D K。它拥有 J R E 所拥有的⼀切，还有编译器（j a v a c）和⼯具（如 j a v a d o c 和 j d b）。它能够创建和编译程序。\n简单来说，J D K 包含 J R E，J R E 包含 J V M\n说说什么是跨平台性？原理是什么 所谓跨平台性，是指 J a v a 语⾔编写的程序，⼀次编译后，可以在多个系统平台上运⾏。\n实现原理：J a v a 程序是通过 J a v a 虚拟机在系统平台上运⾏的，只要该系统可以安装相应的 J a v a 虚拟机，该系统就可以运⾏ j a va 程序\n什么是字节码？采⽤字节码的好处是什么? 所谓的字节码，就是 J a v a 程序经过编译之后产⽣的. c l a s s ⽂件，字节码能够被虚拟机识别，从⽽实现 J a v a 程序的跨平台性。\nJ a v a 程序从源代码到运⾏主要有三步：\n编译：将我们的代码（. j a v a）编译成虚拟机可以识别理解的字节码( . c l a s s ) 解释：虚拟机执⾏ J a v a 字节码，将字节码翻译成机器能识别的机器码 执⾏：对应的机器执⾏⼆进制机器码 只需要把 J a v a 程序编译成 J a v a 虚拟机能识别的 J a v a 字节码，不同的平台安装对应的 J a v a 虚拟机，这样就可以可以实现 J a v a 语⾔的平台⽆关性。\nJava 基础语法 Java 数据类型有哪些 分为基本数据类型和引用数据类型\n基本数据类型在 JVM 的栈（Stack）中存储。栈是用于存储方法调用、局部变量等线程私有的数据的内存区域。基本数据类型的值直接存储在栈中，它们的生命周期是与方法的生命周期一致。\nString 对象的引用存储在栈中，而实际的字符串内容存储在堆（Heap）中。堆是用于存储动态分配的对象和数组的内存区域，由垃圾回收器自动管理。String 对象是不可变的，所以 JVM 会尝试重用字符串的内容，以减少内存使用，这就是字符串常量池（String Pool）的概念。\n基本数据类型：\n数值型 整型：byte、short、int、long 浮点型：float、double 字符型：char 布尔型：boolean 引用数据类型：\n类：class 接口：interface 数组：[] Java 基本数据类型范围和默认值：\n数值范围：\nbyte：-128-127 short：-2^15-2^15 int：-2^31-2^31 ⾃动类型转换、强制类型转换？ J a v a 所有的数值型变量可以相互转换，当把⼀个表数范围⼩的数值或变量直接赋给另⼀个表数范围⼤的变量时，可以进⾏⾃动类型转换；反之，需要强制转换。\nchar=\u0026gt;int=\u0026gt;long=\u0026gt;float=\u0026gt;double\nbyte=\u0026gt;short=\u0026gt;int=\u0026gt;long=\u0026gt;float=\u0026gt;double\n判断正误：\nfloat f = 3.4; 不对，3.4 是单精度数，向下转型，需要进行数据转换\n可以float f = 3.4f\nshort s1 = 1; s1 = s1 + 1;对吗 short s2 = 1; s2 += 1;对吗 第一题不对，第二题对\ns1 是 short 类型，1 是 int 类型，需要进行数据类型转换\ns2 是 short 类型，1 是 int 类型，但是 s2 += 1 就等价于s2 = short(s2 + 1)，也就将 int 类型转换了\nint 和 Integer 的区别 int 和 Integer 都是 Java 中表示整数的数据类型。它们之间的主要区别在于 int 是 Java 的一种基本类型，而 Integer 则是 Java 的一个类。具体区别如下：\n**int 是 Java 中的一种基本数据类型，它可以直接进行数值运算，**不需要进行自动拆装箱操作。 Integer 是 Java 中的一个类，是 int 的包装器类型，它将 int 值转换为对象，可以使用 Java 中的面向对象特性。 由于自动装箱操作的存在，我们通常使用 int 和 Integer 时很容易混淆。在使用上要注意：\nint 转换成 Integer 的操作叫做装箱，而 Integer 转换成 int 的操作叫做拆箱。 当使用 == 运算符比较两个 Integer 对象时，比较的是对象的引用地址而不是数值大小。应该使用 equals() 方法来比较两个 Integer 对象的值（Integer 重写了 equals） **int 可以通过 Integer.valueOf() 方法转换成 Integer 对象，也可以将 Integer 对象通过 intValue() 方法转换成基本数据类型。**但是，需要注意在使用时，考虑到可能出现 null 的情况。 int是 Java 的基本数据类型，是一种原始的整数类型。Integer是 Java 的包装类，它是对基本数据类型int的封装，提供了一些操作和方法。主要的区别有：\nint是基本数据类型，而Integer是引用数据类型。 int的默认值是 0，而Integer的默认值是null。 int在比较时使用==运算符，而Integer在比较时可以使用==，但更推荐使用equals()方法进行比较。 如果将Integer对象设置为null，然后尝试使用int进行比较，会引发NullPointerException（空指针异常）。这是因为int是基本数据类型，不能保存null值，所以在比较时无法进行有效的操作。\n什么是⾃动拆箱/封箱？ 装箱：将基本类型⽤它们对应的引⽤类型包装起来； 拆箱：将包装类型转换为基本数据类型； 自动拆箱（Unboxing）和自动封箱（Boxing）是 Java 中用来在基本类型（primitive type）和包装类型（wrapper class）之间进行转换的概念。\n**自动拆箱（Unboxing）是将包装类型对象转换为对应的基本类型的过程。**当我们需要使用基本类型的值而有一个包装类型的对象时，Java 会自动地将包装类型对象拆箱为基本类型。例如：\nInteger num = 10; // 自动封箱 int value = num; // 自动拆箱 **自动封箱（Boxing）则是将基本类型的值转换为对应的包装类型的对象。**当我们需要将基本类型的值存储在一个对象中时，Java 会自动地将基本类型封装为包装类型。例如：\nint value = 10; // 基本类型 Integer num = value; // 自动封箱 基本数据类型和包装类型如何比较值相等 对于基本类型之间的比较，可以直接使用\u0026quot;==\u0026ldquo;运算符。例如，int a = 10; int b = 10;，可以使用if (a == b)来判断 a 和 b 的值是否相等。 对于包装类型之间的比较，不能使用\u0026rdquo;==\u0026ldquo;运算符，因为**\u0026quot;==\u0026ldquo;运算符比较的是对象的引用是否相等，而不是值是否相等。**应该使用 equals()方法来比较包装类型的值是否相等。例如，Integer num1 = 10; Integer num2 = 10;，应该使用if (num1.equals(num2))来比较 num1 和 num2 的值是否相等。 需要注意的是，当进行包装类型的比较时，由于 Java 中对一些常用的包装类型的缓存机制，小的整数和 Boolean 值可能会被缓存，因此可以使用\u0026rdquo;==\u0026ldquo;运算符来比较它们的值是否相等。例如，Integer num1 = 10; Integer num2 = 10;，在这种情况下，可以使用if (num1 == num2)来判断 num1 和 num2 的值是否相等。\n总之，**基本类型之间可以使用\u0026rdquo;==\u0026ldquo;运算符比较值是否相等，而包装类型之间应使用 equals()方法进行值的比较。**但对于某些缓存的包装类型，也可以使用\u0026rdquo;==\u0026ldquo;运算符进行比较。\n\u0026amp;和\u0026amp;\u0026amp;有什么区别？ \u0026amp;运算符有两种⽤法： 短 路 与\u0026amp;\u0026amp; 、 逻 辑 与\u0026amp;\n\u0026amp;\u0026amp;运算符是短路与运算。逻辑与跟短路与的差别是⾮常巨⼤的，虽然⼆者都要求运算符左右两端的布尔值都是 t r u e 整个表达式的值才是 t r u e。\n\u0026amp;\u0026amp;之所以称为短路运算是因为，如果\u0026amp; \u0026amp;左边的表达式的值是 f a l s e，右边的表达式会被直接短路掉，不会进⾏运算。很多时候我们可能都需要⽤\u0026amp; \u0026amp;⽽不是\u0026amp;。\n例如在验证⽤户登录时判定⽤户名不是 n u l l ⽽且不是空字符串，应当写为 username != null \u0026amp;\u0026amp; username.equals(\u0026quot;\u0026quot;) ，⼆者的顺序不能交换，更不能⽤\u0026amp;运算符，因为第⼀个条件如果不成⽴，根本不能进⾏字符串的 e q u a l s ⽐较，否则会产生 NullPointerException 异常。\n具体分析：\n如果条件是username != null \u0026amp;\u0026amp; username.equals(\u0026quot;\u0026quot;)，当username为null时，程序会根据短路求值策略判断第一个条件username != null为假（false），并直接停止执行后续的条件判断，不会执行username.equals(\u0026quot;\u0026quot;)的比较操作，也就避免了产生NullPointerException异常的可能性。换句话说，当username为null时，第二个条件username.equals(\u0026quot;\u0026quot;)不会被执行，因此不会引发异常。\n如果我们将条件改为username.equals(\u0026quot;\u0026quot;) \u0026amp;\u0026amp; username != null，当username为null时，程序不会先判断username.equals(\u0026quot;\u0026quot;)是否为真，而是会直接执行username != null的判断。在这种情况下，username为null时，就会产生NullPointerException异常，因为在username为null的情况下调用了equals()方法。\n因此，为了避免NullPointerException异常，请确保将判断null的条件放在前面，以便使用短路求值策略来防止异常的发生\n⽤最有效率的⽅法计算 2 乘以 8？ 2 \u0026lt; \u0026lt; 3。位运算，数字的⼆进制位左移三位相当于乘以 2 的三次⽅。\n0000 0010 \u0026laquo; 3 =\u0026gt; 0001 0000\n构造⽅法、成员变量初始化以及静态成员变量三者的初始化顺序？ 先后顺序：静态成员变量、成员变量、构造⽅法。\n详细的先后顺序：⽗类静态变量、⽗类静态代码块、⼦类静态变量、⼦类静态代码块、⽗类⾮静态变量、⽗类⾮静态代码块、⽗类构造函数、⼦类⾮静态变量、⼦类⾮静态代码块、⼦类构造函数。\nJava 代码块执⾏顺序 ⽗类静态代码块（只执⾏⼀次） ⼦类静态代码块（只执⾏⼀次） ⽗类构造代码块 ⽗类构造函数 ⼦类构造代码块 ⼦类构造函数 普通代码块 值传递和引用传递 值传递（Pass by Value）和引用传递（Pass by Reference）是两种参数传递方式，可以用于描述在函数调用中参数的传递方式。\n值传递（Pass by Value）是指将实际参数的值复制一份，然后传递给函数，函数使用的是这个副本，对副本的修改不会影响到原始实际参数。在值传递中，函数对参数进行的任何修改都不会影响原始的实际参数。\n引用传递（Pass by Reference）是指将实际参数的引用（内存地址）传递给函数，函数中使用的是原始实际参数的引用，函数对引用的修改会影响到原始参数的值。在引用传递中，函数对参数进行的修改会直接反映到原始的实际参数。\n需要注意的是，在 Java 中，对于基本数据类型（如 int、float、boolean 等），是采用值传递的方式进行传递。而对于对象类型（如 String、List 等），虽然传递的是对象的引用值，但实际上也是采用值传递的方式进行传递。这是因为在 Java 中，传递对象类型的参数时，传递的实际上是对象的地址值的一个副本，函数内部使用的是这个副本，但仍然是通过引用来修改对象的状态。\n总结起来，简单来说：\n值传递：传递的是实际参数的副本，对副本的修改不会影响到原始实际参数。 引用传递：传递的是实际参数的引用，对引用的修改会影响到原始实际参数。 需要根据具体的编程语言和语义来理解传递方式的具体含义和行为。\n面向对象 面向过程与面向对象的区别 ⾯向过程 ：⾯向过程就是分析出解决问题所需要的步骤，然后⽤函数把这些步骤⼀步⼀步实现，使⽤的时候再⼀个⼀个的⼀次调⽤就可以。\n⾯向对象 ：⾯向对象，把构成问题的事务分解成各个对象，⽽建⽴对象的⽬的也不是为了完成⼀个个步骤，⽽是为了描述某个事件在解决整个问题的过程所发⽣的⾏为。 ⽬的是为了写出通⽤的代码，加强代码的重⽤，屏蔽差异性。\n⽤⼀个⽐喻：⾯向过程是编年体；⾯向对象是纪传体\n像面向过程就是把问题分解成一个一个函数，然后调用函数去解决问题。而面向对象就是把这个世界抽象成一个一个对象，然后赋予这些对象一个个属性，成员变量和方法，然后去调用对象的方法去解决问题，耦合性比较低。\n面向对象有哪些特性 面向对象编程（Object-Oriented Programming，OOP）是一种编程范式，它基于对象的概念，将数据和操作封装在对象中。面向对象编程强调以下几个特性：\n封装（Encapsulation）：封装是将数据和对数据的操作封装在一个对象中，对象对外部提供公共接口来访问和操作数据，同时隐藏内部的实现细节。通过封装，可以实现数据的安全性和模块化，减少了代码的耦合性。\n继承（Inheritance）：继承是一种机制，允许一个类（子类）继承另一个类（父类）的属性和方法。子类可以继承父类的特性，并且可以在此基础上添加新的特性或修改继承的特性。继承可以实现代码的重用和扩展。\n多态（Polymorphism）：**多态是指同一个操作或方法可以在不同的对象上产生不同的行为。**通过多态，可以实现基于对象的动态绑定，提高代码的灵活性和可扩展性。多态可以通过方法重写（覆盖）和方法重载来实现。\n实现多态：继承（多个⼦类对同⼀⽅法的重写）和接口（实现接口并覆盖接口中同⼀⽅法）\nJava 多态可以分为编译时多态和运⾏时多态。\n编译时多态主要指⽅法的重载，即通过参数列表的不同来区分不同的⽅法。 运⾏时多态主要指继承⽗类和实现接⼝时，可使⽤⽗类引⽤指向⼦类对象。 重载（overload）和重写（override）的区别？ ⽅法的重载和重写都是实现多态的⽅式，区别在于前者实现的是编译时的多态性，⽽后者实现的是运⾏时的多态性\n重载（Overload）和重写（Override）是面向对象编程中的两个重要概念，它们有以下区别：\n重载（Overload）：在同一个类中定义多个具有相同名称但参数列表不同的方法\n重载方法具有相同的方法名，但参数类型、参数个数或参数顺序不同。 重载方法根据调用时传入的参数类型或个数来确定具体调用哪个方法。 重载方法可以有不同的返回类型，但仅根据返回类型不能区分重载方法。 重写（Override）：在子类中重新定义父类中已有的方法\n重写方法具有相同的方法名、参数列表和返回类型。 重写方法在子类中提供了对父类方法的新实现。 重写方法必须具有相同或更低级的访问权限。（即父类方法 protected 修饰，子类方法只能是 protected 或者 public 或默认） 重写方法使用@Override注解来明确表示它是对父类方法的重写。 总结：\n重载是在同一个类中定义多个方法，方法名相同但参数不同，用于提供不同的方法重载。 重写是在子类中重新定义父类方法，方法名、参数和返回类型都相同，用于提供对父类方法的新实现。 重载是在编译时静态决定调用哪个方法，而重写是在运行时动态决定调用哪个方法，根据对象的实际类型来确定。 重载方法可以有不同的返回类型，但重写方法必须具有相同的返回类型。 重载和重写是面向对象编程中的两种不同的机制，它们都提供了灵活性和代码复用性，但用于不同的场景和目的。重载用于提供多个方法重载，以适应不同的参数情况；重写用于在子类中修改或扩展父类的方法实现。\n访问修饰符 public、private、protected、以及不写（默认）时的区别？ d e f a u l t (即默认，什么也不写）: 在同⼀包内的类可见，不使⽤任何修饰符。可以修饰在类、接口、变量、⽅法。 p r i v a t e : 在本类内可见。可以修饰变量、⽅法。注意：不能修饰类（外部类） p u b l i c : 对所有类可见。可以修饰类、接口、变量、⽅法 p r o t e c t e d : 对同⼀包内的类和所有⼦类可见。可以修饰变量、⽅法。注意：不能修饰类（外部类） 抽象类(abstract class)和接口(interface)有什么区别？ 抽象类（Abstract Class）和接口（Interface）是面向对象编程中的两个重要概念，它们有以下区别：\n定义： 抽象类是一个类，可以包含抽象方法和非抽象方法。抽象方法是没有实现的方法，需要在子类中进行实现。 接口是一个完全抽象的概念，它**只包含抽象方法和常量，**没有实现的方法。 继承： 一个类可以继承自另一个抽象类，通过使用extends关键字来实现继承。一个类只能继承一个抽象类。 一个类可以实现多个接口，通过使用implements关键字来实现接口。一个类可以实现多个接口。 实现： 抽象类可以包含成员变量、构造函数、非抽象方法以及抽象方法的定义。 接口只能包含常量和抽象方法的定义，不能包含成员变量和非抽象方法。 实例化： 抽象类不能被实例化，即不能直接创建抽象类的对象。需要通过创建其子类对象来实现具体的实例化。 接口也不能被实例化，即不能直接创建接口的对象。需要通过实现接口的类来创建对象。（项目中的 service 的实现层） 设计目的： 抽象类用于表示一种通用的概念，它提供了一些共同的属性和方法，而子类可以根据自身的特性进行扩展和实现。 接口用于定义一组行为规范，它描述了一个类应该具有的方法，通过实现接口，类可以保证遵循这些规范，并具备相应的行为能力。 总结接口 interface 与抽象类 abstract class 的区别：\n抽象类是一个类，可以包含抽象方法和非抽象方法，通过继承来实现代码的复用和扩展。 接口是一个完全抽象的概念，只包含抽象方法和常量，通过实现接口来实现代码的规范和多态性。 抽象类通过继承来实现，而接口通过实现来实现。 一个类可以继承一个抽象类，但可以实现多个接口。 从设计层⾯来说，抽象是对类的抽象，是⼀种模板设计，⽽接口是对⾏为的抽象，是⼀种⾏为的规范 Java8 中接口中可以有默认方法\n接口和抽象类的区别，应用场景，你怎么理解的?怎么使用的? 他们的区别：\n定义：接口是抽象概念，接口只包含抽象方法和常量，没有实例变量和具体方法的实现；抽象类还是类，可以包含抽象方法和具体方法的实现，也可以包含实例变量 实现：一个类可以实现多个接口，但只能继承一个类，接口实现使用implements关键字，而抽象类继承使用extends关键字。 构造函数：接口没有构造函数，抽象类可以有构造函数 访问修饰符：接口的方法默认为public，抽象类的方法可以有不同的访问修饰符 设计目的：接口用于定义类的行为规范，强调\u0026quot;是什么\u0026rdquo;，而抽象类用于代码复用，强调\u0026quot;是什么\u0026quot;和\u0026quot;怎么做\u0026rdquo; 应用场景：\n接口：适用于定义一组相关的方法，用于实现类的多继承，希望不同类能够具有一致的行为规范。比如，定义一个Comparable接口用于比较对象的大小，在不同类中实现该接口，从而可以对对象进行排序。 抽象类：适用于具有部分通用实现的类，希望在这个类中实现一些通用方法，但也希望子类能够继承这些方法，并有机会进行扩展。抽象类可以提供一些默认实现，子类可以选择性地覆盖这些方法。 在 Java 项目中，接口和抽象类都是非常有用的工具，它们帮助我们实现代码的复用、灵活性和可扩展性。以下是接口和抽象类在 Java 项目中的常见用法：\n接口（Interface）的使用场景：\n定义回调接口：接口可以用于定义回调接口，使得不同模块之间可以进行解耦。一个模块定义接口，其他模块实现该接口并传递给调用方，当某个事件发生时，调用方会回调实现了接口的方法。\n实现多继承：Java 中类只能单继承，但一个类可以实现多个接口。接口允许在一个类中实现多继承的特性，从而实现了一种多态的效果。\n定义常量：接口中可以定义常量，用于在多个类中共享某些常用的值。\n规范类的行为：接口用于定义类的行为规范，确保不同类拥有一致的行为。例如，Java 标准库中的Comparable接口，用于定义可以进行比较的类。\n抽象类（Abstract Class）的使用场景：\n提供通用实现：抽象类可以包含抽象方法和具体方法的实现，因此它可以提供一些通用的方法实现，供子类继承和重写。\n实现模板方法模式：抽象类常常用于实现模板方法模式，其中定义一个算法的框架，将一些步骤的实现延迟到子类中。子类可以通过实现抽象类的抽象方法来完成具体步骤。\n部分实现：当一个类具有部分通用实现，但仍然需要在子类中进行扩展和修改时，可以使用抽象类。\n约束继承关系：抽象类可以对子类施加约束，要求子类必须实现抽象类中的某些方法。\n综合使用场景：\n在实际项目中，通常会综合使用接口和抽象类来达到更好的代码设计和灵活性。例如，一个项目中可能会定义多个接口来表示不同的行为规范，而抽象类可以用于实现这些接口中的通用方法。同时，子类可以选择性地继承抽象类，并实现相应的接口，从而实现一组相关类的设计和扩展。\n总的来说，接口和抽象类在 Java 项目中都是非常重要的工具，通过合理使用它们，我们可以提高代码的复用性、可维护性和可扩展性。同时，它们也有助于在面向对象编程中实现良好的设计模式和架构。\n成员变量与局部变量的区别有哪些？ 成员变量（Member Variables）和局部变量（Local Variables）是在编程中常见的两种变量类型，它们有以下区别：\n定义位置： **成员变量定义在类的内部，但在任何方法、构造函数或块之外。**它们属于类的实例，可以被类的所有方法访问。 局部变量定义在方法、构造函数或块的内部，只在其所在的作用域内可见，超出作用域就无法访问。（局部） 作用域： **成员变量的作用域是整个类，可以在类的任何方法中使用。**它们的生命周期与类的实例相同。随类的创建而创建 **局部变量的作用域限定在定义它们的方法、构造函数或块中。**它们的生命周期在方法执行期间，当方法执行完毕时，局部变量就会被销毁。 默认值： 成员变量会被赋予默认值，例如数值类型的成员变量默认为 0，布尔类型默认为 false，引用类型默认为 null。 局部变量没有默认值，必须在使用之前进行显式初始化 访问修饰符： **成员变量可以使用访问修饰符（public、private、protected 等）**来控制其可见性和访问权限。 **局部变量没有访问修饰符，**它们的可见性仅限于定义它们的方法、构造函数或块。 生命周期： 成员变量的生命周期与类的实例相同，当类的实例被销毁时，成员变量也会被销毁。 局部变量的生命周期仅限于其所在的方法、构造函数或块的执行期间，当执行流程离开作用域时，局部变量就会被销毁。 内存占用： 成员变量存储在堆内存中，每个类的实例都会为成员变量分配内存。 局部变量存储在栈内存中，每个方法的执行都会为局部变量分配内存。 存储方式： 如果成员变量是使⽤ s t a t i c 修饰的，那么这个成员变量是属于类的，如果没有使⽤ s t a t i c 修饰，这个成员变量是属于实例的。 对象存于堆内存，如果局部变量类型为基本数据类型，那么存储在栈内存，如果为引⽤数据类型，那存放的是指向堆内存对象的引⽤或者是指向常量池中的地址 总结成员变量与局部变量：\n成员变量定义在类的内部，属于类的实例，可以在类的任何方法中使用。 局部变量定义在方法、构造函数或块的内部，只在其所在的作用域内可见。 成员变量的作用域是整个类，局部变量的作用域限定在定义它们的方法、构造函数或块中。 成员变量具有默认值，而局部变量没有默认值，必须显式初始化。 成员变量的生命周期与类的实例相同，局部变量的生命周期仅限于其所在的方法、构造函数或块的执行期间。 成员变量存储在堆内存中，局部变量存储在栈内存中。 正确理解和使用成员变量和局部变量对于编写正确的程序非常重要。成员变量用于存储对象的状态和属性，而局部变量用于存储临时数据和方法的中间结果。\nfinal 关键字有什么作⽤？ f i n a l 表⽰不可变的意思，可⽤于修饰类、属性和⽅法：\n被 f i n a l 修饰的类不可以被继承 被 f i n a l 修饰的⽅法不可以被重写 被 f i n a l 修饰的变量不可变，被 f i n a l 修饰的变量必须被显式第指定初始值，还得注意的是，这⾥的不可变指的是变量的引⽤不可变，不是引⽤指向的内容的不可变。 例如 ：\nfinal StringBuilder sb = new StringBuilder(\u0026#34;abc\u0026#34;); sb.append(\u0026#34;d\u0026#34;); System.out.println(\u0026#34;sb= \u0026#34; + sb);//abcd sb 是 final 修饰的变量引用，而引用对象的内容可变\n== 和 equals 的区别 ==是 Java 中的运算符，比较的是变量（栈）内存中存放的对象的（堆）内存地址，用来判断两个对象的地址是否相同\n比较基本数据类型如 int,long 时，比较的是两个对象的值是否相等 比较引用数据类型时，比较的是对象的引用是否指向同一个对象，即判断他们是不是一个内存地址 equals 是 JavaObject 类中的方法，用于比较两个对象的内容是否相同\nequals 一般用于比较两个对象的引用是否指向同一个对象，即内存地址是否相同 重写了 equals 方法的类，如 String 类，比较的就是两个对象的内容是否相等 为什么重写 equals 时必须重写 hashcode ⽅法？ 在 Java 中，equals 方法用于比较两个对象是否相等，而 hashCode 方法用于计算对象的哈希码。\n为什么需要重写 equals 方法：\n默认情况下，equals 方法比较的是对象的引用是否相等，即比较两个对象是否指向同一个内存地址。但是在实际应用中，我们通常希望比较对象的内容是否相等，即对象的状态是否相等。 因此，我们需要重写 equals 方法，根据对象的内容来判断它们是否相等。重写 equals 方法时，通常需要比较对象的字段值，而不仅仅是引用。 为什么需要重写 hashCode 方法：\nhashCode 方法用于计算对象的哈希码，它返回一个整数值，用于在哈希表等数据结构中快速定位对象。 在使用集合类时，例如 HashSet 或 HashMap，它们依赖于对象的哈希码来确定对象在集合中的存储位置。 如果两个对象相等（根据 equals 方法），那么它们的哈希码必须相等。否则，它们可能会被错误地存储在不同的位置，导致集合类无法正常工作。 因此，当我们重写 equals 方法时，通常也需要重写 hashCode 方法，以确保相等的对象具有相等的哈希码。 总结：\n重写 equals 方法是为了比较对象的内容是否相等。\n重写 hashCode 方法是为了确保相等的对象具有相等的哈希码，以便在集合类中正常工作。\n在重写 equals 方法时，通常也需要重写 hashCode 方法。\n如果两个对象相等，则hashCode⼀定也是相同的。两个对象相等，对两个对象分别调⽤equals⽅法都返回 t r u e。反之，两个对象有相同的hashCode值，它们也不⼀定是相等的 。\n深拷贝和浅拷贝有什么区别? 浅拷贝（Shallow Copy）\n浅拷贝只复制对象的成员变量的值，包括基本数据类型的值和引用数据类型的地址值。 对于引用类型的变量，浅拷贝只复制了地址值，而没有拷贝堆中的对象本身。 这意味着原对象和浅拷贝对象会共享同一个引用对象，对其中一个对象进行修改可能会影响到另一个对象。 深拷贝（Deep Copy）\n深拷贝是完全复制一个对象，包括复制对象的成员变量的值和堆中的对象本身。 深拷贝会创建一个新的对象，并将原对象的所有成员变量复制到新对象中，包括引用类型的变量所指向的对象。 原对象和深拷贝对象拥有各自独立的对象实例，修改其中一个对象不会影响到另一个对象。 例如现在有⼀个 o r d e r 对象，⾥⾯有⼀个 p r o d u c t s 列表，它的浅拷贝和深拷贝的⽰意图：\n浅拷贝只是拷贝了 order_no 对象的 products 变量的值和 products 列表的引用地址 深拷贝创建了一个新的对象，复制原对象的变量的值和堆中的对象本身 因此深拷贝是安全的，浅拷贝的话如果有引⽤类型，那么拷贝后对象，引⽤类型变量修改，会影响原对象\n实现浅拷贝和深拷贝的方法有多种\n浅拷贝的实现：\n使用对象的拷贝构造函数：如果对象的类提供了拷贝构造函数，可以使用该构造函数创建一个新的对象，将原对象的成员变量值复制到新对象中。 使用对象的克隆方法：如果对象实现了 Cloneable 接口，并重写了 clone 方法，可以使用该方法进行浅拷贝。clone 方法会创建一个新的对象，并将原对象的成员变量值复制到新对象中。 深拷贝的实现：\n递归复制对象及其引用类型成员变量：对于需要进行深拷贝的对象，可以递归地复制对象及其引用类型的成员变量。对于引用类型的成员变量，也需要进行深拷贝操作，以确保拷贝的完整性。 使用序列化和反序列化：通过将对象序列化为字节流，然后再将字节流反序列化为新的对象，可以实现深拷贝。这种方式需要对象及其引用类型成员变量都实现 Serializable 接口。 需要注意的是，不是所有的对象都可以直接进行拷贝。一些特殊情况下，可能需要考虑对象的特殊处理，例如单例模式、不可变对象等。\n下面是一个示例代码，演示了使用拷贝构造函数和递归复制实现浅拷贝和深拷贝的方式：\nclass MyClass { private int value; private MyObject myObject; // 拷贝构造函数实现浅拷贝 public MyClass(MyClass other) { this.value = other.value; this.myObject = other.myObject; } // 递归复制实现深拷贝 public MyClass deepCopy() { MyClass newObject = new MyClass(); newObject.value = this.value; newObject.myObject = new MyObject(this.myObject); // 使用 MyObject 的拷贝构造函数进行深拷贝 return newObject; } } class MyObject { private int data; // 拷贝构造函数实现浅拷贝 public MyObject(MyObject other) { this.data = other.data; } } Java 中创建对象的方式有哪几种 Java 中创建对象的方式有：\nnew 关键字创建新对象 通过反射机制，通过获取类的 class 对象可以使用 newInstance()方法创建对象 拷贝构造函数，拷贝构造函数创建新对象并赋值原对象的成员变量值 采⽤ clone 机制，clone 方法复制对象 工厂方法（Factory Method） 在 Java 中，创建对象的方式有以下几种：\n使用 new 关键字：使用 new 关键字可以创建一个对象，并调用其构造函数进行初始化。例如：\nMyClass obj = new MyClass(); 使用反射（Reflection）：Java 的反射机制可以在运行时动态地创建对象。通过获取类的 Class 对象，可以使用 newInstance() 方法创建对象。例如：\nClass\u0026lt;?\u0026gt; clazz = MyClass.class; MyClass obj = (MyClass) clazz.newInstance(); 使用对象的拷贝构造函数：如果对象的类提供了拷贝构造函数，可以使用该构造函数创建一个新的对象，将原对象的成员变量值复制到新对象中。例如：\nMyClass obj1 = new MyClass(); MyClass obj2 = new MyClass(obj1); // 使用拷贝构造函数创建新对象 使用对象的克隆方法：如果对象实现了 Cloneable 接口，并重写了 clone 方法，可以使用该方法创建对象的副本。例如：\nMyClass obj1 = new MyClass(); MyClass obj2 = (MyClass) obj1.clone(); // 使用克隆方法创建新对象 使用工厂方法（Factory Method）：工厂方法是一种创建对象的设计模式，通过定义一个工厂类和工厂方法来创建对象。工厂方法可以根据不同的条件返回不同的对象实例。例如：\ninterface MyFactory { MyClass createObject(); } class MyClassFactory implements MyFactory { public MyClass createObject() { return new MyClass(); } } MyFactory factory = new MyClassFactory(); MyClass obj = factory.createObject(); continue、break、return 的区别是什么 continue、break、return 都是循环和函数中的控制流程语句，虽然它们有相似的功能，但是用法和作用有很大的不同。\ncontinue，继续下一次循环。当执行到 continue 语句时，当前循环会跳过剩余的语句，直接进行下一次循环 break，退出循环。当执行到 break 时，当前循环会立刻停止，并退出循环体，执行循环之后的程序语句 return，结束函数，执行到 return 时，函数会立即结束并返回一个值（可选） public void test() { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); for(int i = 0; i \u0026lt; 9; i++) { if(...) { res.add(i); } continue;//1. break;//2. return;//3. i += 10; } res.remove(res.size() - 1); } 如上面的程序：\ncontinue，会直接结束本次循环，下面的语句不再执行，继续下一次 for 循环 break，会直接跳出 for 循环，循环结束，执行循环之后的语句 return，直接结束函数，不再执行任何语句 Java 的序列化和反序列化是什么？如何实现 序列化是将对象转换为字节流的过程，使得对象可以在网络上传输、持久化到磁盘或在进程间进行通信。当对象被序列化时，其状态和数据将被写入字节流中。\n**反序列化是将字节流恢复为对象的过程，****使得字节流中的数据可以转换为可操作的对象**。通过反序列化，我们可以重新创建和恢复原始的对象。\nJava 的序列化和反序列化是用于在对象和字节流之间进行转换的机制。\nserialiable 接口有什么用？\n这个接⼜只是⼀个标记，没有具体的作⽤，但是如果不实现这个接⼜，在有些序列化场景会报错，所以⼀般建议，创建的 JavaBean 类都实现 Serialable 接口\nserialVersionUID 又有什么⽤？\nserialVersionUID 序列化 版本号起验证作用，验证序列化和反序列化时的版本是否相同\nprivate final long serialVersionUID = 1L; 我们经常会看到这样的代码，这个 I D 其实就是⽤来验证序列化的对象和反序列化对应的对象 I D 是否⼀致。\n这个 I D 的数字其实不重要，⽆论是 1 L 还是 I D E ⾃动⽣成的，只要序列化时候对象的 serialVersionUID 和反序列化时候对象的 serialVersionUID ⼀致的话就⾏。\n如果没有显⽰指定serialVersionUID，则编译器会根据类的相关信息⾃动⽣成⼀个，可以认为是⼀个指纹。\n所以如果你没有定义⼀个serialVersionUID， 结果序列化⼀个对象之后，在反序列化之前把对象的类的结构改了，⽐如增加了⼀个成员变量，则此时的反序列化会失败。\n因为类的结构变了，所以serialVersionUID就不⼀致\nJ a v a 序列化不包含静态变量？\n序列化的时候是不包含静态变量的。\n如果有些变量不想序列化，怎么办？\n对于不想进⾏序列化的变量，使⽤ t r a n s i e n t 关键字修饰。\n**t r a n s i e n t 关键字的作⽤是：阻⽌实例中那些⽤此关键字修饰的的变量序列化；**当对象被反序列化时，被 t r a n s i e n t 修饰的变量值不会被持久化和恢复。 t r a n s i e n t 只能修饰变量，不能修饰类和⽅法。\n说说有⼏种序列化⽅式？ 常用的有两个：\n标准序列化，需要实现 Serialiable 接口 JSON 序列化，不需要实现 Serialiable 接口 如何实现标准的序列化与反序列化？\n通过实现 Serializable 接口来实现序列化，将对象写入字节流 通过调用 ObjectInputStream 的 readObject() 方法来实现反序列化，将字节流恢复成对象 要实现序列化和反序列化，需要满足以下条件：\n类必须实现 java.io.Serializable 接口。该接口是一个标记接口，没有方法定义，仅表示该类可以被序列化。\n序列化和反序列化操作通常使用 java.io.ObjectOutputStream 和 java.io.ObjectInputStream 类。这些类提供了序列化和反序列化对象的方法。\n实现序列化的步骤如下：\n在类声明中实现 java.io.Serializable 接口。 public class MyClass implements Serializable { // 类的声明和定义 // ... } 创建一个 ObjectOutputStream 将对象写入字节流。 MyClass obj = new MyClass(); FileOutputStream fileOut = new FileOutputStream(\u0026#34;data.ser\u0026#34;); ObjectOutputStream out = new ObjectOutputStream(fileOut); out.writeObject(obj); out.close(); fileOut.close(); 实现反序列化的步骤如下：\n创建一个 ObjectInputStream 从字节流中读取对象。 FileInputStream fileIn = new FileInputStream(\u0026#34;data.ser\u0026#34;); ObjectInputStream in = new ObjectInputStream(fileIn); MyClass obj = (MyClass) in.readObject(); in.close(); fileIn.close(); 需要注意的是，序列化和反序列化的对象的类必须具有相同的 serialVersionUID（序列化版本号）。否则，在反序列化过程中可能会出现 InvalidClassException 异常。\n此外，还可以通过实现 Externalizable 接口来自定义序列化和反序列化的行为，该接口提供了 writeExternal() 和 readExternal() 方法，允许对对象的序列化和反序列化过程进行更细粒度的控制。\n实现 JSON 序列化与反序列化\nJSON 序列化：\n使用第三方库（如 Jackson、Gson 等）：使用第三方库可以将对象转换为 JSON 字符串，并将 JSON 字符串转换回对象。 对象转 JSON 字符串：使用库提供的方法将对象转换为 JSON 字符串。 JSON 字符串转对象：使用库提供的方法将 JSON 字符串转换为对象。 库函数将 JSON 转为 Java 对象和转为 JSON 字符串（hutool 工具类）\nJSONUtil.toJsonStr()，将 Java 对象转为 JSON 字符串，用于以 String 类型的 key 写入缓存时 JSONUtil.toBean()，将 JSON 字符串转为 Java 对象，用于读取 JSON 字符串格式的缓存时 GSON 工具类库转 JSON 字符串与 Java 对象\nGson 是 Google 提供的一个用于在 Java 对象和 JSON 数据之间进行转换的开源库。它可以将 Java 对象转换成 JSON 字符串，也可以将 JSON 字符串转换成相应的 Java 对象。\n1.使用 Gson 开源库首先要引入 Gson 依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.code.gson\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;gson\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.8\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 2.Gson 将 JSON 字符串转成 Java 对象的一般做法\n// 创建 Gson 对象 Gson gson = new Gson(); // 将 JSON 字符串转换为指定类型的 Java 对象 T object = gson.fromJson(jsonString, T.class); 3.示例：项目中将 JSON 字符串形式的标签转为 Java 对象\n// 创建 Gson 对象 Gson gson = new Gson(); //将JSON类型字符串转换成Set\u0026lt;String\u0026gt;类型 //创建了一个匿名的 TypeToken 对象，通过 {}.getType() 方法来获取 Set\u0026lt;String\u0026gt; 类型的具体类型 Set\u0026lt;String\u0026gt; tempTagNameList = gson.fromJson(tagStr, new TypeToken\u0026lt;Set\u0026lt;String\u0026gt;\u0026gt;() {}.getType()); 堆，栈，常量池，方法区各自用来存储什么 在 Java 中，堆、栈、常量池和方法区是用来存储不同类型数据和对象的内存区域。\n栈、堆、静态域、常量池这样理解：\n栈(stack)：存放基本类型的变量数据和对象的引用，但对象本身不存放在栈中，而是存放在堆（new 出来的对象）或者常量池中（字符串常量对象存放在常量池中。 堆(heap)：存放所有 new 出来的对象。 常量池：存放字符串常量（String）和基本类型常量（public static final）。 方法区：用来存储已加载的类信息、静态变量、常量池、**即时编译器编译后的代码等内容。 栈存放基本数据类型变量数据和对象的 “引用”，对象本身不存在这个里面。\n常量池：\n像 int i=1 中的 1 就不放在常量池中，因为这个 1 不是常量属于变量。 像 public static final int i=1 中的 1 就放在这个常量池中。 https://blog.csdn.net/cxywangshun/article/details/122175050\n官方解释：\n堆（Heap）：**堆是用来存储对象实例的区域。**在堆中分配的内存由垃圾回收器进行管理。所有通过new关键字创建的对象，以及数组对象，都存储在堆中。堆是线程共享的，所有线程都可以访问堆中的对象。 栈（Stack）：**栈是用来存储方法调用和局部变量的区域。**每个线程在运行时都会创建一个栈，用于存储方法调用的上下文信息。每个方法在调用时会在栈上创建一个栈帧，栈帧包含方法的参数、局部变量和方法的返回地址等信息。当方法调用结束时，对应的栈帧被弹出栈。栈是线程私有的，每个线程有自己独立的栈空间。 常量池（Constant Pool）：**常量池是用来存储字符串常量、类和接口的常量、以及编译时期生成的其他字面量和符号引用的区域。**常量池是在编译阶段生成的，它位于方法区中的一部分。在运行时，类加载器会将常量池中的常量加载到内存中，并提供给运行时使用。 方法区（Method Area）：**方法区是用来存储已加载的类信息、静态变量、常量池、**即时编译器编译后的代码等内容。方法区也是线程共享的，存储的内容在整个应用程序的生命周期内保持不变。方法区是 Java 虚拟机规范中的一个概念，并不一定对应具体的物理区域。 需要注意的是，Java 虚拟机的内存布局在不同的实现中可能会有所不同，以上是一种常见的内存分配方式。此外，从 Java 8 开始，永久代（PermGen）被移除，方法区被替换为元数据区（Metaspace），用于存储类的元数据信息。\nJava 语⾔中关键字 static 的作⽤是什么？ static 的主要作⽤有两个：\n为某种特定数据类型或对象分配与创建对象个数⽆关的单⼀的存储空间。 使得某个⽅法或属性与类⽽不是对象关联在⼀起，即在不创建对象的情况下可通过类直接调⽤⽅法或使⽤类的属性。 具体⽽⾔ static ⼜可分为 4 种使⽤⽅式：\n**修饰成员变量。**⽤ static 关键字修饰的静态变量在内存中只有⼀个副本。只要静态变量所在的类被加载，这个静态变量就会被分配空间，可以使⽤“类.静态变量”和“对象.静态变量”的⽅法使⽤。 修饰成员⽅法。 static 修饰的⽅法⽆需创建对象就可以被调⽤。 static ⽅法中不能使⽤ this 和 super 关键字，不能调⽤⾮ static ⽅法，只能访问所属类的静态成员变量和静态成员⽅法。 修饰代码块。 JVM 在加载类的时候会执⾏ static 代码块。 static 代码块常⽤于初始化静态变量。 static 代码块只会被执⾏⼀次。 修饰内部类。 static 内部类可以不依赖外部类实例对象⽽被实例化。静态内部类不能与外部类有相同的名字，不能访问普通成员变量，只能访问外部类中的静态成员和静态成员⽅法。 常用类 String 类的常用方法 列举一下：\nhascode 方法，返回字符串对象的哈希值\nequals 方法，重写的 Object 类的方法，比较两个对象的值是否相等\nsubstring 方法，灵活的切割字符串\nindexOf 方法，返回指定字符串在原字符串中第一次出现的索引位置，可以指定搜索的起始位置。\nlength()方法，返回字符串长度\nisEmpty()，判断字符串是否为空\ncharAt()，返回指定位置的字符\ngetBytes()，用于返回字符串的字节数组，可以指定编码方式\ntrim()方法，去除字符串两侧的空白字符\nequals方法\n看一段代码示例：\nString alita = new String(\u0026#34;小萝莉\u0026#34;); String luolita = new String(\u0026#34;小萝莉\u0026#34;); System.out.println(alita.equals(luolita)); // true System.out.println(alita == luolita); // false ==比较的是引用对象的地址是否相同，alita 和 luolita 是两个 String 对象，对象地址也不同\n而 equals 方法在这里比较的是 alita 和 luolita 两个对象的值是否相等\nString 类的 equals 方法是继承了 Object 类的\nObject 类的 equals 方法：\npublic boolean equals(Object obj) { return (this == obj); } Object 类的 .equals() 方法默认采用的是“==”操作符进行比较。假如子类没有重写该方法的话，那么“==”操作符和 .equals() 方法的功效就完全一样——比较两个对象的内存地址是否相等。\nJava8 中 String 类的 equals 方法如下：\npublic boolean equals(Object anObject) { // 判断是否为同一对象 if (this == anObject) { return true; } // 判断对象是否为 String 类型 if (anObject instanceof String) { String anotherString = (String)anObject; int n = value.length; // 判断字符串长度是否相等 if (n == anotherString.value.length) { char v1[] = value; char v2[] = anotherString.value; int i = 0; // 判断每个字符是否相等 while (n-- != 0) { if (v1[i] != v2[i]) return false; i++; } return true; } } return false; } JDK 8 比 JDK 17 更容易懂一些：\n首先判断两个对象是否为同一个对象，如果是，则返回 true。\n接着，判断对象是否为 String 类型，如果不是，则返回 false。\n如果对象为 String 类型，则比较两个字符串的长度是否相等，如果长度不相等，则返回 false。\n如果长度相等，则逐个比较每个字符是否相等，如果都相等，则返回 true，否则返回 false。\nString、StringBuffer、StringBuilder 区别 String 是不可变类，一旦创建了 String 对象就不能修改它的值。String 是线程安全的。每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。\nStringBuffer 和 StringBuilder 都是可变的类，它们可以对字符串进行增删改操作，而不需要创建新的对象。\n区别在于：**StringBuffer 是线程安全的，它的每个方法都使用了 synchronized 关键字来保证线程安全性。**但 StringBuilder 不是线程安全的。\nString 适用于操作少量的数据 StringBuffer 适用于需要频繁进行字符串操作且需要线程安全的场景 StringBuilder 适用于单线程环境下需要频繁进行字符串操作的场景。 使用场景：\n如果字符串操作较少且线程安全性要求较高，可以使用 String。\n如果需要频繁进行字符串操作且线程安全性要求较高，可以使用 StringBuffer。如果在单线程环境下需要频繁进行字符串操作，可以使用 StringBuilder 来获得更好的性能。\nString s1 = new String(\u0026ldquo;123\u0026rdquo;)和\u0026quot;123\u0026quot;的区别 原题如下：\nString s1 = new String(\u0026#34;123\u0026#34;); String s2 = \u0026#34;123\u0026#34;; 问：这两个有什么区别\n结合图来看一下，String 类型的引用 s1 和 s2 存放在栈中，两个引用最后都会指向常量池中的\u0026quot;123\u0026quot;，但 s1 会在堆中再创建一个 String 类型对象的实例，并指向常量池中的\u0026quot;123\u0026quot;，而 s2 是直接指向常量池中的\u0026quot;123\u0026quot;\nString s = new String(\u0026#34;123\u0026#34;)中创建了几个对象 如果常量池中有\u0026quot;123\u0026quot;，则只创建了一个对象，一个堆中的对象实例\n如果常量池中没有\u0026quot;123\u0026quot;，则创建两个对象，一个是常量池中的\u0026quot;123\u0026quot;，一个是堆中 new String 创建的，跟\u0026quot;123\u0026quot;内容相同的对象实例\nString a = \u0026ldquo;str_a\u0026rdquo;, String b = new String(a + \u0026ldquo;str_b\u0026rdquo;); 创建了几个对象\n在这段代码中，创建了三个对象。\n\u0026quot;str_a\u0026quot;：这是字符串字面量，创建了一个字符串对象。它是编译时常量，存储在常量池中。\na：这是一个引用变量，指向上面创建的字符串对象 \u0026quot;str_a\u0026quot;。\nnew String(a + \u0026quot;str_b\u0026quot;)：这是通过将字符串 \u0026quot;str_a\u0026quot; 和字符串字面量 \u0026quot;str_b\u0026quot; 进行拼接得到的新字符串，然后通过 new String() 构造函数创建了一个新的字符串对象。\n首先，a + \u0026quot;str_b\u0026quot; 这个表达式会创建一个新的字符串对象，其中的字符是 \u0026quot;str_astr_b\u0026quot;。 接着，使用 new String() 构造函数创建了一个新的字符串对象，它是对前面拼接出的字符串的一个拷贝。由于使用了 new String() 构造函数，会在堆内存中创建一个新的字符串对象。 所以总共创建了三个对象：一个字符串字面量对象 \u0026quot;str_a\u0026quot;，一个引用变量 a，一个通过拼接和 new 关键字创建的字符串对象 b。\nString 怎么转成 Integer 的？原理？ 将 String 转换为 Integer 可以使用以下方法：\n使用 Integer.parseInt() 方法：可以将 String 对象转换为对应的 int 值。 String str = \u0026#34;123\u0026#34;; int num = Integer.parseInt(str); 使用 Integer.valueOf() 方法：可以将 String 对象转换为对应的 Integer 对象。 String str = \u0026#34;123\u0026#34;; Integer num = Integer.valueOf(str); 这两种方法都可以将字符串表示的整数转换为 Integer 类型的对象。它们的内部机制如下：\nInteger.parseInt() 方法解析整数，并返回对应的 int 类型的值。它首先检查字符串的格式是否合法，如果不是一个有效的整数格式，则抛出 NumberFormatException 异常。如果格式正确，它会将字符串中的数字序列解析为一个整数。该方法仅适用于将字符串转换为 int 类型，不能转换其他类型。\nInteger.valueOf() 方法将字符串转换为一个 Integer 对象。它首先调用 Integer.parseInt() 方法解析整数。然后，它使用解析得到的 int 值创建一个新的 Integer 对象，并将其返回。因此，它既可以用于将字符串转换为 int，也可以用于将字符串转换为 Integer 对象。\n总结起来，**将 String 转换为 Integer 本质上是将字符串中表示的数字解析为整数类型的值或对象。**可以使用 Integer.parseInt() 方法将字符串转换为 int 值，或使用 Integer.valueOf() 方法将字符串转换为 Integer 对象。在转换过程中，Java 会先检查字符串的格式是否合法，然后将有效的数字序列解析为整数。\nString 和 List 的转换 将 String 转换为 List，可以使用 String 类的 split() 方法，该方法可以将字符串按照指定的分隔符拆分成字符串数组，然后再将数组转换为 List。示例如下：\nString str = \u0026#34;apple,banana,orange\u0026#34;; List\u0026lt;String\u0026gt; list = Arrays.asList(str.split(\u0026#34;,\u0026#34;)); 上述代码中，使用逗号作为分隔符将字符串拆分成数组，然后使用 Arrays.asList() 方法将数组转换为 List。\n将 List 转换为 String，可以使用 Java 8 引入的 String 类的 join() 方法，该方法接收一个 CharSequence 的集合，使用指定的分隔符将集合中的元素连接成一个字符串。示例如下：\nList\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;orange\u0026#34;); String str = String.join(\u0026#34;,\u0026#34;, list); 上述代码中，使用逗号作为分隔符，将 List 中的元素连接成一个字符串。\n需要注意的是，使用 Arrays.asList() 方法将数组转换为 List 时，得到的 List 是一个固定大小的列表，不能执行添加或删除元素的操作。如果需要对 List 进行增删操作，可以创建一个 ArrayList，并将数组或其他集合传递给 ArrayList 的构造函数来创建可修改的 List。\n为什么把 string 类设计成不可变类？ 将 String 设计为不可变类有以下几个原因：\n性能优化：字符串在 Java 中被广泛使用，因此性能是一个重要考量因素。**由于 String 是不可变的，可以在字符串常量池中缓存字符串字面值，并且多个变量可以共享同一个字符串对象。**这样可以节省内存空间，并提高字符串比较的效率。\n安全性：**字符串作为参数在很多地方使用，**如果可变的话，那么一旦字符串被修改，不可预知的后果可能会导致系统功能出现问题。**通过将 String 设计为不可变类，****可以确保字符串不会被意外修改，提高系统的安全性和稳定性。**\n线程安全：由于 String 是不可变类，可以保证多个线程同时访问字符串对象而不会引发竞争条件或数据一致性问题。在涉及多线程操作中不需要同步操作。\n缓存哈希码：String 类在创建对象时会计算并缓存字符串的哈希码，因为哈希码被广泛用于数据结构中的查找，如 HashMap、HashSet 等。如果字符串是可变的，那么它们的哈希码也会随之变化，导致这些数据结构无法正常工作。\nString 为什么不是基本数据类型 在 Java 中，String不是基本数据类型，而是引用数据类型。这意味着String对象是通过引用来访问和操作的，而不是直接存储在内存的栈空间中。\n下面是一些原因说明为什么String不是基本数据类型：\n不可变性： String对象的值在创建后是不可更改的。当我们对一个String对象执行拼接、删除、替换等操作时，实际上是创建了一个新的String对象。这种不可变性使得String对象在多线程环境下更安全，也提供了字符串池的机制，以减少内存消耗。\n继承自Object类：String类是 Java 中java.lang.Object类的一个子类，而基本数据类型是没有继承关系的。\n存储方式：基本数据类型的值是直接存储在栈空间中，而String对象的引用和实际字符串内容是分别存储在堆和栈空间中的。\nString对象本身是一个引用，在栈空间中存储。这个引用指向的是堆中实际存储字符串内容的对象。 实际的字符串内容是存储在堆空间中的，由 Java 虚拟机进行管理。 由于上述原因，String被归类为引用数据类型而非基本数据类型。\nString 类的 intern 方法怎么理解？ 在 Java 中，String 类的 intern() 方法是一个用于字符串常量池（String Pool）的方法。它的作用是将字符串添加到字符串池中，并返回池中对应的字符串引用。如果字符串池中已经包含了该字符串，则直接返回池中的引用，否则将该字符串添加到池中后再返回引用。\n具体来说，当你调用 intern() 方法时，Java 会首先检查字符串池中是否已经存在该内容的字符串，如果存在，则返回池中的引用。如果池中没有该字符串，则将其添加到池中后返回引用。\n这个方法的主要目的是为了节省内存和加快字符串比较的速度。因为字符串池中存储的是唯一的字符串，所以在进行字符串比较时，只需比较引用地址，而不需要逐个字符比较，从而提高了比较的效率。\npublic class StringInternExample { public static void main(String[] args) { String str1 = \u0026#34;hello\u0026#34;; String str2 = new String(\u0026#34;hello\u0026#34;).intern(); // true，因为它们引用的是字符串池中同一个对象 System.out.println(\u0026#34;str1 == str2: \u0026#34; + (str1 == str2)); String str3 = new String(\u0026#34;world\u0026#34;); String str4 = str3.intern(); // false，因为在执行 intern() 方法时，字符串池中已经有了 \u0026#34;world\u0026#34; 这个字符串的引用，堆中对象的引用 System.out.println(\u0026#34;str3 == str4: \u0026#34; + (str3 == str4)); } } String s1 = new String(\u0026#34;二哥\u0026#34;) + new String(\u0026#34;三妹\u0026#34;); String s2 = s1.intern(); System.out.println(s1 == s2); 第一行代码，会在字符串常量池中创建两个对象，一个是“二哥”，一个是“三妹”，然后在堆中会创建两个匿名对象“二哥”和“三妹”，最后还有一个“二哥三妹”的对象（稍后会解释）， s1 引用的是堆中“二哥三妹”这个对象。\n第二行代码，对 s1 执行 intern() 方法，该方法会从字符串常量池中查找“二哥三妹”这个对象是否存在，此时不存在的，但堆中已经存在了，所以字符串常量池中保存的是堆中这个“二哥三妹”对象的引用，也就是说， s2 和 s1 的引用地址是相同的，所以输出的结果为 true。\n实际执行过程如下：\n创建一个 StringBuilder 对象。\n在 StringBuilder 对象上调用 append(\u0026ldquo;二哥\u0026rdquo;)，将 \u0026ldquo;二哥\u0026rdquo; 追加到 StringBuilder 中。 在 StringBuilder 对象上调用 append(\u0026ldquo;三妹\u0026rdquo;)，将 \u0026ldquo;三妹\u0026rdquo; 追加到 StringBuilder 中。 在 StringBuilder 对象上调用 toString() 方法，将 StringBuilder 转换为一个新的字符串对象，内容为\u0026quot;二哥三妹\u0026quot; s1 和 s2 引用地址都相同，都指向常量池中的字符串对象\u0026quot;二哥三妹\u0026quot;\nObject 类的常用方法 Object 类是 Java 中的根类，所有类都直接或间接地继承自 Object 类。因此，Object 类的常见方法可以在所有的 Java 类中使用。\n下面是一些 Object 类中常见的方法：\n**equals(Object obj)：用于比较两个对象是否相等。****默认情况下，它比较的是两个对象的引用是否相同，即对象是否指向同一个内存地址**。可以根据需要在自定义类中重写该方法，以实现对象之间按特定标准的比较。\nhashCode()：返回对象的哈希码值。默认情况下，hashCode() 方法返回对象的内部地址，但可以在自定义类中重写该方法，根据对象的内容计算哈希码。\ntoString()：返回对象的字符串表示。默认情况下，返回类的全限定名和对象的内部地址。可以在自定义类中重写该方法，以返回更有意义的字符串表示。\ngetClass()：**返回对象的运行时类（Runtime Class）对象。**通过该方法可以获取对象所属的类的相关信息，如类的名称、修饰符、父类、接口等。\nclone()：创建并返回对象的一个副本。需要注意的是，clone() 方法是浅拷贝，只复制对象的引用，而不复制对象所引用的内容。如果需要进行深拷贝，可以通过实现 Cloneable 接口，并重写 clone() 方法来实现。\nfinalize()：在对象即将被垃圾回收器回收时被调用的方法。可以在自定义类中重写该方法，进行对象的资源释放和清理操作。\nnotify()、notifyAll()、wait()：用于线程间的通信和同步操作。这些方法是与多线程相关的，可以在实现多线程的类中使用。\n简述内部类及其作用 成员内部类：作为成员对象的内部类。可以访问 private 及以上外部类的属性和⽅法。**外部类想要访问内部类属性或⽅法时，必须要创建⼀个内部类对象，然后通过该对象访问内部类的属性或⽅法。**外部类也可访问 private 修饰的内部类属性。 局部内部类：**存在于⽅法中的内部类。访问权限类似局部变量，只能访问外部类的 final 变量。**局部内部类的生命周期仅限于作用域内 匿名内部类：只能使⽤⼀次，没有类名，只能访问外部类的 final 变量。 静态内部类：类似类的静态成员变量。 以下是各种内部类的代码示例：\n成员内部类（Member Inner Class）： public class OuterClass { private int outerVar; public class InnerClass { public void display() { System.out.println(\u0026#34;Inner class: \u0026#34; + outerVar); } } public void createInnerClass() { InnerClass inner = new InnerClass(); inner.display(); } } 局部内部类（Local Inner Class）： public class OuterClass { public void outerMethod() { final int localVar = 10; class LocalInnerClass { public void display() { System.out.println(\u0026#34;Local inner class: \u0026#34; + localVar); } } LocalInnerClass localInner = new LocalInnerClass(); localInner.display(); } } 匿名内部类（Anonymous Inner Class）： public class ThreadDemo { public static void main(String[] args) { Thread t = new Thread(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName()); } }); t.start(); } } 静态内部类 **静态内部类和成员内部类类似，只是多了一个 static **\npublic class Wangsi { static int age; double money; static class Wangxxiaosi { public Wangxxiaosi (){ System.out.println(age); } } } 由于 static 关键字的存在，静态内部类是不允许访问外部类中非 static 的变量和方法的\nIO 流 大纲\nJava 中的 IO 有哪些 IO 流，即输入输出流，是计算机程序用于与外部环境（如文件、网络等）进行数据交换的方式。在 Java 中，**IO 流主要用于处理文件和网络通信。**IO 流按照传输方式可分为字节流和字符流两种类型，用于处理不同类型的数据。\n下面是一些常见的 IO 流以及它们的用途：\n字节流（Byte Streams）：\nInputStream 和 OutputStream：用于处理字节数据的输入和输出流，是所有字节流类的基类。常用于读写二进制文件。 FileInputStream 和 FileOutputStream：用于读写文件中的字节数据。 BufferedInputStream 和 BufferedOutputStream：对字节流进行缓冲，提高读写效率。 DataInputStream 和 DataOutputStream：用于以基本数据类型进行读写操作。 ObjectInputStream 和 ObjectOutputStream：用于读写对象的二进制表示，实现对象的序列化和反序列化。 PrintStream 字节打印流，PrintWriter 字符打印流 字符流（Character Streams）：\nReader 和 Writer：处理字符数据的输入和输出流的基类。 FileReader 和 FileWriter：用于读写字符文件。 BufferedReader 和 BufferedWriter：对字符流进行缓冲，提高读写效率。 InputStreamReader 和 OutputStreamWriter：将字节流转换为字符流，用于处理字符编码。 在处理 IO 流时，需要注意以下几点：\n使用 try-with-resources 语句块确保资源的正确关闭，避免资源泄漏。 在处理文本文件时，要注意字符编码，以免出现乱码问题。 在处理大文件时，使用缓冲流能够提高读写效率。 对于频繁的 IO 操作，考虑使用 NIO（New I/O）来提高性能。 字节流和字符流有什么区别 字节流和字符流的区别：\n**字节流一般用来处理图像、视频、音频、PPT、Word 等类型的文件。字符流一般用于处理纯文本类型的文件，如 TXT 文件等，但不能处理图像视频等非文本文件。**用一句话说就是：字节流可以处理一切文件，而字符流只能处理纯文本文件。 字节流本身没有缓冲区，缓冲字节流相对于字节流，效率提升非常高。而字符流本身就带有缓冲区，缓冲字符流相对于字符流效率提升就不是那么大了。 transient 关键字作用 1 一旦字段被 transient 修饰，成员变量将不再是对象持久化的一部分，该变量的值在序列化后无法访问。\n2）transient 关键字只能修饰字段，而不能修饰方法和类。\n3）被 transient 关键字修饰的字段不能被序列化，一个静态变量（static 关键字修饰）不管是否被 transient 修饰，均不能被序列化。\n什么是 BIO、NIO、AIO？ BIO、NIO、AIO 都是 Java 中网络编程的 I/O 模型。\nBIO（Blocking IO ）是 JDK1.4 之前的传统 IO 模型\n特点就是同步阻塞等待数据，直到数据读取完毕才会返回结果，线程会一直阻塞在read/write 方法上，不能处理其他的 IO 请求，它的并发性能比较差。 NIO（Non-Blocking IO）是 Java 1.4 之后新增的 IO 模型，它支持同步非阻塞式的 IO 操作。\n**NIO 采用了多路复用器来处理 IO 请求，通过一个线程处理多个 IO 请求，实现了高并发处理。**NIO 主要有三个核心概念：Selector、Channel、Buffer。Selector 负责监听多个 Channel 上的事件，Channel 可以理解为对原始 IO 的封装，Buffer 则是对数据的封装。 应用程序不断进行 I/O 系统调用轮询数据是否已经准备好的过程是十分消耗 CPU 资源的。 AIO（Asynchronous IO）是 Java 1.7 之后新增的 IO 模型，它支持异步非阻塞 IO 操作。\n与 NIO 不同的是，**AIO 在进行读写操作时不需要像 NIO 一样一直轮询，而是通过回调函数的方式在数据准备好后通知应用程序进行数据的读取，这样可以更加高效地利用系统资源，提高吞吐量。**但是 AIO 在处理小文件和小数据量时的性能并不如 NIO。 BIO 同步阻塞 IO，即打算约女神，给女神发短信后，没见到女神就一直等在宿舍楼下。\nNIO 同步非阻塞 IO，即打算约女神，给女神发短信后，没见到女神就一直发短信。\nNIO java 中的 NIO，就是打算约女神，你让宿管大妈去挨个看每一个下楼的妹子，女神下楼了大妈就通知你。\nAIO 就是打算约女神，你发完短信，你就去玩游戏了，女神下楼了，发短信给你，你才出现。\n异常 Java 异常的分类 Java 异常分为 Error（程序⽆法处理的错误），和 Exception（程序本身可以处理的异常）。这两个类均继承 Throwable。\nError 常⻅的有 StackOverFlowError、 OutOfMemoryError 等等。\nException 可分为运⾏时异常和⾮运⾏时异常。\n对于运⾏时异常，可以利⽤ try catch 的⽅式进⾏处理，也可以不处理。 对于⾮运⾏时异常，必须处理，不处理的话程序⽆法通过编译。 throw 和 throws 的区别\nthrow ⼀般是⽤在⽅法体的内部，由开发者定义当程序语句出现问题后主动抛出⼀个异常。 throws ⼀般⽤于⽅法声明上，代表该⽅法可能会抛出的异常列表。 常见的异常有：\nNullPointerException（空指针异常）：当尝试在一个为null的引用上执行方法调用或访问成员时抛出。 IndexOutOfBoundsException（下标越界异常）：当尝试访问数组或集合中不存在索引的元素时抛出。 IllegalArgumentException（非法参数异常）：当传递给方法的参数不合法或不正确时抛出，如传递了一个负数给一个不能接受负数的方法。 ArithmeticException（算术异常）：当执行算术运算时发生错误，比如除以零。 ClassCastException（类型转换异常）：当尝试将一个对象强制转换为不兼容的类型时抛出。 IOException（输入/输出异常）：当处理输入/输出操作时发生错误时抛出。 ClassNotFoundException（类未找到异常）：当尝试加载不存在的类时抛出。 出现在 Java 程序中的 finally 代码块是否⼀定会执⾏？ 当遇到下⾯情况不会执⾏：\n当程序在进⼊ try 语句块之前就出现异常时会直接结束。 当程序在 try 块中强制退出时，如使⽤ System.exit(0)，也不会执⾏ finally 块中的代码。 其它情况下，在 try/catch/finally 语句执⾏的时候， try 块先执⾏，当有异常发⽣， catch 和 finally 进⾏处理后程序就结束了，当没有异常发⽣，在执⾏完 finally 中的代码后，后⾯代码会继续执⾏。\n值得注意的是，当 try/catch 语句块中有 return 时， finally 语句块中的代码会在 return 之前执⾏。如果 try/catch/finally 块中都有 return 语句， finally 块中的 return 语句会覆盖 try/catch 模块中的 return 语句\n看下面一段代码输出什么？\npublic class TryDemo { public sattic void main(String args[]) { System.out.println(test1); } public static int test1() { int i = 0; try { i = 2; return i; } finally { i = 3; } } } 结果是 2\n虽然 finally 是在 return 之前执行，但其实，在执⾏ f i n a l l y 之前，JVM 会先将 i 的结果暂存起来，然后 f i n a l l y 执⾏完毕后，会返回之前暂存的结果，⽽不是返回 i，所以即使 i 已经被修改为 3，最终返回的还是之前暂存起来的结果 2。\ntry，catch 如果抛出异常被 catch 抓到了 那 finally 还能执行么 在 Java 中，如果异常被 try-catch 块捕获并处理，那么无论异常是否发生，finally块中的代码都会执行。\n如果异常被捕获并且在 catch 块中进行了处理，则执行完 catch 块中的代码后，会继续执行finally块中的代码。 如果异常没有被捕获，则不会执行catch块中的代码，直接跳转到finally块并执行其中的代码。 在finally块中的代码最终会被执行，不论是否有异常被捕获。\n如果在 finally 里面 return 了，那 try，catch 还能 return 么\n关于第二个问题，如果在finally块中使用return语句，那么最终的返回值会以finally中的返回值为准。即使在try或catch块中已经有了返回值，但是如果在finally中使用return语句，最终的返回结果将会是finally中的返回值。finally块中的return会覆盖try或catch中的return语句。\n例如：\npublic static int test() { try { return 1; } finally { return 2; } } 在以上代码中，无论try块中的返回值是什么，最终的返回结果都会是 2，因为finally中的return语句会覆盖try中的return语句。\nfinal、 finally 和 finalize 的区别是什么？ final ⽤于声明属性、⽅法和类，分别表示属性不可变、⽅法不可重写、类不可继承。 finally 作为异常处理的⼀部分，只能在 try/catch 语句中使⽤， finally 附带⼀个语句块⽤来表示这个语句最终⼀定被执⾏，经常被⽤在需要释放资源的情况下。 finalize 是 Object 类的⼀个⽅法，在垃圾收集器执⾏的时候会调⽤被回收对象的 finalize()⽅法。当垃圾回收器准备好释放对象占⽤空间时，⾸先会调⽤ finalize()⽅法，并在下⼀次垃圾回收动作发⽣时真正回收对象占⽤的内存 泛型与注解，反射 Java 泛型了解么？什么是类型擦除？介绍⼀下常⽤的通配符 ？ 泛型，即“参数化类型”，解决不确定对象具体类型的问题。在编译阶段有效。\nJava 泛型是 Java 5 引入的一种类型参数化机制，它允许在类、接口和方法中使用参数化类型，以达到在编译时期提供更强的类型检查和更高的代码复用性。本质是参数化类型\n类型擦除\n类型擦除是 Java 泛型实现的方式之一。在 Java 编译器编译泛型代码时，会把泛型类型参数擦除为它的上界（如果没有指定上界则擦除为 Object）。这样做的目的是为了兼容之前的 Java 代码，可以将泛型代码与之前的非泛型代码进行互操作。\n常用的通配符包括：\n?：表示未知类型，相当于? extends Object。 ? extends 类型：表示任何继承自类型的子类型，表示上界。 ? super 类型：表示任何类型，该类型是类型的父类型包括本身，表示下界。 通配符可用于方法参数、返回值、泛型类和泛型接口的定义。它可以增加代码的灵活性，使得可以接收不同类型的参数，并且可以与泛型类或接口一起使用，实现更加通用和可重用的代码。\n泛型的使用方式有以下几种：\n泛型类 泛型接口 泛型方法 类型参数化类：在类的定义中，使用尖括号\u0026lt;T\u0026gt;表示类型参数，可以在类中使用该类型参数。 public class MyClass\u0026lt;T\u0026gt; { private T value; public T getValue() { return value; } public void setValue(T value) { this.value = value; } } 类型参数化接口：与类型参数化类类似，可以在接口的定义中使用类型参数。 public interface MyInterface\u0026lt;T\u0026gt; { void doSomething(T item); } 类型参数化方法：在方法的定义中，使用尖括号\u0026lt;T\u0026gt;表示类型参数，可以在方法中使用该类型参数。 public \u0026lt;T\u0026gt; void printArray(T[] array) { for (T item : array) { System.out.println(item); } } 举例：\nMyClass\u0026lt;Integer\u0026gt; myObj = new MyClass\u0026lt;\u0026gt;(); myObj.setValue(10); int value = myObj.getValue(); // 10 MyInterface\u0026lt;String\u0026gt; myInterface = new MyInterface\u0026lt;String\u0026gt;() { @Override public void doSomething(String item) { System.out.println(\u0026#34;Doing something with \u0026#34; + item); } }; myInterface.doSomething(\u0026#34;Hello\u0026#34;); Integer[] array = {1, 2, 3, 4, 5}; printArray(array); 说⼀下你对注解的理解？ J a v a 注解本质上是⼀个标记，可以理解成⽣活中的⼀个⼈的⼀些⼩装扮，⽐如戴什么什么帽⼦，戴什么眼镜。\n注解在 Java 中是一种用来提供元数据的机制。它可以被用于标记代码中的元素，比如类、方法、字段等，来提供一些额外的信息。\n注解本质上是在源代码中的代码注释，它可以在编译时、运行时或者通过工具进行解析和处理。通过使用注解，可以给代码添加一些特定的含义和行为，从而影响编译器、工具或者程序的运行时行为。\n例如我们常见的 A O P，使⽤注解作为切点就是运⾏期注解的应⽤；⽐如 l o m b o k，就是注解在编译期的运⾏。 注解⽣命周期有三⼤类，分别是：\nRetentionPolicy.SOURCE：给编译器⽤的，不会写⼊ c l a s s ⽂件 RetentionPolicy.CLASS：会写⼊ c l a s s ⽂件，在类加载阶段丢弃，也就是运⾏的时候就没这个信息了 RetentionPolicy. RUNTIME：会写⼊ c l a s s ⽂件，永久保存，可以通过反射获取注解信息所以我上⽂写的是解析的时候，没写具体是解析啥，因为不同的⽣命周期的解析动作是不同的。 在 Java 中，注解可以被自定义，也可以使用内置的注解，比如@Override、@Deprecated、@SuppressWarnings 等。我们可以根据需要自定义注解，并通过反射的方式读取和解析注解信息，以实现更灵活和可扩展的编程。\n简述元注解 元注解可以理解为注解的注解，即在注解中使⽤，实现想要的功能。其具体分为：\n@Retention: 表示注解存在阶段是保留在源码，还是在字节码（类加载）或者运⾏期（JVM 中运⾏）。 @Target：表示注解作⽤的范围。 @Documented：将注解中的元素包含到 Javadoc 中去。 @Inherited：⼀个被@Inherited 注解了的注解修饰了⼀个⽗类，如果他的⼦类没有被其他注解修饰，则它的⼦类也继承了⽗类的注解。 @Repeatable：被这个元注解修饰的注解可以同时作⽤⼀个对象多次，但是每次作⽤注解⼜可以代表不同的含义。 反射是什么？应用？原理？ Java 反射机制是指在运行时动态地获取类的信息、创建对象以及调用对象的属性和方法的机制。Java 反射机制提供了运行时检查 Java 类型信息的能力，让 Java 程序可以通过程序获取其本身的信息。\n反射是一种在运行时获取、检查和操作类或对象的方式。通过反射，我们可以在程序运行时动态地获取和使用类的信息（如方法、字段、构造器等），并且可以通过反射来调用类的方法、访问和修改类的字段，甚至可以创建和实例化对象。\n反射最核心的四个类：\n反射的应⽤场景？\n⼀般我们平时都是在在写业务代码，很少会接触到直接使⽤反射机制的场景。但是，这并不代表反射没有⽤。相反，正是因为反射，你才能这么轻松地使⽤各种框架。像 S p r i n g / S p r i n g B o o t、M y B a t i s 等等框架中都⼤量使⽤了反射机制。\n像 S p r i n g ⾥的很多 注解 ，它真正的功能实现就是利⽤反射。\n就像为什么我们使⽤ S p r i n g 的时候 ，⼀个 @ C o m p o n e n t 注解就声明了⼀个类为 S p r i n g B e a n 呢？\n为什么通过⼀个 @ Va l u e 注解就读取到配置⽂件中的值呢？究竟是怎么起作⽤的呢？\n这些都是因为我们可以基于反射操作类，然后获取到类/属性/⽅法/⽅法的参数上的注解，注解这⾥就有两个作⽤，⼀是标记，我们对注解标记的类/属性/⽅法进⾏对应的处理；⼆是注解本⾝有⼀些信息，可以参与到处理的逻辑中。\n反射的原理？\n我们都知道 J a v a 程序的执⾏分为编译和运⾏两步，编译之后会⽣成字节码( . c l a s s )⽂件，J V M 进⾏类加载的时候，会加载字节码⽂件，将类型相关的所有信息加载进⽅法区，反射就是去获取这些信息，然后进⾏各种操作\n什么是反射机制？说说反射机制的优缺点、应用场景？ 来自：编程导航官方\nJava 反射机制是指在运行时动态地获取类的信息、创建对象以及调用对象的属性和方法的机制。Java 反射机制提供了运行时检查 Java 类型信息的能力，让 Java 程序可以通过程序获取其本身的信息。\nJava 反射机制的优点：\n可以动态地获取类的信息，不需要在编译时就知道类的信息。 可以动态地创建对象，不需要在编译时就知道对象的类型。 可以动态地调用对象的属性和方法，可以在运行时动态地改变对象的行为。 Java 反射机制的缺点：\n由于反射是动态的，所以它的运行效率较低，不如直接调用方法或属性。 由于反射是动态的，所以它会破坏 Java 的封装性，可能会使代码变得复杂和不稳定。 Java 反射机制的应用场景：\n动态代理。动态代理可以使用反射机制在运行时动态地创建代理对象，而不需要在编译时就知道接口的实现类。 单元测试。JUnit 等单元测试框架可以使用反射机制在运行时动态地获取类和方法的信息，实现自动化测试。 配置文件加载。许多框架（如 Spring）使用反射机制来读取和解析配置文件，从而实现依赖注入和面向切面编程等功能。 RPC 框架，RPC 框架就是动态的生成类对象，然后调用方法的。 简述 Java 中 Class 对象 java 中对象可以分为实例对象和 Class 对象，每⼀个类都有⼀个 Class 对象，其包含了与该类有关的信息。 获取 Class 对象的⽅法：\nClass.forName(“类的全限定名”) 实例对象.getClass() 类名.class Java8 新特性 Lambda 表达式了解多少？ L a m b d a 表达式本质上是⼀段匿名内部类，也可以是⼀段可以传递的代码\n⽐如我们以前使⽤ R u n n a b l e 创建并运⾏线程：\nnew Thread((new Runnable) { @Override public void run() { System.out.println(\u0026#34;实现Runnable创建线程\u0026#34;); } }).start(); 这是通过内部类的⽅式来重写 r u n ⽅法，使⽤ L a m b d a 表达式，还可以更加简洁：\nnew Thread( () -\u0026gt; System.out.println(\u0026#34;使用Lambda表达式创建线程\u0026#34;) ).start(); 当然不是每个接口都可以缩写成 Lambda 表达式。只有那些函数式接口（Function Interface）才能缩写成 L a m b d a 表⽰式。 所谓函数式接口（Function Interface）就是只包含⼀个抽象⽅法的声明。针对该接⼜类型的所有 Lambda 表达式都会与这个抽象⽅法匹配\nComparator 接口： List\u0026lt;Integer\u0026gt; strings = Arrays.asList(1, 2, 3); Collections.sort(strings, new Comparator\u0026lt;Integer\u0026gt;() { @Override public int compare(Integer o1, Integer o2) { return o1 - o2;} }); //Lambda Collections.sort(strings, (Integer o1, Integer o2) -\u0026gt; o1 - o2); //分解开 Comparator\u0026lt;Integer\u0026gt; comparator = (Integer o1, Integer o2) -\u0026gt; o1 - o2; Collections.sort(strings, comparator); J a v a 8 有哪些内置函数式接口？\nJ D K 1 . 8 A P I 包含了很多内置的函数式接⼜。其中就包括我们在⽼版本中经常见到的 C o m p a r a t o r 和 R u n n a b l e，J a v a 8 为他们都添加了 @FunctionInterface 注解，以⽤来⽀持 Lambda 表达式。除了这两个之外，还有 Callable、Predicate、Function、Suplier、Consumer 等等。\nOptional 了解吗？ O p t i o n a l 是⽤于防范 N u l l P o i n t e r E x c e p t i o n 。\n可以将 O p t i o n a l 看做是包装对象（可能是 n u l l , 也有可能⾮ n u l l ）的容器。当我们定义了 ⼀个⽅法，这个⽅法返回的对象可能是空，也有可能⾮空的时候，我们就可以考虑⽤ O p t i o n a l 来包装它，这也是在 J a v a 8 被推荐使⽤的做法\n看一段程序：\nOptional\u0026lt;String\u0026gt; optional = Optional.of(\u0026#34;bam\u0026#34;); //判断值是否存在 optional.isPresent();//true //返回值，不存在则抛出异常 optional.get();//\u0026#34;bam\u0026#34; //返回值，不存在就返回默认的值 optional.ofElse(\u0026#34;fallback\u0026#34;);//\u0026#34;bam\u0026#34; //判断值是否存在，存在则返回true optional.ifPresent((s) -\u0026gt; System.out.println(s.charAt(0)));//\u0026#34;b\u0026#34; Optional 类提供了一些常用的方法，用于操作和处理包装在其中的值。下面是一些常用的 Optional 方法的介绍：\nof(T value)\n创建一个包装了指定非空值的 Optional 实例。 如果传入的值为 null，则会抛出 NullPointerException。 ofNullable(T value)\n创建一个可能包装了指定值的 Optional 实例。 如果传入的值为 null，则创建一个空的 Optional 实例。 empty()\n创建一个空的 Optional 实例。 get()\n如果值存在，则返回包装的值。 如果值不存在，则抛出 NoSuchElementException。 isPresent()\n判断值是否存在。 如果值存在，则返回 true；否则返回 false。 ifPresent(Consumer\u0026lt;? super T\u0026gt; action)\n如果值存在，则执行给定的操作。 如果值不存在，则不执行任何操作。 orElse(T other)\n如果值存在，则返回包装的值。 如果值不存在，则返回传入的参数 other。 orElseGet(Supplier\u0026lt;? extends T\u0026gt; other)\n如果值存在，则返回包装的值。 如果值不存在，则执行传入的 Supplier 函数，并返回其结果。 orElseThrow(Supplier\u0026lt;? extends X\u0026gt; exceptionSupplier)\n如果值存在，则返回包装的值。 如果值不存在，则执行传入的异常 Supplier 函数，并抛出其结果。 这些方法可以根据具体的需求来选择使用，可以方便地对包装在 Optional 中的值进行操作和处理，避免空指针异常的发生。\nStream 流⽤过吗？ Stream 是 Java 8 新增的一个处理集合数据的概念。它可以让我们以一种更简洁、更流畅的方式对数据进行操作和处理。Stream 提供了一系列的中间操作和终端操作，可以实现数据的筛选、映射、排序、聚合等操作，使得代码更具可读性和可维护性。\n流的特点包括：\n以声明式的方式处理数据，而不是指令式的迭代操作集合。 流支持链式调用，可以将多个操作组合在一起形成流水线。 流可以进行惰性求值，只有在需要结果时才会执行。 常用的 Stream 操作包括：\n筛选数据：filter, distinct 转换数据：map, flatMap 排序数据：sorted 聚合数据：reduce, collect 统计数据：count, sum, average, min, max 循环数据：forEach, forEachOrdered 使用 Stream 需要注意以下几点：\n流是一次性消费的，对同一个流只能执行一次终端操作。 中间操作是惰性求值的，只有在进行终端操作时才会触发中间操作的执行。 可以对有序流和并行流进行操作。 Stream 的使用可以简化代码，并且提高代码的可读性和可维护性。它是现代 Java 开发中非常强大和常用的工具之一。\n演示常用的 Stream 操作：\n假设我们有一个列表，包含一些学生对象：\nList\u0026lt;Student\u0026gt; students = Arrays.asList( new Student(\u0026#34;Alice\u0026#34;, 22, \u0026#34;Math\u0026#34;), new Student(\u0026#34;Bob\u0026#34;, 19, \u0026#34;English\u0026#34;), new Student(\u0026#34;Charlie\u0026#34;, 20, \u0026#34;History\u0026#34;), new Student(\u0026#34;David\u0026#34;, 18, \u0026#34;Math\u0026#34;), new Student(\u0026#34;Eve\u0026#34;, 21, \u0026#34;English\u0026#34;) ); 筛选数据：筛选出年龄大于等于 20 岁的学生。 List\u0026lt;Student\u0026gt; result1 = students.stream() .filter(student -\u0026gt; student.getAge() \u0026gt;= 20) .collect(Collectors.toList()); System.out.println(result1); 输出结果：[Student{name=\u0026lsquo;Alice\u0026rsquo;, age=22, major=\u0026lsquo;Math\u0026rsquo;}, Student{name=\u0026lsquo;Charlie\u0026rsquo;, age=20, major=\u0026lsquo;History\u0026rsquo;}, Student{name=\u0026lsquo;Eve\u0026rsquo;, age=21, major=\u0026lsquo;English\u0026rsquo;}]\n转换数据：将学生列表转换为姓名的列表。 List\u0026lt;String\u0026gt; result2 = students.stream() .map(Student::getName) .collect(Collectors.toList()); System.out.println(result2); 输出结果：[Alice, Bob, Charlie, David, Eve]\n排序数据：按照年龄从小到大对学生进行排序。 List\u0026lt;Student\u0026gt; result3 = students.stream() .sorted(Comparator.comparingInt(Student::getAge)) .collect(Collectors.toList()); System.out.println(result3); 输出结果：[Student{name=\u0026lsquo;David\u0026rsquo;, age=18, major=\u0026lsquo;Math\u0026rsquo;}, Student{name=\u0026lsquo;Bob\u0026rsquo;, age=19, major=\u0026lsquo;English\u0026rsquo;}, Student{name=\u0026lsquo;Charlie\u0026rsquo;, age=20, major=\u0026lsquo;History\u0026rsquo;}, Student{name=\u0026lsquo;Eve\u0026rsquo;, age=21, major=\u0026lsquo;English\u0026rsquo;}, Student{name=\u0026lsquo;Alice\u0026rsquo;, age=22, major=\u0026lsquo;Math\u0026rsquo;}]\n聚合数据：计算学生的平均年龄。 double averageAge = students.stream() .mapToInt(Student::getAge) .average() .orElse(0); System.out.println(averageAge); 输出结果：20.0\n设计模式 抽象类体现了怎么样的设计思想?在哪些设计模式中有使用到? 抽象类体现了面向对象设计中的\u0026quot;抽象\u0026quot;和\u0026quot;封装\u0026quot;两个重要思想。\n抽象思想：抽象类是一种不能被实例化的类，它的目的是定义一组相关的抽象方法，但不提供其具体实现。抽象方法只有方法签名，没有方法体。通过这种方式，抽象类定义了一种规范和契约，要求其子类必须实现这些抽象方法，从而使得子类具有一定的一致性和可替代性。抽象类关注的是\u0026quot;是什么\u0026quot;，而不关心\u0026quot;怎么做\u0026quot;。\n封装思想：抽象类将一组相关的方法和属性封装在一个类中，对外部提供统一的抽象接口。外部代码只需要知道如何使用这些接口，而不需要了解具体实现细节。这样可以降低系统的复杂性，提高代码的可维护性和可扩展性。\n在设计模式中，抽象类被广泛应用在以下几种设计模式中：\n模板方法模式（Template Method Pattern）：模板方法模式是一种行为型模式，它定义了一个操作中的算法框架，将一些步骤的具体实现延迟到子类中。抽象类作为模板类，其中定义了模板方法和一些抽象方法，子类通过实现这些抽象方法来完成特定的步骤。\n工厂方法模式（Factory Method Pattern）：工厂方法模式是一种创建型模式，它将对象的创建延迟到子类中。抽象类可以作为工厂方法的返回类型或参数类型，从而使得具体的创建逻辑由子类实现。\n组合模式（Composite Pattern）：组合模式是一种结构型模式，它允许将对象组合成树形结构来表示\u0026quot;部分-整体\u0026quot;的层次关系。在组合模式中，抽象类可以用于定义组合对象和叶子对象的统一接口，以便于对整个树形结构进行递归处理。\n策略模式（Strategy Pattern）：策略模式是一种行为型模式，它定义了一组算法，将每个算法都封装起来，并使它们可以相互替换。抽象类可以用于定义策略的公共接口，具体策略则由子类实现。\n这只是抽象类在设计模式中的一些应用，实际上抽象类在面向对象设计中还有更广泛的应用。通过合理运用抽象类，可以提高代码的复用性、灵活性和可维护性，使得系统更加稳健和易于扩展。\n接口体现的三种设计模式 策略模式\n策略模式的思想是：针对一组算法，将每一种算法封装到具有共同接口的实现类中，接口的实现者可以在不影响调用者的情况下自定义实现算法。\ninterface Coach { void defend(); } class Hesai implements Coach { @Override public void defend() { System.out.println(\u0026#34;防守赢得冠军\u0026#34;); } } //德普·瓜迪奥拉 class Guatu implements Coach { @Override public void defend() { System.out.println(\u0026#34;进攻就是最好的防守\u0026#34;); } } 适配器模式\n适配器模式的思想是，针对调用者的需求对原有的接口进行转接。生活当中最常见的适配器就是 HDMI（英语： High Definition Multimedia Interface ，中文：高清多媒体接口）线，可以同时发送音频和视频信号。\n适配器模式的示例如下 ：\ninterface Coach { void defend(); void attack(); } //抽象类实现接口， 并置空方法 abstract class AdapterCoach implements Coach { public void defend() {}; public void attack() {}; } // 新类继承适配器 class Hesai extends AdapterCoach { public void defend() { System.out.println(\u0026#34;防守赢得冠军\u0026#34;); } } public class Demo { public static void main(String[] args) { Coach coach = new Hesai(); coach.defend(); } } Coach 接口中定义了两个方法（ defend() 和 attack() ），如果类直接实现该接口的话，就需要对两个方法进行实现。\n如果我们只需要对其中一个方法进行实现的话，就可以使用一个抽象类作为中间件，即适配器（AdapterCoach），用这个抽象类实现接口，并对抽象类中的方法置空（方法体只有一对花括号），这时候，新类就可以绕过接口，继承抽象类，我们就可以只对需要的方法进行覆盖，而不是接口中的所有方法\n工厂模式\n// 教练 interface Coach { void command(); } // 教练学院 interface CoachFactory { Coach createCoach(); } // A级教练 class ACoach implements Coach { @Override public void command() { System.out.println(\u0026#34;我是A级证书教练\u0026#34;); } } // A级教练学院 class ACoachFactory implements CoachFactory { @Override public Coach createCoach() { return new ACoach(); } } // C级教练 class CCoach implements Coach { @Override public void command() { System.out.println(\u0026#34;我是C级证书教练\u0026#34;); } } // C级教练学院 class CCoachFactory implements CoachFactory { @Override public Coach createCoach() { return new CCoach(); } } public class Demo { public static void create(CoachFactory factory) { factory.createCoach().command(); } public static void main(String[] args) { // 对于一支球队来说， 需要什么样的教练就去找什么样的学院 // 学院会介绍球队对应水平的教练。 create(new ACoachFactory()); create(new CCoachFactory()); } } 有两个接口，一个是 Coach（教练），可以 command() （指挥球队）；另外一个是 CoachFactory（教练学院），能 createCoach() （教出一名优秀的教练）。然后 ACoach 类实现 Coach 接口， ACoachFactory 类实现 CoachFactory 接口； CCoach 类实现 Coach 接口， CCoachFactory 类实现 CoachFactory 接口。当需要 A 级教练时，就去找 A 级教练学院；当需要 C 级教练时，就去找 C 级教练学院。\n依次类推，我们还可以用 BCoach 类实现 Coach 接口， BCoachFactory 类实现 CoachFactory 接口，从而不断地丰富教练的梯队。\n单例模式懒汉式 Java 单例模式的不同写法：https://blog.csdn.net/fly910905/article/details/79286680\nJava 中单例(Singleton)模式是一种广泛使用的设计模式。单例模式的主要作用是保证在 Java 程序中，某个类只有一个实例存在。\n它的核心在于，单例模式可以保证一个类仅创建一个实例，并提供一个访问它的全局访问点。\n// 懒汉模式 + synchronized 同步锁 + double-check public class Singleton { // 延迟加载保证多线程安全 Private volatile static Singleton singleton; private Singleton(){} public static Singleton getInstance(){ //当singleton为Null时 if(singleton == null){ synchronized(Singleton.class){ //双重校验 if(singleton == null){ singleton = new Singleton(); } } } return singleton; } } 使用 volatile 是防止指令重排序，保证对象可见，防止读到半初始化状态的对象 第一层 if(singleton == null) 是为了防止有多个线程同时创建 synchronized 是加锁防止多个线程同时进入该方法创建对象 第二层 if(singleton == null) 是防止有多个线程同时等待锁，一个执行完了后面一个又继续执行的情况 其他 OOM 怎么去分析，怎么触发，怎么解决？ OOM（Out of Memory）指的是内存溢出，即在程序运行时无法分配到足够的内存空间。接下来，我将介绍如何分析、触发和解决 OOM 问题。\n分析 OOM：\n查看错误日志：程序在发生 OOM 时，一般会输出错误日志。可以查看错误日志中的堆栈信息来确定具体的异常和发生异常的位置。 使用内存分析工具：使用工具如 MAT（Memory Analyzer Tool）或 VisualVM 等，可以分析 OOM 问题。这些工具能够检测内存泄漏、查看内存使用情况、查找大对象等，帮助快速定位引发 OOM 的原因。 触发 OOM：\n创建大量的对象：通过循环创建大量对象，超过 JVM 堆内存限制，可以触发 OOM。 加载大量数据：读取大型文件或从数据库中加载大量数据至内存时，如果超出 JVM 堆内存限制，也可能触发 OOM。 递归调用：在递归调用中，如果没有正确的退出条件或递归的深度过大，也可能导致 OOM。 解决 OOM：\n检查内存泄漏：查看代码，确保无用的对象及时被释放，避免长时间保留引用。确保及时关闭资源，如文件句柄或数据库连接等。 调整 JVM 参数：可以通过调整 JVM 参数来增加堆内存大小，如-Xmx 和-Xms 参数分别用于指定 JVM 最大堆内存和初始堆内存大小。 优化代码逻辑：检查代码中是否存在无限循环、过大的数据集、频繁的创建大对象等问题，进行合理优化。 使用合适的数据结构：合理选择存储大数据集的数据结构，如使用迭代器、分页查询等方式，减少内存占用。 在解决 OOM 问题时，需要结合具体的应用场景和代码进行分析和优化，保证程序在运行过程中能够合理地利用内存资源。\n集合 Java 中集合有哪些？ 集合：Collection 和 Map\nCollection 包括 list、set、queue，其中 list 的实现主要有 ArrayList 和 LinkedList，set 的实现主要有 HashSet 和 LinkedHashSet，queue 的实现主要有 ArrayQueue、LinkedList、PriorityQueue\nMap 实现主要有 HashMap、LinkedHashMap 和 TreeMap\n如果我们需要按照顺序存储一组元素，那么 ArrayList 和 LinkedList 可能更适合； 如果我们需要存储键值对并根据键进行查找，那么 HashMap 可能更适合。 当然，在某些情况下， HashSet仍然是最好的选择。例如，当我们需要快速查找一个元素是否存在于某个集合中，并且我们不需要对元素进行排序时， HashSet 可以提供高效的性能。 List：有序可重复\nArrayList，基于数组实现的支持随机访问元素 LinkedList，基于链表实现，不需要扩容但需要存储节点 Vector 数组实现，线程安全 Set：无序不可重复\nHashSet，存储元素无序且不重复，底层是 HashMap LinkedHashSet，底层是哈希表和链表 TreeSet 底层是红黑树结构 Map：\nHashTable 哈希表结构，线程安全\nHashMap，哈希表结构，存储键值对，有线程安全问题\nLinkedHashMap，底层是 HashMap 和链表\nConcurrentHashMap，线程安全，HashMap 线程安全的实现\nTreeMap 红黑树结构\nQueue：\nPriorityQueue （优先队列）是在 JDK1.5 中被引入的, 其与 Queue 的区别在于元素出队顺序是与优先级相关的，即总是优先级最高的元素先出队\nBlockingQueue （阻塞队列）是一个接口，继承自 Queue。BlockingQueue阻塞的原因是其支持当队列没有元素时一直阻塞，直到有元素；还支持如果队列已满，一直等到队列可以放入新元素时再放入。\n数据结构\n算法复杂度分析：\nJava 中线程安全的数据结构有哪些 HashTable：哈希表的线程安全版，效率低 ConcurrentHashMap：哈希表 Hashmap 的线程安全版，效率高，用于替代 HashTable Vector：线程安全版 ArrayList Stack：线程安全版栈 BlockingQueue 及其子类：线程安全版队列 Set 集合实现原理 Set 集合是一种不允许重复元素的数据结构，它的实现原理主要涉及两个方面：哈希表（Hash Table）和哈希算法（Hashing）。\n哈希表（Hash Table）：Set 集合的底层数据结构通常使用哈希表来实现。哈希表是一个以键值对存储数据的数据结构，它使用哈希函数将元素的键（或者值）映射到存储位置上。哈希表通过数组和链表或红黑树的组合来实现。\n哈希算法（Hashing）：Set 集合使用哈希算法来确定元素在哈希表中的存储位置。哈希算法将元素的键通过哈希函数进行计算，生成一个哈希值作为元素在数组中的索引。哈希函数应满足以下特点：\n一致性：对于同一个元素，哈希函数计算得到的哈希值应相同。 均匀性：哈希函数应该尽可能均匀地分布元素，以减少哈希冲突。 高效性：哈希函数应该具有高效的计算性能。 随机性：哈希函数应该对不同的元素生成不同的哈希值。 在 Set 集合中，当插入一个元素时，首先根据哈希函数计算出该元素的哈希值，然后将元素存储在哈希值对应的数组位置中。如果发现该位置已经存在元素（即出现哈希冲突），则根据链表或红黑树的特性，在该位置上继续存储元素。\n在进行元素的查找、插入和删除操作时，Set 集合会先计算出要操作的元素的哈希值，并根据哈希值找到元素在哈希表中的位置，然后根据链表或红黑树的特性进行相应的操作。\n通过哈希表和哈希算法的结合，Set 集合能够在常数时间内（平均情况下）实现元素的查找、插入和删除操作，同时还能保证元素的唯一性。\nSet 集合如何保证元素不重复 Set 集合保证元素的唯一性是通过以下两个机制来实现的：\n哈希表（Hash Table）：Set 集合的底层数据结构通常使用哈希表来存储元素。哈希表使用哈希函数将元素的键（或者值）映射到存储位置上。当插入元素时，Set 集合会先计算元素的哈希值，然后根据哈希值找到对应的存储位置。如果该位置已经存在了元素，则说明发生了哈希冲突，Set 集合会根据链表或红黑树的特性，在该位置继续存储元素。在查询元素时，Set 集合也会通过哈希值找到元素的存储位置。\nequals() 方法：Set 集合通过元素的 equals()方法来判断元素的唯一性。当插入一个元素时，Set 集合会先计算该元素的哈希值，并根据哈希值找到存储位置。然后，Set 集合会调用元素类的 equals()方法，将要插入的元素与已存在的元素进行比较。如果 equals()方法返回 true，说明要插入的元素已经存在于 Set 集合中，Set 集合会拒绝插入该元素。如果 equals()方法返回 false，说明要插入的元素不与已存在的元素相等，Set 集合会根据链表或红黑树的特性，在存储位置上继续存储该元素。\n综合使用哈希表和 equals()方法，Set 集合能够在插入元素时避免重复元素的插入，以及在查询元素时能够准确地判断元素是否存在。如果元素的哈希算法和 equals()方法实现得合理，Set 集合可以高效地保证元素的唯一性。\nArrayList 和 LinkedList 的区别？ 面试题：ArrayList 和 LinkedList 的区别？\n底层数据结构：ArrayLists 底层是基于动态数组，而 LinkedList 底层是基于双向链表实现 插入和删除元素：ArrayList 在插入/删除元素时效率不如 LinkedList 高效，当需要在链表中的任意位置进行插入和删除操作时，LinkedList 相较于 ArrayList 通常更具优势，因为它不需要移动其他元素，只需要修改对应的节点指向。 访问元素时，ArrayList 可以随机访问元素，时间复杂度 O（1），LinkedList 需要遍历找到目标元素，时间复杂度 O（n） 内存使用方面，ArrayList 是动态数组，根据存储元素的大小进行适当的扩容，而 LinkedList 不需要扩容，但是需要保存节点的指针，在小数据量时 ArrayList 占用内存较少 线程安全方面，ArrayList 和 LiknedList 都是线程不安全的，但由于 LinkedList 对元素的操作不需要同时修改节点的指针，因此在多线程修改时可能会比 ArrayList 更安全。 补充：ArrayList 和 LinkedList 的使用场景：\n**需要频繁随机访问元素的时候，例如读取大量数据并进行处理或者需要对数据进行排序或查找的场景，可以使用 ArrayList。**例如一个学生管理系统，需要对学生列表进行排序或查找操作，可以使用 ArrayList 存储学生信息，以便快速访问和处理。\n**当需要频繁插入和删除元素的时候，例如实现队列或栈，或者需要在中间插入或删除元素的场景，可以使用 LinkedList。**例如一个实时聊天系统，需要实现一个消息队列，可以使用 LinkedList 存储消息，以便快速插入和删除消息。\nArrayList 底层扩容原理 ArrayList 底层的扩容是通过ensureCapacityInternal()方法实现的。\n下面是 ArrayList 底层扩容的简要过程：\nArrayList 在创建时，默认容量是 10，也可以通过构造方法指定初始容量。当需要添加元素时，如果当前元素个数已经等于容量大小，就需要进行扩容操作。\n在进行扩容操作时，会调用grow()方法进行实际的扩容。\ngrow()方法会将旧数组扩容到原来的 1.5 倍容量，然后判断新数组容量是否大于最小需要容量，如果还是小于最小需要容量就赋值最小需要容量。 然后还会判断新容量是否超出了阈值（Integer.MAX_VALUE - 8），如果超出，进入(执行) hugeCapacity() 方法来比较 minCapacity 和 MAX_ARRAY_SIZE，如果 minCapacity 大于最大容量，则新容量则为Integer.MAX_VALUE，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 Integer.MAX_VALUE - 8 然后调用Arrays.copyOf()方法创建一个新的数组，将原数组中的元素复制到新数组中。\n最后，将新数组设置为 ArrayList 的内部存储数组，用于存储后续的元素。\n这种扩容机制可以保证在大部分情况下，ArrayList 的扩容操作的时间复杂度是 O(1)的。但在少数情况下，如果需要扩容到极限值(Integer.MAX_VALUE)，则会导致扩容操作的时间复杂度变为 O(n)，其中 n 为当前 ArrayList 的元素个数。\nArrayList 底层是用动态数组实现的\nArrayList 初始化容量为 0，给初始数组扩容到 10\n如何实现数组和 List 之间的转换？ 如何实现数组和 List 之间的转换： 数组转为 List：Arrays.asList()方法\nList 转为数组：toArray()方法\n如果数组转为 List 之后，如果修改了数组内容，list 会受影响吗？ 答案是会的。\n数组转集合，底层是使用 Arrays 类中的一个内部类创建的集合，并没有创建新的对象，他们指向同一个内存地址，所以修改了数组集合也会变化。\n如果 List 转为数组之后，如果修改了 List 内容，数组会受影响吗？ 答案是不会。\nList 调用了 toArray()方法转数组后，如果修改了 list 内容，数组不会有影响，底层进行的是数组的拷贝，创建了新的数组并拷贝数据，所以修改的 list 跟数组没什么关系，当然数组也不会有影响。\n类似于 Java 中 的浅拷贝和深拷贝\nComparable 和 Comparator 的区别 Comparable 接口 如果一个类实现了 Comparable 接口（只需要干一件事，重写 compareTo() 方法），就可以按照自己制定的规则将由它创建的对象进行比较。\ncompareTo() 方法，该方法的返回值可能为负数，零或者正数，代表的意思是该对象按照排序的规则小于、等于或者大于要比较的对象。如果指定对象的类型与此对象不能进行比较，则引发 ClassCastException 异常（自从有了泛型，这种情况就少有发生了）。\nComparator 接口 public interface Comparator\u0026lt;T\u0026gt; { int compare(T o1, T o2); boolean equals(Object obj); } Comparator 接口比 Comparable 接口要复杂多了，但核心方法就两个：\n第一个方法 compare(T o1, T o2) 的返回值可能为负数，零或者正数，代表的意思是第一个对象小于、等于或者大于第二个对象。 第二个方法 equals(Object obj) 需要传入一个 Object 作为参数，并判断该 Object 是否和 Comparator 是否相等 Comparable 和 Comparator 两者之间的区别：\n一个类实现了 Comparable 接口，意味着该类的对象可以直接进行比较（排序），但比较（排序）的方式只有一种，很单一。 一个类如果想要保持原样，又需要进行不同方式的比较（排序），就可以定制比较器（实现 Comparator 接口）。 Comparable 接口在 java.lang 包下，而 Comparator 接口在 java.util 包下，算不上是亲兄弟，但可以称得上是表（堂）兄弟。 **总而言之，如果对象的排序需要基于自然顺序，请选择 Comparable ，如果需要按照对象的不同属性进行排序，请选择 Comparator **\nCollection.sort 方法使用的哪种排序算法 在 Java 中，Collection.sort()方法用于对集合进行排序。具体使用哪种排序方法取决于集合元素的类型和排序需求。\n默认情况下，Collection.sort()方法使用的是 Java 的默认排序算法，即“TimSort”。该算法是结合了归并排序和插入排序的混合算法，能够在多种情况下都表现出良好的性能。\n如果要对自定义的对象进行排序，可以根据对象的比较规则实现Comparable接口，并在实现类中重写compareTo()方法，然后调用Collection.sort()方法进行排序。排序时将使用对象的compareTo()方法进行比较。\n除了默认排序外，还可以使用Comparator接口实现自定义的排序。Comparator接口定义了compare()方法，通过实现该方法可以指定特定的排序规则。Collection.sort()方法还有一个重载版本，接受一个Comparator对象作为额外的参数，用于指定自定义的排序方法。\n需要注意的是，对于大多数的基本数据类型（例如int、double等），它们的包装类已经实现了Comparable接口和默认的排序规则，可以直接使用Collection.sort()方法进行排序。\n总而言之，Collection.sort()方法用于对集合进行排序时，可以使用默认的排序算法或者自定义的排序规则，以实现各种排序需求。\n比较 HashSet、LinkedHashSet 和 TreeSet 三者的异同 Set 即集合，该数据结构不允许元素重复且⽆序。 Java 对 Set 有三种实现⽅式：\nHashSet、LinkedHashSet 和 TreeSet 都是 Set 接口的实现类，都能保证元素唯一，并且都不是线程安全的。 HashSet、LinkedHashSet 和 TreeSet 的主要区别在于底层数据结构不同： HashSet 的底层数据结构是哈希表（基于 HashMap 实现）。 LinkedHashSet 的底层数据结构是链表和哈希表，元素的插入和取出顺序满足 FIFO。 TreeSet 底层数据结构是红黑树，元素是有序的，排序的方式有自然排序和定制排序。 底层数据结构不同又导致这三者的应用场景不同： HashSet 用于不需要保证元素插入和取出顺序的场景， LinkedHashSet 用于保证元素的插入和取出顺序满足 FIFO 的场景， TreeSet 用于支持对元素自定义排序规则的场景 Queue 与 Deque 的区别 Queue 是单端队列，只能从一端插入元素，另一端删除元素，实现上一般遵循 先进先出（FIFO） 规则。\nQueue 扩展了 Collection 的接口，根据 因为容量问题而导致操作失败后处理方式的不同 可以分为两类方法: 一种在操作失败后会抛出异常，另一种则会返回特殊值。\nQueue 接口 抛出异常 返回特殊值 插入队尾 add(E e) offer(E e) 删除队首 remove() poll() 查询队首元素 element() peek() Deque 是双端队列，在队列的两端均可以插入或删除元素。\nDeque 扩展了 Queue 的接口, 增加了在队首和队尾进行插入和删除的方法，同样根据失败后处理方式的不同分为两类：\nDeque 接口 抛出异常 返回特殊值 插入队首 addFirst(E e) offerFirst(E e) 插入队尾 addLast(E e) offerLast(E e) 删除队首 removeFirst() pollFirst() 删除队尾 removeLast() pollLast() 查询队首元素 getFirst() peekFirst() 查询队尾元素 getLast() peekLast() 事实上，Deque 还提供有 push() 和 pop() 等其他方法，可用于模拟栈\nMap XXX HashMap 和 Hashtable 有什么区别？ **线程是否安全：**HashMap 是非线程安全的， HashTable 是线程安全的。因为 HashTable 内部的方法基本都经过 synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap）；\n**效率：**因为线程安全的问题， HashMap 要比 HashTable 效率高一点。另外， HashTable 基本被淘汰，不要在代码中使用它；\n**对 Null key 和 Null value 的支持：**HashMap 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个；HashTable 不允许有 null 键和 null 值，否则会抛出 NullPointerException 。\n初始容量带下和每次扩充容量大小的不同 ：\n① 创建时如果不指定容量初始值， Hashtable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。\n② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为 2 的幂次方大小（ HashMap 中的tableSizeFor()方法保证）。\n**底层数据结构：**JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。\nHashtable 使⽤ Enumeration 进⾏遍历， HashMap 使⽤ Iterator 进⾏遍历\nHashMap 和 HashSet 区别 HashSet 底层就是基于 HashMap 实现的。（HashSet 的源码非常非常少，因为除了 clone()、writeObject()、readObject()是 HashSet 自己不得不实现之外，其他方法都是直接调用 HashMap 中的方法。\nHashMap HashSet 实现了 Map 接口 实现 Set 接口 存储键值对 仅存储对象 调用 put()向 map 中添加元素 调用 add()方法向 Set 中添加元素 HashMap 使用键（Key）计算 hashcode HashSet 使用成员对象来计算 hashcode 值，对于两个对象来说 hashcode 可能相同，所以equals()方法用来判断对象的相等性 HashSet 中， equals 与 hashCode 之间的关系？ equals 和 hashCode 这两个⽅法都是从 object 类中继承过来的， equals 主要⽤于判断对象的内存地址引⽤是否是同⼀个地址； hashCode 根据定义的哈希规则将对象的内存地址转换为⼀个哈希码。\n**HashSet 中存储的元素是不能重复的，**主要通过 hashCode 与 equals 两个⽅法来判断存储的对象是否相同：\n如果两个对象的 hashCode 值不同，说明两个对象不相同。可以直接存储。 如果两个对象的 hashCode 值相同，接着会调⽤对象的 equals ⽅法，如果 equlas ⽅法的返回结果为 true，那么说明两个对象相同，则不能加入集合。 当你把对象加入HashSet时，HashSet 会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他加入的对象的 hashcode 值作比较，如果没有相符的 hashcode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用equals()方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让加入操作成功。\n注意：\n在 JDK1.8 中，HashSet的add()方法只是简单的调用了HashMap的put()方法，并且判断了一下返回值以确保是否有重复元素。\n在 JDK1.8 中，实际上无论HashSet中是否已经存在了某元素，HashSet都会直接插入，只是会在add()方法的返回值处告诉我们插入前是否存在相同元素\n简述 Java 的 LinkedHashMap 与 TreeMap 首先，我们知道 HashMap 是一种常用的哈希表数据结构，它可以快速地进行键值对的查找和插入操作。但是， HashMap 本身并不保证键值对的顺序，如果我们需要按照插入顺序或访问顺序来遍历键值对，就需要使用 LinkedHashMap 了。\nLinkedHashMap 继承自 HashMap，它在 HashMap 的基础上，增加了一个双向链表来维护键值对的顺序。这个链表可以按照插入顺序或访问顺序排序，它的头节点表示最早插入或访问的元素，尾节点表示最晚插入或访问的元素。这个链表的作用就是让 LinkedHashMap 可以保持键值对的顺序，并且可以按照顺序遍历键值对。\nLinkedHashMap 还提供了两个构造方法来指定排序方式，分别是按照插入顺序排序和按照访问顺序排序。在按照访问顺序排序的情况下，每次访问一个键值对，都会将该键值对移到链表的尾部，以保证最近访问的元素在最后面。如果需要删除最早加入的元素，可以通过重写 removeEldestEntry() 方法来实现。\nTreeMap 是底层利⽤红⿊树实现的 Map 结构，底层实现是⼀棵平衡的⼆叉搜索树，由于红⿊树的插⼊、删除、遍历时间复杂度都为 O(logN)，所以性能上低于哈希表。但是哈希表⽆法提供键值对的有序输出，红⿊树可以按照键的值的⼤⼩有序输出。\n红黑树\n红黑树的示意图（R 即 Red「红」、 B 即 Black「黑」）：\n红黑树是一种自平衡的二叉搜索树，索引的红黑规则都是希望红黑树能够保持平衡\n红黑树，顾名思义，就是节点是红色或者黑色的平衡二叉树，它通过颜色的约束来维持二叉树的平衡，它要求任意一条路径上的黑色节点数目相同，同时还需要满足一些其他特定的条件，如红色节点的父节点必须为黑色节点等。\n每个节点都只能是红色或者黑色\n根节点是黑色\n每个叶节点（Null 节点，空节点）是黑色的。\n如果一个节点是红色的，则它两个子节点都是黑色的。也就是说在一条路径上不能出现相邻的两个红色节点。\n从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。\n由于红黑树的平衡度比 AVL 树稍低，因此在进行插入和删除操作时需要进行的旋转操作较少，但是在查找操作时效率仍然较高。\n红黑树适用于读写操作比较均衡的场景 （保持平衡）\nHashMap 底层原理 实现原理\nJDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashcode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) \u0026amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的长度）。\n如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同\n如果 hash 值相同 key 也相同的话，就直接覆盖 如果 hash 值相同 key 不相同就通过拉链法解决哈希冲突。 所谓扰动函数指的就是 HashMap 的 hash 方法。\n使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少哈希碰撞。\nJDK1.7 和 JDK1.8，HashMap 的区别？\n相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。\n所谓 “拉链法” 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。\n相比于之前的版本， JDK1.8 之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。\ntreeifyBin方法：将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树\nTreeMap、TreeSet 以及 JDK1.8 之后的 HashMap 底层都用到了红黑树。红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。\n为什么是链表转红黑树的阈值是 8？\n节点到 8 的时候转红黑树，因为当链表的节点达到 8 个时，哈希冲突的概率已经非常大了，查询的性能有可能会从 o(1)退化成链表的 o(n)\nHashMap 和 CurrentHashMap 的对比 HashMap 和 ConcurrentHashMap 都是 Java 中用于存储键值对的数据结构，但它们在并发性和用法方面有一些重要的区别。\n并发性：\nHashMap：HashMap 不是线程安全的。当多个线程同时访问或修改同一个 HashMap 时，可能会导致数据不一致或其他问题。 ConcurrentHashMap：ConcurrentHashMap 是专为多线程环境设计的，它提供了更高的并发性。多个线程可以同时进行读取操作，而写入操作仍然是线程安全的。它通过分段锁（Segment）来实现并发控制，不同的分段可以被不同的线程同时访问。 性能：\nHashMap：在单线程环境下，HashMap 的性能通常比 ConcurrentHashMap 略好，因为后者需要额外的并发控制。 ConcurrentHashMap：在高并发环境下，特别是多线程进行读取操作的情况下，ConcurrentHashMap 的性能通常比 HashMap 要好，因为它能够支持更多的并发操作。 迭代器和一致性：\nHashMap：HashMap 的迭代器不保证在多线程环境下的一致性。在迭代期间如果发生结构性的修改（如插入或删除操作），可能会导致 ConcurrentModificationException 异常。 ConcurrentHashMap：ConcurrentHashMap 的迭代器保证不会抛出 ConcurrentModificationException 异常，并且提供了弱一致性保证。在迭代期间，迭代器能够看到其他线程的修改，但不一定是最新的状态。 null 值：\nHashMap：允许键和值都为 null。 ConcurrentHashMap：不允许键或值为 null，否则会抛出 NullPointerException。 总的来说，如果您在多线程环境中需要使用一个映射数据结构，并且需要高并发性能，那么 ConcurrentHashMap 是一个更好的选择。如果您在单线程环境中使用，或者并发性要求不高，那么普通的 HashMap 可能更适合。在选择时，根据应用程序的需求和使用场景来决定使用哪种数据结构。\nJDK8 之前底层实现是数组 + 链表， JDK8 改为数组 + 链表/红⿊树。主要成员变量包括存储数据的 table 数组、元素数量 size、加载因⼦ loadFactor。 HashMap 中数据以键值对的形式存在，键对应的 hash 值⽤来计算数组下标，如果两个元素 key 的 hash 值⼀样，就会发⽣哈希冲突，被放到同⼀个链表上。\ntable 数组记录 HashMap 的数据，每个下标对应⼀条链表，所有哈希冲突的数据都会被存放到同⼀条链表， Node/Entry 节点包含四个成员变量： key、 value、 next 指针和 hash 值。在 JDK8 后链表超过 8 会转化为红⿊树。\n若当前数据/总数据容量\u0026gt;负载因⼦， Hashmap 将执⾏扩容操作。默认初始化容量为 16，扩容容量必须是 2 的幂次⽅、最⼤容量为 1\u0026laquo; 30 、默认加载因⼦为 0.75\nHashMap1.7 和 HashMap1.8 的区别\n不同点 hashMap 1.7 hashMap 1.8 数据结构 数组+链表 数组+链表+红黑树 插入数据的方式 头插法 尾插法 hash 值计算方式 9 次扰动处理(4 次位运算+5 次异或) 2 次扰动处理(1 次位运算+1 次异或) 扩容策略 插入前扩容 插入后扩容 ConcurrentHashMap1.7 和 1.8 的区别\n只记重点\n不同点 concurrentHashMap 1.7 concurrentHashMap 1.8 锁粒度 基于 segment 基于 entry 节点 锁 reentrantLock synchronized 底层结构 Segment + HashEntry + Unsafe Synchronized + CAS + Node + Unsafe HashMap 多线程操作导致死循环问题 JDK1.7 hashmap 底层结构：数组+链表 链表是头插法 JDK1.8 hashmap 底层结构：数组+链表+红黑树 链表是尾插法 JDK1.7 及之前版本的 HashMap 在多线程环境下扩容操作可能存在死循环问题，这是由于当一个桶位中有多个元素需要进行扩容时，多个线程同时对链表进行操作，头插法可能会导致链表中的节点指向错误的位置，从而形成一个环形链表，进而使得查询元素的操作陷入死循环无法结束。\n为了解决这个问题，**JDK1.8 版本的 HashMap 采用了尾插法而不是头插法来避免链表倒置，使得插入的节点永远都是放在链表的末尾，避免了链表中的环形结构。**但是还是不建议在多线程下使用 HashMap，因为多线程下使用 HashMap 还是会存在数据覆盖的问题。并发环境下，推荐使用 ConcurrentHashMap 。\n一般面试中这样介绍就差不多，不需要记各种细节，个人觉得也没必要记。如果想要详细了解 HashMap 扩容导致死循环问题，可以看看耗子叔的这篇文章：Java HashMap 的死循环\n在 JDK 1.7 中，HashMap 使用了一个数组+链表的数据结构来存储键值对。头插法插入元素是指在链表的头部插入新节点，这样插入效率较高。然而，这种插入方式可能在某些情况下会导致环形链表的问题，也就是链表中出现环。\n这个问题主要是由于链表的头插法插入方式，以及多线程并发修改 HashMap 时可能引发的问题所致。在多线程环境下，当多个线程同时进行插入、删除等操作时，可能会导致链表的结构发生变化，使得某些链表节点的 next 指针指向了之前的节点，从而形成环。\n这种环形链表的情况可能导致 HashMap 的一些操作（如查找、插入、删除等）变得低效，甚至可能导致死循环。为了解决这个问题，JDK 1.8 对 HashMap 进行了改进，引入了红黑树（或者在链表长度较小的情况下仍然使用链表），以及更好的并发控制，从而避免了环形链表问题。\n如果你在使用 HashMap 时遇到类似的问题，建议升级到 JDK 1.8 及更高版本，或者采取其他的线程安全的数据结构。\n为何 HashMap 线程不安全 三种表现：\n在 JDK1.7 中， HashMap 采⽤头插法插⼊元素，因此并发情况下同时插入元素可能会导致环形链表，产⽣死循环。 虽然 JDK1.8 采⽤了尾插法解决了这个问题，但是并发下的 put 操作也会使前⼀个 key 被后⼀个 key 覆盖。 由于 HashMap 有扩容机制存在，也存在 A 线程进⾏扩容后， B 线程执⾏ get ⽅法出现失误的情况 JDK1.7 及之前版本，在多线程环境下，HashMap 扩容时会造成死循环和数据丢失的问题。\n数据丢失这个在 JDK1.7 和 JDK 1.8 中都存在，这里以 JDK 1.8 为例进行介绍。\nJDK 1.8 后，在 HashMap 中，多个键值对可能会被分配到同一个桶（bucket），并以链表或红黑树的形式存储。多个线程对 HashMap 的 put 操作会导致线程不安全，具体来说会有数据覆盖的风险。\n举个例子：\n两个线程 1,2 同时进行 put 操作，并且发生了哈希冲突（hash 函数计算出的插入下标是相同的）。 不同的线程可能在不同的时间片获得 CPU 执行的机会，当前线程 1 执行完哈希冲突判断后，由于时间片耗尽挂起。线程 2 先完成了插入操作。 随后，线程 1 获得时间片，由于之前已经进行过 hash 碰撞的判断，所有此时会直接进行插入，这就导致线程 2 插入的数据被线程 1 覆盖了。 public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { // ... // 判断是否出现 hash 碰撞 // (n - 1) \u0026amp; hash 确定元素存放在哪个桶中，桶为空，新生成结点放入桶中(此时，这个结点是放在数组中) if ((p = tab[i = (n - 1) \u0026amp; hash]) == null) tab[i] = newNode(hash, key, value, null); // 桶中已经存在元素（处理hash冲突） else { // ... } 还有一种情况是这两个线程同时 put 操作导致 size 的值不正确，进而导致数据覆盖的问题：\n线程 1 执行 if(++size \u0026gt; threshold) 判断时，假设获得 size 的值为 10，由于时间片耗尽挂起。 线程 2 也执行 if(++size \u0026gt; threshold) 判断，获得 size 的值也为 10，并将元素插入到该桶位中，并将 size 的值更新为 11。 随后，线程 1 获得时间片，它也将元素放入桶位中，并将 size 的值更新为 11。 线程 1、2 都执行了一次 put 操作，但是 size 的值只增加了 1，也就导致实际上只有一个元素被添加到了 HashMap 中。 public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { // ... // 实际大小大于阈值则扩容 if (++size \u0026gt; threshold) resize(); // 插入后回调 afterNodeInsertion(evict); return null; } 讲一下 HashMap 的 put 方法底层 X HashMap在 put 添加元素过程可以分为下面 9 个步骤：\n1.使用 put()方法时，直接调putVal()方法，计算 key 的 hash 值并将 hash 值、key、value 传入putVal方法 2.在 put 的时候先判断数组 table 是否为空，如果为空调用 resize 方法初始化该数组，长度为 16 3.根据键值 Key 计算 hash 值得出在数组中的索引，如果索引指定的位置为空（if）1、：则代表可以插入，直接插入一个新的 node，添加成功 4.索引不为空就说明发生了哈希冲突，所以要解决哈希冲突（else）2、：判断该位置数据的key是否和新添加数据的key一样 2.1、，如果一样则直接替换旧值 5.如果 key 不存在，则判断当前节点是否为红黑树 2.2、，如果是红黑树则直接在树中插入键值对。 6.如果也不是红黑树，就进入链表逻辑 2.3、 7.进入链表循环逻辑之后先判断 key 是否存在，如果存在就直接覆盖 value，不存在就在链表尾部插入新的节点。 8.插入后判断当前链表是否超过最大允许链表长度 8，如果超过则将链表转为红黑树，转为红黑树前要判断当前数组长度是否大于 64，如果不是要先进行数组扩容。 9.插入成功后进行判断 3、，判断实际存在的键值对数量 size 是否超过了阈值（数组长度*0.75），如果超过，则进行数组扩容。 HashMap 的扩容机制 在添加元素或初始化的时候需要调用 resize 方法进行扩容，第一次添加数据初始化数组长度为 16，以后每次每次扩容都是达到了扩容阈值（数组长度 * 0.75）\n每次扩容的时候，都是扩容之前容量的 2 倍；\n扩容之后，会新创建一个数组，需要把老数组中的数据挪动到新的数组中\n没有 hash 冲突的节点，则直接使用 e.hash \u0026amp; (newCap - 1) 计算新数组的索引位置\n如果是红黑树，走红黑树的添加\n如果是链表，则需要遍历链表，可能需要拆分链表，判断(e.hash \u0026amp; oldCap)是否为 0，该元素的位置要么停留在原始位置，要么移动到原始位置+增加的数组大小这个位置上\nHashMap 的扩容机制是为了保持哈希表中的负载因子（load factor）在一个可接受的范围内，以提供更好的性能和空间利用率。负载因子是哈希表中已存储元素数量与哈希表容量的比值，当负载因子过高时，哈希冲突的概率增加，可能导致性能下降。\n当 HashMap 中的元素数量达到一定阈值时，就会触发扩容。扩容的过程包括以下步骤：\n创建新的数组： 首先，HashMap 会创建一个新的更大容量的数组，通常是当前容量的两倍。 重新哈希： 然后，HashMap 会将已存储在旧数组中的元素重新哈希到新的数组中。这是因为数组的大小发生了变化，原来的哈希函数计算出的索引位置可能不再有效，需要重新计算。 迁移元素： 在重新哈希的过程中，HashMap 将元素从旧数组移到新数组中。这可能涉及到复制和重新计算哈希值。 更新容量和阈值： 扩容完成后，HashMap 会更新其内部的容量和阈值。新的容量用于计算负载因子，新的阈值用于触发下一次扩容。 需要注意的是，HashMap 的扩容过程可能会带来一些性能开销，因为在重新哈希和迁移元素的过程中需要进行大量的数据操作。因此，在选择初始容量和负载因子时，需要权衡内存利用率和性能。默认情况下，HashMap 的初始容量为 16，负载因子为 0.75。\nHashMap 源码的 hash 方法有了解吗? static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } 哈希函数：将键（key）映射为数组下标的函数叫做散列函数，即哈希函数\nhash 方法主要是对 key 的 hashcode 进行处理，得到最终的哈希值，增加哈希值的随机性，减少了哈希冲突的概率，也称为扰动运算\n具体是将原哈希值右移 16 位之后再与原哈希值进行异或运算，这样就把原哈希值的高位和低位混合，增加哈希值的随机性。可以使得哈希值更加散列，减少哈希冲突的概率。\n最终得到的哈希值也就是这个 key 要放入的 hashmap 桶的位置\n解决散列冲突-拉链法\n如果通过计算两个 key 的 hash 值相同，即多个 key 映射到同一数组下标位置。\n数组的每个位置称之为桶（bucket）或者槽（slot），每个桶会对应一条链表，hash 冲突后的元素都放在相同槽位对应的链表中或红黑树中。\nHashMap 的寻址算法？为何 hashmap 的数组长度一定是 2 的次幂？ static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } 寻址算法：\n首先计算对象的 hashcode 值 调用 hash 方法进行二次哈希，hashcode 值右移 16 位再异或运算，让哈希分布更为均匀 通过(capacity - 1) \u0026amp; hash得到索引 hashmap 的数组长度一定是 2 的次幂好处：\n计算索引时效率更高，如果是 2 的 n 次幂可以使用位运算代替取模运算 扩容时重新计算索引效率更高：hash \u0026amp; oldCap == 0的元素留在原来位置，否则新位置 = 旧位置 + oldCap 怎么处理 hash 碰撞的，添加两个相同的 key 是怎么个流程? **当两个不同的键（key）计算得到相同的哈希值，导致它们应该放在数组的同一个位置上时，就发生了哈希碰撞。**为了解决哈希碰撞，HashMap 采用了链表和红黑树的方式。\n在 Java 8 及以后版本的 HashMap 中，当哈希碰撞发生时，采用链表和红黑树结合的方式来解决：\n链表：当哈希碰撞发生时，新的键值对会被插入到该桶对应的链表的末尾。 红黑树：当一个桶上的链表长度达到一定阈值（默认为 8），链表会被转换为红黑树，但在转换之前会先判断数组长度是否已经大于 64 了，如果未大于 64 要先进行数组扩容。 红黑树是一种自平衡的二叉查找树，可以提高查找、插入和删除的性能。 添加两个相同的键（key）的流程如下：\n计算两个键的哈希值，并根据哈希值找到对应的桶（bucket）位置。\n如果该桶上没有元素，直接插入新的键值对到该位置。\n如果该桶上已经有元素，首先通过equals()方法比较两个键是否相等。如果两个键相等（即键的内容相同），则将新的值覆盖原来的值。\n如果两个键不相等，说明发生了哈希碰撞。此时，如果桶上的元素个数还没有达到链表转换为红黑树的阈值（默认为 64），则将新的键值对插入到链表的末尾。\n如果桶上的元素个数已经达到链表转换为红黑树的阈值，那么链表将会被转换为红黑树，并再次插入新的键值对。\nConcurrentHashMap 和 Hashtable 的区别 ConcurrentHashMap 和 Hashtable 的区别主要体现在实现线程安全的方式上不同。\n底层数据结构： JDK1.7 的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟 HashMap1.8 的结构一样，数组+链表/红黑二叉树。Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的； 实现线程安全的方式（重要）： 在 JDK1.7 的时候，ConcurrentHashMap 对整个桶数组进行了分割分段(Segment，分段锁)，每一把锁只锁容器其中一部分数据（下面有示意图），多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 到了 JDK1.8 的时候，ConcurrentHashMap 已经摒弃了 Segment 的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6 以后 synchronized 锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在 JDK1.8 中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本； Hashtable(同一把锁) :使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。 下面，我们再来看看两者底层数据结构的对比图。\nHashTable：\nJava1.7 的 ConcurrentHashMap：\nConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成。\nSegment 数组中的每个元素包含一个 HashEntry 数组，每个 HashEntry 数组属于链表结构。\nJava1.8 的 ConcurrentHashMap：\nJDK1.8 的 ConcurrentHashMap 不再是 Segment 数组 + HashEntry 数组 + 链表，而是 Node 数组 + 链表 / 红黑树。不过，Node 只能用于链表的情况，红黑树的情况需要使用 TreeNode。当冲突链表达到一定长度时，链表会转换成红黑树。\nTreeNode是存储红黑树节点，被TreeBin包装。TreeBin通过root属性维护红黑树的根结点，因为红黑树在旋转的时候，根结点可能会被它原来的子节点替换掉，在这个时间点，如果有其他线程要写这棵红黑树就会发生线程不安全问题，所以在 ConcurrentHashMap 中TreeBin通过waiter属性维护当前使用这棵红黑树的线程，来防止其他线程的进入。\nstatic final class TreeBin\u0026lt;K,V\u0026gt; extends Node\u0026lt;K,V\u0026gt; { TreeNode\u0026lt;K,V\u0026gt; root; volatile TreeNode\u0026lt;K,V\u0026gt; first; volatile Thread waiter; volatile int lockState; // values for lockState static final int WRITER = 1; // set while holding write lock static final int WAITER = 2; // set when waiting for write lock static final int READER = 4; // increment value for setting read lock ... } ConcurrentHashMap 线程安全的具体实现方式/底层具体实现 JDK1.8 以前：\n首先将数据分为一段一段（这个“段”就是 Segment）的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。\nConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成。\nSegment 继承了 ReentrantLock,所以 Segment 是一种可重入锁，扮演锁的角色。HashEntry 用于存储键值对数据。\n一个 ConcurrentHashMap 里包含一个 Segment 数组，Segment 的个数一旦初始化就不能改变。 Segment 数组的大小默认是 16，也就是说默认可以同时支持 16 个线程并发写。\nSegment 的结构和 HashMap 类似，是一种数组和链表结构，一个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素，每个 Segment 守护着一个 HashEntry 数组里的元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment 的锁。也就是说，对同一 Segment 的并发写入会被阻塞，不同 Segment 的写入是可以并发执行的。\nJDK1.8 以后：\nJava 8 几乎完全重写了 ConcurrentHashMap，代码量从原来 Java 7 中的 1000 多行，变成了现在的 6000 多行。\nConcurrentHashMap 取消了 Segment 分段锁，采用 Node + CAS + synchronized 来保证并发安全。数据结构跟 HashMap 1.8 的结构类似，数组+链表/红黑二叉树。Java 8 在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为 O(N)）转换为红黑树（寻址时间复杂度为 O(log(N))）。\nJava 8 中，锁粒度更细，synchronized 只锁定当前链表或红黑二叉树的首节点，这样只要 hash 不冲突，就不会产生并发，就不会影响其他 Node 的读写，效率大幅提升\nJDK 1.7 和 JDK 1.8 的 ConcurrentHashMap 实现有什么不同？\n线程安全实现方式：JDK 1.7 采用 Segment 分段锁来保证安全， Segment 是继承自 ReentrantLock。JDK1.8 放弃了 Segment 分段锁的设计，采用 Node + CAS + synchronized 保证线程安全，锁粒度更细，synchronized 只锁定当前链表或红黑二叉树的首节点。 Hash 碰撞解决方法 : JDK 1.7 采用拉链法，JDK1.8 采用拉链法结合红黑树（链表长度超过一定阈值时，将链表转换为红黑树）。 并发度：JDK 1.7 最大并发度是 Segment 的个数，默认是 16。JDK 1.8 最大并发度是 Node 数组的大小，并发度更大 如何选择 Map？ 是否需要按照键的自然顺序或者自定义顺序进行排序。如果需要按照键排序，则可以使用 TreeMap；如果不需要排序，则可以使用 HashMap 或 LinkedHashMap。 是否需要保持插入顺序。如果需要保持插入顺序，则可以使用 LinkedHashMap；如果不需要保持插入顺序，则可以使用 TreeMap 或 HashMap。 是否需要高效的查找。如果需要高效的查找，则可以使用 LinkedHashMap 或 HashMap，因为它们的查找操作的时间复杂度为 O(1)，而是 TreeMap 是 O(log n)。 LinkedHashMap 内部使用哈希表来存储键值对，并使用一个双向链表来维护插入顺序，但查找操作只需要在哈希表中进行，与链表无关，所以时间复杂度为 O(1)\nArrayDeque\nArrayDeque 双端队列，Deque 的实现类。\n在 Java 中如果要用到栈，就使用 ArrayDeque Java 中 Deque 的实现有 ArrayDeque 和 LinkedList 当需要实现先进先出(FIFO)或者先进后出(LIFO)的数据结构时，可以考虑使用 ArrayDeque。以下是一些使用 ArrayDeque 的场景：\n管理任务队列：如果需要实现一个任务队列，可以使用 ArrayDeque 来存储任务元素。在队列头部添加新任务元素，从队列尾部取出任务进行处理，可以保证任务按照先进先出的顺序执行。 实现栈： ArrayDeque 可以作为栈的实现方式，支持 push、 pop、 peek 等操作，可以用于需要后进先出的场景。 实现缓存：在需要缓存一定数量的数据时，可以使用 ArrayDeque。当缓存的数据量超过容量时，可以从队列头部删除最老的数据，从队列尾部添加新的数据。 实现事件处理器： ArrayDeque 可以作为事件处理器的实现方式，支持从队列头部获取事件进行处理，从队列尾部添加新的事件。 什么是红黑树，特点是什么？ 红黑树（Red Black Tree）是一种特化的 AVL 树（平衡二叉树），都是在进行插入和删除操作时通过特定操作保持二叉查找树的平衡，从而获得较高的查找性能。\n红黑树的特点有 5 个：\n结点是红色或黑色。 根结点是黑色。 所有叶子都是黑色（叶子是 NIL 结点） 每个红色结点的两个子结点都是黑色。（从每个叶子到根的所有路径上不能有两个连续的红色结点） 从任一结点到其每个叶子的所有路径都包含相同数目的黑色结点。 Collection 和 Collections 有什么区别？ Collection 和 Collections 是 Java 集合框架中的两个概念，它们有以下区别：\nCollection 是 Java 集合框架中的一个接口，它是所有集合类的父接口。Collection 接口定义了基本的集合操作，如添加、删除、查询元素等。它是一种通用的集合管理接口，可以用于操作各种不同类型的集合对象。\nCollections 是 java.util 包下的一个工具类，其中包含一组静态方法，用于操作集合对象。它提供了一些常用的方法，如排序、搜索、填充、反转等，用于对集合进行操作和处理。Collections 类中的方法是通过静态调用来实现的。⽐如提供的排序⽅法： Collections.sort(list);提供的反转⽅法： Collections.reverse(list)\n综上所述，**Collection 是一个接口，代表一组对象的集合，而 Collections 是一个工具类，提供了对集合进行操作和处理的一些常用方法。**它们的关系是 Collection 接口是集合框架中的一部分，而 Collections 是一个工具类，用于对集合进行操作和处理。\n","permalink":"https://lidengxm.github.io/posts/java/java%E5%9F%BA%E7%A1%80%E5%85%AB%E8%82%A1/","summary":"Java 基础高频考点 抽象类和接口的区别？应用场景，怎么理解的 重载与重写的区别 HashMap 扩容原理、底层数据结构 OOP 是什么？ Java 语言基础 Java 语言的特点 ⾯向对象（封装，继承，多态） 平台⽆关性，平台⽆关性的具体表现在于，J a v a 是“⼀次编写，到处运⾏（Wr i t e O n c e，R u n a n y W h e r e）”的语⾔，因","title":"Java基础八股"},{"content":"写在前面 1、一条 SQL 是如何执行的？也就是说，从MySQL 客户端执行了一条 SQL 语句，MySQL 服务端会进行哪些处理。\n2、索引相关：索引是如何实现的？MySQL 的索引采用的哪种数据结构？哈希索引和 B+ 树索引的区别是什么？\n3、事务相关：事务的四大特性是什么？什么是幻读、脏读、不可重复读？、MVCC 了解吗？怎么实现的？\n4、锁相关：表锁、行锁、意向锁、乐观锁、死锁，这些锁主要是用来解决什么问题的？\n5、日志相关：MySQL 日志文件有哪些？binlog 和 redo log 有什么区别？redo log 是怎么刷入磁盘的呢？\n6、高可用/性能相关：数据库读写分离了解吗？读写分离是如何实现的？主从复制的原理了解吗？分库分表了解吗？\n7、SQL 优化相关：慢 SQL 如何定位？如何优化慢 SQL？\n基础 请解释数据库中的连接（join）操作，并介绍不同类型的连接。 在数据库中，连接（join）操作是用于将两个或多个表中的行组合在一起，基于它们之间的某个共同列或条件进行关联。连接操作允许在查询中获取来自多个表的相关数据。\n常见的连接类型包括：\ninner join 内连接，在两张表进行连接查询时，只保留两张表中完全匹配的结果集\nleft join 在两张表进行连接查询时，会返回左表所有的行，即使在右表中没有匹配的记录。\nright join 在两张表进行连接查询时，会返回右表所有的行，即使在左表中没有匹配的记录。\n**交叉连接（cross join）：**显示两张表所有记录一一对应，没有匹配关系进行筛选，它是笛卡尔积在SQL中的实现，如果A表有m行，B表有n行，那么A和B交叉连接的结果就有m*n行。\n**笛卡尔积：**返回两个集合组合的所有情况。是数学中的一个概念，例如集合A={a,b}，集合B={1,2,3}，那么A✖️B={\u0026lt;a,o\u0026gt;,\u0026lt;a,1\u0026gt;,\u0026lt;a,2\u0026gt;,\u0026lt;b,0\u0026gt;,\u0026lt;b,1\u0026gt;,\u0026lt;b,2\u0026gt;,}。\n**内连接（Inner Join）：**返回两个表中相匹配的行。只有在连接列上有匹配的值时，才返回结果。内连接可以使用JOIN关键字来实现。\n例如：返回满足连接条件的table1和table2中的匹配行。 SELECT column1, column2, ... FROM table1 INNER JOIN table2 ON table1.column_name = table2.column_name; **左连接（Left Join）：**左连接返回左表中的所有行，以及与右表中匹配的行。如果右表中没有匹配的行，则返回NULL值。左连接可以通过使用LEFT JOIN或LEFT OUTER JOIN关键字来实现。\n例如：返回左表table1中的所有行和与之匹配的右表table2中的行 SELECT column1, column2, ... FROM table1 LEFT JOIN table2 ON table1.column_name = table2.column_name; 右连接（Right Join）：右连接返回右表中的所有行，以及与左表中匹配的行。如果左表中没有匹配的行，则返回NULL值。右连接可以通过使用RIGHT JOIN或RIGHT OUTER JOIN关键字来实现。\n例如：返回右表table2中的所有行和与之匹配的左表table1中的行 SELECT column1, column2, ... FROM table1 RIGHT JOIN table2 ON table1.column_name = table2.column_name; varchar与char的区别？ char：\nchar表示定长字符串，长度是固定的； 如果插入数据的长度小于char的固定长度时，则用空格填充； 因为长度固定，所以存取速度要比varchar快很多，甚至能快50%，但正因为其长度固定，所以会占据多余的空间，是空间换时间的做法； 对于char来说，最多能存放的字符个数为255，和编码无关 varchar：\nvarchar表示可变长字符串，长度是可变的； 插入的数据是多长，就按照多长来存储； varchar在存取方面与char相反，它存取慢，因为长度不固定，但正因如此，不占据多余的空间，是时间换空间的做法； 对于varchar来说，最多能存放的字符个数为65532 drop、delete 与 truncate 的区别？ 三者都表示删除，但是三者有一些差别：\ndelete truncate drop 类型 属于DML 属于DDL 属于DDL 回滚 可回滚 不可回滚 不可回滚 删除内容 表结构还在，删除表的全部或者一部分数据行 表结构还在，删除表中的所有数据 从数据库中删除表，所有数据行，索引和权限也会被删除 删除速度 删除速度慢，需要逐行删除 删除速度快 删除速度最快 因此，在不再需要一张表的时候，用drop；在想删除部分数据行时候，用delete；在保留表而删除所有数据的时候用truncate。\ndelete from user where id = 1; //删除某一行数据 TRUNCATE TABLE Orders; //删除整个表的数据 drop table user; //删除表 tinyint(1)和 tinyint(4) 有什么区别 在MySQL中，tinyint(n)是用于定义整数类型的列，其中n表示字段的显示宽度。对于tinyint类型，存储的范围是从-128到127。\ntinyint(1)和tinyint(4)在存储范围和数据类型上并没有区别。它们的含义主要体现在显示的宽度上。\ntinyint(1)：表示以宽度1来显示该字段的值。这并不表示该字段只能存储1位的数据，仍然可以存储完整的tinyint类型范围内的整数值，只是在显示时，该字段的宽度为1。 tinyint(4)：表示以宽度4来显示该字段的值。同样，它也可以存储完整的tinyint类型范围内的整数值，只是在显示时，该字段的宽度为4。 在实际存储和处理数据时，tinyint(1)和tinyint(4)是没有区别的。它们只是在显示上的差异。\n需要注意的是，虽然tinyint(n)可以指定显示宽度，但这并不影响存储和运算的大小和范围，因为tinyint的存储大小在MySQL中是固定的。显示宽度主要用于控制在查询结果中显示的列宽，并非用于限制存储的整数范围。\nUNION与UNION ALL的区别？ 联合查询，即使用union关键字对多次查询的结果进行合并，形成一个新的查询结果\nunion 和 union all的注意事项：\nunion all 是直接合并，union是合并后对数据进行去重 对于联合查询的多张表的列数必须保持一致，字段类型也需要保持一致 效率 UNION 高于 UNION ALL count(1)、count(*) 与 count(列名) 的区别？ 执行效果：\ncount(*)包括了所有的列，相当于行数，在统计结果的时候，不会忽略列值为 NULL count(1)包括了忽略所有列，用 1 代表代码行，在统计结果的时候，不会忽略列值为 NULL count(列名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是只空字符串或者 0，而是表示 null）的计数，即某个字段值为 NULL 时，不统计。 执行速度：\n列名为主键，count(列名)会比 count(1)快 列名不为主键，count(1)会比 count(列名)快 如果表多个列并且没有主键，则 count（1） 的执行效率优于 count（*） 如果有主键，则 select count（主键）的执行效率是最优的 如果表只有一个字段，则 select count（*）最优 count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是NULL，累计值就加 1，否则不加，最后返回累计值。\n用法：count（*）、count（主键）、count（字段）、count（数字）\n按照效率排序的话，count(字段) \u0026lt; count(主键 id) \u0026lt; count(1) ≈ count(*)，所以尽量使用 count(*)\nSQL语句中的函数有哪些 字符串函数：\nconcat 拼接字符串 substring 截取字符串 length 求字符串长度 upper 字符串转为大写 lower 字符串转为小写 trim 去除字符串首尾的空格 数值函数\nround 四舍五入到指定位数的数字 abs 返回绝对值 聚合函数\ncount 求行数 avg 求平均值 sum 求和 max min 数据库中整数转为字符串的函数：\ncast convert SELECT CAST(column_name AS CHAR) AS result FROM table_name; SELECT CONVERT(column_name, CHAR) AS result FROM table_name; DQL语句的执行顺序 基本查询-\u0026gt;条件查询-\u0026gt;聚合函数-\u0026gt;分组查询-\u0026gt;排序查询-\u0026gt;分页查询 MySQL的基础架构 MySQL 逻辑架构图主要分三层：\n客户端：最上层的服务并不是 MySQL 所独有的，大多数基于网络的客户端/服务器的工具或者服务都有类似的架构。比如连接处理、授权认证、安全等等。 Server 层：大多数 MySQL 的核心服务功能都在这一层，包括查询解析、分析、优化、缓存以及所有的内置函数（例如，日期、时间、数学和加密函数），所有跨存储引擎的功能都在这一层实现：存储过程、触发器、视图等。 存储引擎层：第三层包含了存储引擎。存储引擎负责 MySQL 中数据的存储和提取。Server 层通过 API 与存储引擎进行通信。这些接口屏蔽了不同存储引擎之间的差异，使得这些差异对上层的查询过程透明 连接层 最上层是一些客户端和链接服务，包含本地sock 通信和大多数基于客户端/服务端工具实现的类似于TCP/IP的通信。**主要完成一些类似于连接处理、授权认证、及相关的安全方案。**在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。\n服务层 第二层架构主要完成大多数的核心服务功能，**如SQL接口，并完成缓存的查询，SQL的分析和优化，部分内置函数的执行。**所有跨存储引擎的功能也在这一层实现，如 过程、函数等。在该层，服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化如确定表的查询的顺序，是否利用索引等，最后生成相应的执行操作。如果是select语句，服务器还会查询内部的缓存，如果缓存空间足够大，这样在解决大量读操作的环境中能够很好的提升系统的性能。\n引擎层 存储引擎层， 存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API和存储引擎进行通信。不同的存储引擎具有不同的功能，这样我们可以根据自己的需要，来选取合适的存储引擎。数据库中的索引是在存储引擎层实现的。索引是在引擎层的\n存储层 数据存储层， 主要是将数据(如: redolog、undolog、数据、索引、二进制日志、错误日志、查询日志、慢查询日志等)存储在文件系统之上，并完成与存储引擎的交互\n一条SQL查询语句在MySQL中是如何执行的？ X 先检查该语句是否有权限，如果没有权限，直接返回错误信息，如果有权限会先查询缓存 (MySQL8.0 版本以前)。 如果没有缓存，分析器进行语法分析，提取 sql 语句中 select 等关键元素，然后判断 sql 语句是否有语法错误，比如关键词是否正确等等。 语法解析之后，MySQL的服务器会对查询的语句进行优化，确定执行的方案。 完成查询优化后，按照生成的执行计划调用数据库引擎接口，返回执行结果。 小林coding的图，执行一条SQL查询语句的流程\n连接器，与MySQL建立连接，管理连接，校验用户身份\n查询缓存，查询SQL语句会先去查询缓存，如果有缓存就直接返回，没有的话就查询数据库，查询结果会被存入缓存中。（MySQL8.0开始查询缓存就被删除了）\n解析SQL，在进行SQL查询之前，MySQL会先对SQL语句进行解析，由【解析器】完成\n**词法分析，**MySQL 会根据你输入的字符串识别出关键字出来，构建出 SQL 语法树，这样方便后面模块获取 SQL 类型、表名、字段名、 where 条件等等。 **语法分析，**根据词法分析的结果，语法解析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。如果输入的SQL语句语法不对，就会在解析器这个阶段报错 执行SQL，经解析器后就要进行SQL的查询流程了，每个查询SQL分为这三个阶段：prepare阶段（预处理）、optimize阶段（优化）、execute阶段（执行）\nprepare预处理阶段：\n检查 SQL 查询语句中的表或者字段是否存在；\n将 select * 中的 * 符号，扩展为表上的所有列；\noptimize优化阶段：预处理阶段后，还要为SQL查询语句指定一个执行计划，由优化器完成。优化器主要负责将 SQL 查询语句的执行方案确定下来，比如在表里面有多个索引的时候，优化器会基于查询成本的考虑，来决定选择使用哪个索引。\nexecute执行阶段：根据执行计划执行SQL查询语句，由执行器完成。从存储引擎读取记录，返回给客户端\n什么是主键和外键？它们之间有什么关系？如何在创建表时定义主键和外键？ **主键（Primary Key）是用于唯一标识表中每一行数据的字段或字段组合。**它的特点是唯一性和非空性。在一个表中，只能定义一个主键。\n**外键（Foreign Key）是用于建立表与表之间关联关系的字段。**它是一个指向另一个表的主键的字段，用于确保数据的完整性和一致性。\n主键和外键之间存在一种关系：\n主键是用于唯一标识表中每一行数据的字段，确保数据的唯一性。 外键是用于建立表与表之间的关联关系的字段，指向其他表的主键，建立表与表之间的引用关系。 在创建表时定义主键和外键可以使用以下语法：\n定义主键：\nCREATE TABLE table_name ( column1 data_type PRIMARY KEY, column2 data_type, ... ); 其中，column1是要定义为主键的字段，data_type是字段的数据类型。通过将字段定义为主键，该字段将具有唯一性和非空性。\n定义外键：\nCREATE TABLE table_name ( column1 data_type, column2 data_type, ... FOREIGN KEY (column1) REFERENCES referenced_table (referenced_column) ); 其中，column1是要定义为外键的字段，referenced_table是被引用的表名，referenced_column是被引用表中的主键列名。通过这种方式，定义的外键字段将与被引用表中的主键建立关联关系。\n需要注意的是，定义外键时要确保被引用的表已经存在，并且被引用的主键列需要有索引。在MySQL中，使用InnoDB存储引擎才能支持外键的定义和引用关系的维护。\n通过定义主键和外键，可以在数据库中建立表与表之间的关联关系，确保数据的完整性和一致性。\n数据库三大范式及优缺点？ 数据库的范式是用来规范化数据库表结构的一组原则或规则。它定义了关系型数据库中表的设计规范，目的是消除数据冗余、提高数据一致性和完整性。\n第一范式： 数据表中的每一列（每个字段）都不可以再拆分。每一列都应该具有原子性数据，避免多值属性和重复属性。 1NF主要关注的是表的结构是否满足最基本的规范，确保每个属性都是原子性的。 第二范式： 在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。如果存在部分依赖，需要将非主键列分离出来形成新表。 2NF的目的是消除表中的部分依赖，确保数据的完整性和一致性 第三范式： **在满足第二范式的基础上，要求非主键列之间不存在传递依赖关系。**如果存在传递依赖，需要将非主键列进一步分离形成新表。 3NF的目的是消除表中的传递依赖，减少数据冗余和提高数据的一致性。 三大范式的作用是为了控制数据库的冗余，是对空间的节省，实际上，一般互联网公司的设计都是反范式的，通过冗余一些数据，避免跨表跨库，利用空间换时间，提高性能。\n总结来说，**第一范式要求每个属性都是原子的，第二范式要求非主键字段完全依赖于整个主键，第三范式要求消除非主键字段之间的传递依赖关系。**通过遵循这些范式，可以规范化设计数据库结构，提高数据的一致性和可靠性。\n优点：\n数据冗余减少：范式化的数据库表结构减少了数据冗余，避免了多次存储相同的数据，节省了存储空间。 数据一致性提高：通过范式化的设计，可以保持数据的一致性和完整性，减少了数据更新异常的风险。 查询性能优化：在一些情况下，通过合理使用索引和查询优化可以提高查询性能。 缺点：\n多表关联：范式化的数据库设计可能需要进行多表关联查询，增加了查询的复杂性和性能开销。 数据库更新开销增加：在插入、更新和删除数据时，由于范式化的设计需要操作多个表，可能导致性能开销增加。 适应复杂查询的复杂性：对于一些复杂查询，范式化的设计可能需要更多的表关联和查询操作，使查询语句更复杂。 需要根据具体的应用需求和数据特点来选择合适的范式化级别。在设计数据库表结构时，可以根据实际情况进行范式化和反范式化的权衡，以满足应用的性能和灵活性需求。\n存储引擎 MySQL 的存储引擎有哪些？各自的特点和适用场景是什么？ MySQL的存储引擎是指MySQL用于存储和管理数据的内部组件。MySQL支持多种存储引擎，每个存储引擎都有其独特的特点和适用场景。以下是MySQL的一些常见存储引擎及其特点和适用场景：\nInnoDB： 特点：MySQL5.5以后默认的存储引擎，**支持ACID事务，支持行级锁，支持外键约束；**提供行级锁定和多版本并发控制（MVCC），具有高度的数据完整性和可靠性。 适用场景：如果应用对事务的完整性有比较高的要求，在并发条件下要求数据的一致性，数据操作除了插入和查询之外，还包含很多的更新、删除操作，如电子商务、银行系统等。 MyISAM： 特点：不支持事务和行级锁定，不支持外键，但具有快速读取和插入的特性，支持全文索引和压缩表。 适用场景：如果应用是以读操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高，适合读取密集型的应用，如新闻网站、博客等。适用于非事务性和不需要高度数据完整性的场景。 Memory（也称为Heap）： 特点：将数据存储在内存中，提供非常快速的读写性能，不支持持久性存储。支持Hash索引 适用场景：通常用于临时表及缓存和高速数据处理等场景，不适用于需要持久性存储的应用。 需要注意的是，不同存储引擎的功能和性能特点有所不同。在选择存储引擎时，需要根据应用程序的需求和性能要求来评估和选择合适的存储引擎。有时候，一个应用程序中可以使用多个存储引擎来处理不同类型的数据或实现不同的功能。\n存储引擎是什么？有哪些分类？ 存储引擎，是存储数据、建立索引、更新/查询数据等技术的实现方式 。存储引擎是基于表的，而不是\n基于库的，所以存储引擎也可被称为表类型。在创建表的时候，可以来指定选择的存储引擎，如果没有指定将自动\n选择默认的存储引擎。\n-- 查看当前数据库支持的存储引擎 show engines; -- 创建表my_memory，指定存储引擎为memory create table my_memory( id int, name varchar(10) ) engine = memory; 存储引擎主要有三个：\nInnoDB，MySQL5.5之后的默认存储引擎，支持事务，行级锁，外键约束 MEMORY，数据存储在内存中的 MYISAM，支持表级锁 InnoDB存储引擎，MySQL5.5之后的默认存储引擎：\nDML操作遵循ACID特性，支持事务 支持行级锁，提高并发访问性能 支持外键FOREIGN KEY约束，保证数据的完整性和正确性 MYISAM存储引擎：\n不支持外键，不支持事务 支持表锁，不支持行锁 访问速度快 MEMORY存储引擎：\na. 内存存放 b. 支持Hash索引 InnoDB 和 MyISAM存储引擎的区别是什么？ InnoDB和MyISAM是MySQL数据库中两种常见的存储引擎，它们具有以下区别：\n**事务支持：**InnoDB是一个支持事务的存储引擎，它遵循ACID（原子性、一致性、隔离性和持久性）属性。它提供了提交（commit）和回滚（rollback）事务的能力，适用于需要数据一致性和事务处理的应用场景。而MyISAM不支持事务，主要适用于读操作较多、写操作较少的场景。 并发性能：****InnoDB采用了行级锁（row-level locking）来支持并发操作，多个事务可以同时读取和写入不同的行，提供了更好的并发性能。而MyISAM采用了表级锁（table-level locking），当有一个事务在对表进行写操作时，其他事务无法读取和写入该表，这可能导致并发性能的下降。 **外键约束：**InnoDB存储引擎支持外键，而MYISAM不支持 索引类型：MyISAM 的索引为非聚簇索引，数据结构是 B 树；InnoDB 的索引是聚簇索引，数据结构是 B+树。 数据安全性：InnoDB对数据的持久性有较好的支持，它将数据存储在磁盘上的日志文件（redo log）中，并定期进行刷新。在故障恢复的情况下，InnoDB可以使用日志文件来恢复数据的一致性。而MyISAM没有提供像InnoDB那样的持久性保证，当系统崩溃时，可能会导致数据丢失或损坏。 全文索引和空间数据索引：MyISAM存储引擎支持全文索引，可以进行全文搜索，而InnoDB在早期版本中不支持全文索引，但在MySQL 5.6版本之后，InnoDB也开始支持全文索引。此外，MyISAM还支持空间数据索引，用于处理地理空间数据。 简单来说：\n事务：InnoDB存储引擎支持事务，而MYISAM不支持 锁：InnoDB存储引擎支持行锁和表锁，而MYISAM只支持表锁 外键约束：InnoDB存储引擎支持外键，而MYISAM不支持 索引类型：MyISAM 的索引为非聚簇索引，数据结构是 B 树；InnoDB 的索引是聚簇索引，数据结构是 B+树。 如何选择存储引擎 绝大部分都是使用的InnoDB存储引擎，需要全文搜索和检索时可以选择MYISAM，需要使用临时表或者缓存时选择MEMORY\n存储引擎的选择：\nInnoDB: 是Mysql的默认存储引擎，支持事务、外键。如果应用对事务的完整性有比较高的要求，在并发条件下要求数据的一致性，数据操作除了插入和查询之外，还包含很多的更新、删除操作，那么InnoDB存储引擎是比较合适的选择。 MyISAM ： 如果应用是以读操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高，那么选择这个存储引擎是非常合适的。（或者需要全文搜索和检索时） MEMORY：将所有数据保存在内存中，访问速度快，通常用于临时表及缓存。MEMORY的缺陷就是对表的大小有限制，太大的表无法缓存在内存中，而且无法保障数据的安全性。 需要根据具体的应用场景来选择合适的存储引擎。\n如果你的应用需要支持事务、高并发性能和数据完整性，建议选择InnoDB。 如果应用主要进行读操作且对全文搜索有需求，可以考虑使用MyISAM。 如果对速度要求较高且数据不需要持久化，可以使用Memory存储引擎。 另外，还可以根据具体需求选择不同存储引擎的混合使用。一张表可以使用一个存储引擎\n事务 什么是 ACID？如何保证？ 什么是事务的 ACID 特性？请逐个解释。\n事务的特性有哪些？\n什么是数据库事务？请解释ACID属性。\n原子性（Atomicity）：一个事务中的所有操作，要么全部完成，要么全部失败，不会结束在中间某个环节，而且事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样，就好比买一件商品，购买成功时，则给商家付了钱，商品到手；购买失败时，则商品在商家手中，消费者的钱也没花出去。\n保证方式：通过日志记录和回滚操作来实现。在事务执行过程中，undo log日志中记录着操作的逆操作，以便在回滚时能够通过重放undo log就可以回滚事务。 一致性（Consistency）：**是指事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态。**比如，用户 A 和用户 B 在银行分别有 800 元和 600 元，总共 1400 元，用户 A 给用户 B 转账 200 元，分为两个步骤，从 A 的账户扣除 200 元和对 B 的账户增加 200 元。一致性就是要求上述步骤操作后，最后的结果是用户 A 还有 600 元，用户 B 有 800 元，总共 1400 元，而不会出现用户 A 扣除了 200 元，但用户 B 未增加的情况（该情况，用户 A 和 B 均为 600 元，总共 1200 元）。\n保证方式：reod log日志中记录着所有操作记录，当出现意外情况时就会重放redo log日志回复数据 隔离性（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，**隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，**因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的。也就是说，消费者购买商品这个事务，是不影响其他消费者购买的。\n保证方式：通过并发控制机制（如锁、MVCC等）来隔离事务。 持久性（Durability）：事务一旦提交之后，对数据的修改就是永久的，即便系统故障也不会丢失。\n也是通过redo log日志保证的，在事务提交之后，redo log会将记录写入磁盘 什么是事务？如何保证数据库事务的一致性和完整性？ 事务一组原子操作的集合，这一组操作集合要么全部执行成功，要么全部执行失败\n事务具有以下特性（通常被称为ACID属性）：\n原子性（Atomicity）：事务中的所有操作要么全部成功执行，要么全部回滚，不允许部分执行部分回滚。 一致性（Consistency）：事务执行前后数据库处于一致的状态。事务中的操作必须遵循预定义的规则和约束，以保持数据的完整性。 隔离性（Isolation）：事务的执行应该与其他并发事务相互隔离，使每个事务感觉到它在独立地操作数据，即使多个事务并发执行。 持久性（Durability）：一旦事务提交成功，其结果将永久保存在数据库中，即使系统发生故障，也能够恢复到提交事务后的状态。 为了保证数据库事务的一致性和完整性，可以采取以下措施：\n使用事务：将相关的数据库操作包装在一个事务中，以确保这些操作作为一个原子单元执行。在事务中，如果任何操作失败，可以回滚所有已经执行的操作，保持数据的一致性。\n使用数据库约束：通过在数据库中定义约束（如主键约束、唯一约束、外键约束、检查约束等），可以强制执行数据的完整性。这些约束可以防止无效的数据插入、更新或删除，保持数据的一致性。\n锁机制：数据库使用锁来保证事务的隔离性。锁可以防止并发事务之间的数据冲突，保持数据的一致性和完整性。\n日志和回滚：数据库系统会记录事务的操作日志，包括事务开始、操作内容和事务提交或回滚等信息。这样，在系统发生故障时，可以使用日志来恢复事务，并保持数据的一致性。\n事务隔离级别：数据库提供多个事务隔离级别，如读未提交、读已提交、可重复读和串行化。通过选择适当的隔离级别，可以控制事务之间的数据可见性和并发操作的行为，以满足一致性和隔离性的需求。\n综上所述，通过使用事务、约束、锁机制、日志和回滚以及适当的隔离级别，可以确保数据库事务的一致性和完整性。这些措施可以保证多个操作作为一个原子单元执行，并确保数据的一致性和可靠性。\n什么是事务？MySQL 如何支持事务？ 事务是数据库中一组相关的操作，这些操作被视为一个逻辑单元，要么全部成功执行，要么全部回滚。\n在MySQL中，事务是通过以下方式支持的：\n开启事务：使用START TRANSACTION或BEGIN语句来开始一个新的事务。 提交事务：使用COMMIT语句将事务的结果永久保存到数据库中，保证事务的持久性。 回滚事务：使用ROLLBACK语句取消事务，并撤销所有未提交的操作，将数据库恢复到事务开始前的状态。 设置事务隔离级别：MySQL支持多个事务隔离级别，包括读未提交、读已提交、可重复读和串行化。可以使用SET TRANSACTION ISOLATION LEVEL + 具体的隔离级别 语句设置事务隔离级别。 自动提交模式：MySQL默认使用自动提交模式，即每个SQL语句都会被视为一个独立的事务并自动提交。可以使用SET AUTOCOMMIT语句关闭自动提交模式。 通过使用这些事务相关的语句和设置，MySQL提供了对事务的支持。在使用事务时，需要注意以下事项：\n在事务中，如果发生错误或需要回滚，应该及时执行回滚操作，以保持数据的一致性。 在并发环境下，需要合理选择事务隔离级别，以满足应用程序的需求，并防止并发事务之间的数据冲突。 适当利用事务可以提高数据的完整性、一致性和可靠性，但过度使用事务也可能导致性能问题，因此需要根据具体情况进行权衡和优化。 综上所述，MySQL通过提供事务支持的语句和设置，使得开发者可以使用事务来确保数据库操作的原子性、一致性、隔离性和持久性。\n请解释数据库的隔离级别，并说明它们的区别 MySQL 的隔离级别有哪些？各自的特点和应用场景是什么？\n**数据库的事务隔离级别是指在并发事务执行过程中，一个事务对其他事务的影响程度。**隔离级别定义了各个事务之间的可见性和影响范围，涉及并发事务之间的读取和写入操作。事务的隔离级别越高，并发性能效率越低\n在SQL标准中，定义了四个常见的隔离级别，分别是：\n读未提交（Read Uncommitted）：\n最低级别的隔离级别，一个事务可以读取另一个事务尚未提交的数据。 可能会导致脏读（Dirty Read），即读取到了未提交的、可能被回滚的数据，还可能导致不可重复读和幻读 不提供任何事务隔离，不推荐在实际应用中使用。 读已提交（Read Committed）：\n保证一个事务只能读取到已经提交的数据。 避免了脏读，但可能会导致不可重复读（Non-repeatable Read）问题，即在同一个事务中多次读取同一行数据得到不同的结果，还可能导致幻读 可重复读（Repeatable Read）：\n保证在同一个事务中多次读取同一行数据时，得到的结果是一致的。 避免了脏读和不可重复读，但可能会导致幻读（Phantom Read）问题，即在同一个事务中多次执行某个范围的查询，得到的结果集不一致。 可重复读，指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，MySQL InnoDB 引擎的默认隔离级别，解决了不可重复读的问题，并且以很大程度上避免幻读现象的发生。 串行化（Serializable）：\n最高级别的隔离级别，要求事务串行执行，确保每个事务独立执行，互不干扰，可以避免所有并发问题 避免了脏读、不可重复读和幻读问题，但牺牲了并发性能，因为事务需要依次执行。 MySQL的默认隔离级别是可重复读(Repeatable Read)。\n其中，脏读指一个事务读到了另一个事务未提交的数据，不可重复读指同一个事务多次读取同一数据得到不同结果，幻读指同一个事务前后读取的数据集合不一致。\n这些隔离级别在保证事务隔离性的同时，也引入了不同程度的开销和并发性能影响。随着隔离级别的提高，事务之间的隔离性增强，但同时也会导致更多的锁和并发控制机制的使用，可能会降低并发性能。\n在MySQL中，可以使用SET TRANSACTION ISOLATION LEVEL语句来设置事务的隔离级别，如：SET TRANSACTION ISOLATION LEVEL READ COMMITTED;。此外，也可以在连接字符串或配置文件中配置默认的隔离级别。\nSerializable隔离级别提供了最高级别的数据隔离和保证，确保事务的一致性和完整性。然而，它的缺点是牺牲了并发性能，因为事务之间必须串行执行，导致系统的吞吐量下降。\nSerializable隔离级别在以下场景中适用：\n对数据的一致性和完整性要求非常高，不容忍任何数据冲突或不一致性。 数据库中同时进行大量复杂查询和更新操作，需要确保查询结果的准确性和一致性。 并发访问量较小，对性能要求不高，更注重数据的准确性和稳定性。 并发事务会导致什么问题？ 其中，脏读指一个事务读到了另一个事务未提交的数据，不可重复读指同一个事务多次读取同一数据得到不同结\n果，幻读指同一个事务前后读取的数据集合不一致。\n幻读、不可重复读和脏读是数据库中的隔离级别（Isolation Level）问题，用来描述并发环境下读取数据时可能遇到的不一致情况。\n脏读（Dirty Read）：**一个事务读取到了另外一个事务未提交的事务。**在事务A修改了某个数据行但尚未提交时，事务B读取了同一数据行，此时事务B读取到的数据是事务A未提交的数据，即读取了\u0026quot;脏\u0026quot;数据。 不可重复读（Non-repeatable Read）：**同一个事务中多次读取同一行数据得到不同的结果。**在事务A读取某个数据行时，事务B修改了该数据行并提交，然后事务A再次读取同一数据行时，发现数据行的内容与之前读取的不一样，出现了不一致的情况。 幻读（Phantom Read）：**在同一个事务中多次执行某个范围的查询，得到的结果集不一致。**在事务A读取某个范围内的数据行时，事务B在此范围内插入了新的数据行，然后事务A再次读取同一范围内的数据行时，会发现出现了新的数据行，就像出现了幻觉一样。 MySQL事务的隔离级别及特点 MySQL事务有四种隔离级别，隔离级别越高，性能效率越低\n1.读未提交(Read Uncommitted):事务可以读取未提交的数据，可能导致幻读、不可重复读、脏读\n⒉.读已提交(Read Committed)∶只能读取已经提交的数据，可以避免脏读问题，但是可能会遇到不可重复读、幻\n读问题;\n3.可重复读(Repeatable Read)︰保证同一个事务中多次读取同一数据的结果是一致的，避免了脏读和不可重复读问题，但是可能会遇到幻读问题;\n可重复读，指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，MySQL InnoDB 引擎的默认隔离级别，解决了不可重复读的问题，并且以很大程度上避免幻读现象的发生。\n4.序列化(Serializable)︰最高的隔离级别，可以避免所有并发问题，但是并发性能非常低，开销很大。在这个级别，事务被完全串行化执行，即一个事务必须在另一个事务完成后才能开始。\nMySQL的默认隔离级别是可重复读(Repeatable Read)。\n并发条件下的并发问题\n其中，脏读指一个事务读到了另一个事务未提交的数据，不可重复读指同一个事务多次读取同一数据得到不同结果，幻读指同一个事务前后读取的数据集合不一致。\n三个并发问题的区别如下：\n脏读的重点在于未提交。脏读应该是三个里面最好理解的，其定义很轻易便能理解，一个事务中读取了另外一个事务未提交的数据，是先修改再读； 不可重复读的重点在于对单条数据读取了两遍。T1先读取了一遍，而后T2修改该数据并提交，最后T1再次读取了该数据发现与之前的不同； 幻读的重点在于针对一类条件对一系列数据读取了两遍。比较特殊的点在于幻读是具备条件的查询，这种查询可能查出来的并不只有一条数据，而在两次查询过程中另外一个事务对查询的结果集中的某条数据进行了变动。 也就是说：\n在「读未提交」隔离级别下，可能发生脏读、不可重复读和幻读现象； 在「读提交」隔离级别下，可能发生不可重复读和幻读现象，但是不可能发生脏读现象； 在「可重复读」隔离级别下，可能发生幻读现象，但是不可能脏读和不可重复读现象； 在「串行化」隔离级别下，脏读、不可重复读和幻读现象都不可能会发生。 所以，要解决脏读现象，就要升级到「读提交」以上的隔离级别；要解决不可重复读现象，就要升级到「可重复读」的隔离级别，要解决幻读现象不建议将隔离级别升级到「串行化」。\nMySQL的隔离级别具体是如何实现的呢？ 事务的隔离级别是通过并发控制机制来实现的，主要涉及锁机制和数据版本控制。不同的隔离级别采用不同的并发控制策略来处理并发事务之间可能发生的问题。以下是各个隔离级别的实现方式：\n对于「读未提交」隔离级别的事务来说，因为可以读到未提交事务修改的数据，所以直接读取最新的数据 对于「串行化」隔离级别的事务来说，通过加读写锁的方式来避免并行访问； 对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 Read View 来实现的，它们的区别在于创建 Read View 的时机不同，大家可以把 Read View 理解成一个数据快照，就像相机拍照那样，定格某一时刻的风景。「读提交」隔离级别是在「每个语句执行前」都会重新生成一个 Read View，而「可重复读」隔离级别是「启动事务时」生成一个 Read View，然后整个事务期间都在用这个 Read View。 注意，执行「开始事务」命令，并不意味着启动了事务。在 MySQL 有两种开启事务的命令，分别是：\n第一种：begin/start transaction 命令； 第二种：start transaction with consistent snapshot 命令； 这两种开启事务的命令，事务的启动时机是不同的：\n执行了 begin/start transaction 命令后，并不代表事务启动了。只有在执行这个命令后，执行了增删查改操作的 SQL 语句，才是事务真正启动的时机； 执行了 start transaction with consistent snapshot 命令，就会马上启动事务。 在MySQL中，默认的事务隔离级别是REPEATABLE READ。您可以通过以下SQL查询来查看当前的隔离级别：\nSHOW VARIABLES LIKE \u0026#39;transaction_isolation\u0026#39;; 如果您想要更改事务隔离级别，可以使用以下SQL命令：\nSET TRANSACTION ISOLATION LEVEL [隔离级别]; 什么是MVCC机制？它的作用是什么？ **MVCC（Multi-Version Concurrency Control）机制是一种并发控制技术，用于处理数据库系统中并发事务的冲突问题。**它通过创建数据的多个版本来实现并发访问和修改，提高数据库的并发性能和事务隔离性。\nMVCC的作用主要体现在以下几个方面：\n并发访问：**MVCC允许多个事务同时读取数据库中的数据，从而提高数据库的并发性能。**每个事务读取的数据版本是事务开始时的一个一致快照，因此不会受到其他事务的并发修改影响。 事务隔离：**MVCC通过为每个事务创建独立的数据版本来实现事务隔离。**每个事务只能看到自己开始之前已经提交的数据版本，对于在事务开始后提交的数据版本，事务无法看到。这样可以避免脏读、不可重复读和幻读等并发问题。 数据一致性：**MVCC保证了事务读取的数据是一致的，即事务开始后对数据的读取操作都将看到一致的数据快照。**这样可以避免不一致的数据状态对事务的影响。 高并发性能：由于MVCC允许多个事务并发读取和修改数据，减少了事务之间的互斥和冲突，提高了数据库的并发性能和吞吐量。MVCC的并发控制方式相对于传统的锁机制能够更好地支持高并发访问。 MVCC的实现方式可能因数据库系统而异。通常，MVCC使用数据版本控制和快照视图来实现。每个数据行都有一个或多个版本与之关联，每个版本都有一个时间戳表示。当事务开始时，它会创建一个读操作的快照视图，该视图包含了事务开始时的数据库状态。快照视图中只能看到在事务开始之前已经提交的版本，对于在事务开始后提交的版本，事务无法看到。\n需要注意的是，MVCC在提供高并发性能和事务隔离性的同时，也会增加存储空间的需求，因为每个数据版本都需要占用存储空间。因此，在使用MVCC时需要综合考虑数据库系统的存储能力和性能要求。\nMVCC是如何实现并发控制的？简要解释MVCC的原理和实现方式。 MVCC（Multi-Version Concurrency Control）是一种并发控制技术，通过创建数据的多个版本来实现并发访问和修改。其原理是基于每个事务的读操作创建一个一致的数据快照视图，从而实现事务隔离和高并发性能。\nMVCC的实现方式通常涉及以下几个关键组件：\n版本号（Version Number）：每个数据行都有一个或多个与之关联的版本，每个版本都有一个时间戳表示。 数据行的版本管理：对于每个数据行，数据库系统维护多个版本。每次对数据行进行更新操作时，都会创建一个新版本，并将更新操作的时间戳与之关联。 读操作的快照视图：当事务开始时，它会创建一个读操作的快照视图，该视图包含了事务开始时的数据库状态。快照视图中只能看到在事务开始之前已经提交的版本，对于在事务开始后提交的版本，事务无法看到。 版本的可见性判断：在执行读操作时，数据库会根据事务的快照视图判断哪些数据版本对该事务是可见的。只有早于或等于事务开始时间的已提交版本对该事务可见，未提交的或晚于事务开始时间的版本对该事务不可见。 基于以上原理和实现方式，MVCC实现并发控制的过程可以简述如下：\n当事务开始时，为该事务分配一个唯一的版本号。 执行读操作时，根据事务的快照视图和版本的时间戳判断哪些数据版本对该事务可见。 执行写操作时，创建新的数据版本，并将更新操作的时间戳与之关联。新版本会被其他正在执行的事务忽略，直到当前事务提交。 当事务提交时，将事务的版本号标记为已提交，并对该事务创建的新版本进行持久化操作。 通过MVCC的实现，多个事务可以同时读取数据库中的数据，彼此之间相互隔离，读操作不会受到其他事务的并发修改的影响。同时，MVCC通过创建数据的多个版本，避免了脏读、不可重复读和幻读等并发问题，提供了一定的事务隔离性。\nMVCC如何实现RC和RR？ MVCC（多版本并发控制）是一种数据库事务管理的方法，旨在允许多个事务同时进行而不会相互干扰。在 MVCC 中，有两种常见的隔离级别，即 RC（Read Committed）和 RR（Repeatable Read）。下面我将简要解释如何在 MVCC 中实现这两种隔离级别：\nRead Committed（RC）隔离级别： 在 RC 隔离级别下，一个事务只能读取已经提交的数据，也就是说，一个事务只能看到已经提交的其他事务所做的更改。要实现 RC 隔离级别，数据库会为每个事务分配一个特定的时间戳，并在读取数据时检查该时间戳，只读取小于等于该时间戳的已提交数据版本。\nRepeatable Read（RR）隔离级别： 在 RR 隔离级别下，一个事务的读操作不会受到其他事务的修改影响，即使其他事务在该事务执行期间进行了更改。为了实现 RR 隔离级别，数据库会在事务开始时记录一个快照（Snapshot），该快照包含了所有已提交的数据版本。在事务执行期间，该快照将作为事务的数据视图，而不会受到其他事务的影响。\n在 MVCC 中，通常使用以下几种技术来实现 RC 和 RR 隔离级别：\n版本链（Version Chains）： 数据库会为每个数据行维护一个版本链，其中包含了所有已提交的数据版本。当一个事务开始时，它会被分配一个时间戳，读取操作只能访问在该时间戳之前提交的数据版本。\n快照隔离（Snapshot Isolation）： 在 RR 隔离级别中，数据库会为每个事务创建一个快照，记录事务开始时的数据状态。事务的读操作将使用该快照，而不会受到其他事务的影响。\n多版本索引（Multi-Version Indexing）： 在支持 MVCC 的数据库中，索引通常会包含多个版本的数据，以支持不同事务的并发操作。索引中的版本信息可以帮助事务正确地获取符合其隔离级别要求的数据。\n总之，MVCC 是一种高效的并发控制方法，允许多个事务同时操作数据库，而不会出现读取脏数据或写入冲突等问题。 RC 和 RR 隔离级别是 MVCC 的两种典型实现方式，通过使用时间戳、版本链和快照等技术来确保事务的隔离性和一致性。\nMVCC 是什么？InnoDB 是如何实现 MVCC 机制的？ 来自：haha\nMVCC MVCC指的是多版本并发控制，是指维护一条记录的多个版本，使得读写操作没有冲突。\nMVCC的实现 InnoDB对于MVCC的实现，我主要从下面几个点来讲：\n当前读和快照读的概念 MySQL数据的隐藏字段 undo 日志中的版本链 readView（读视图） MVCC实现流程 当前读与快照读 当前读：就是读取的记录总是最新的，实现的原理就是对正在读的记录加锁，使得读写互斥，这样保证每次读取的都是数据库中最新的记录\n快照读：每次读取的时候不一定是最新的数据，而是这条记录的快照版本，这样可以保证读写不互斥，能够并发执行\nMySQL的隐藏字段 MVCC的实现主要依赖于：MySQL的隐藏字段、undo log中的版本链、readView MySQL中每条记录是有两个隐藏的字段，分别是：\nDB_TRX_ID 最近修改事务ID：记录着上一次修改该条记录的日志id DB_ROLL_PTR 回滚指针：指向上一个版本的记录的地址，用于配合undo log，指向上一个版本 undo log 版本链 版本链顾名思义就是记录的版本的链表。当事务并发执行修改某条记录的时候，不同的事物对这条数据的修改产生多个版本，每次修改之前都会记录下这条记录之前的数据，在隐藏字段中设置上本次操作事务的ID，并让回滚指针指向上一个版本，这样就会形成一条链表，就锁所谓的版本链。\nreadView 读视图 readView读视图记录并维护了当前系统活跃的事务ID,为快照读时MVCC提供数据的依据。其主要有以下几个属性：\n当前读：读取当前记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。 快照读：读取某一时刻的可见版本，可能是历史数据，不加锁，非阻塞读 READ COMMITED：每次查询都生成一个快照读 REPEATED READ：开启事务后第一个select语句才是快照读的地方 Readview中四个核心字段：\nm_ids 记录当前活跃事务的ID集合 min_trx_id 最小活跃事务ID max_trx_id 预分配事务ID，就是最大活跃事务ID+1（因为事务ID是自增的） creator_trx_id ReadView创建者的事务ID MVCC执行的流程 当并发事务执行的时候，执行查询操作的时候，会根据这个事务的ID（trx_id）和readView中的事务ID进行一些比较来确定读取哪个版本的快照记录，具体的规则为：\n当前事务的ID小于读视图中的最小事务ID，说明记录的修改已经提交了，可以访问 当前事务ID等于读视图中创建者ID，说明这条记录就是当前事务修改的，可以访问 当前事务ID大于等于读视图中预分配事务ID，说明这条记录是在读视图创建之后修改的，不可访问当前版本。 如果当前事务ID在最小事务ID和最大ID之间且不在ID集合m_ids中，说明当前版本的修改事务已经提交，可以访问 RC和RR的区别 RC级别时，在一个事务中，每执行一次查询都会生成一次读视图\nRR级别时，在一个事务中，只有第一次查询会生成一个读视图，后面的查询都是复用这个读视图，保证了可重复读\n索引 了解索引吗？索引的优缺点？ 索引是一种高效查询数据的的数据结构，类似于书的目录，通过创建特定的数据结构，将表中的一个或多个列的值与其在表中的物理位置相关联。加速数据的查找和访问，从而提高数据库的查询性能。\n数据库索引是一种数据结构，用于提高数据库表的查询效率。索引可以帮助数据库快速定位和检索存储在表中的数据，从而加快数据查询的速度。在数据量比较大时，使用索引可以极大地提高数据检索的效率。\n索引的作用是通过构建一个额外的数据结构（B-tree、哈希表等）来加速数据的检索。它是在数据库表上创建的一种数据结构，它包含一些指向表中数据的指针，可以快速地定位到满足查询条件的数据行，从而提高查询效率。索引可以包含一个或多个列，可以使用单列索引、组合索引、全文索引等多种方式来创建。\n适合使用索引的场景包括：\n频繁查询的列，如主键、外键等。 经常作为查询条件的列，如WHERE、ORDER BY、GROUP BY 等语句中的列。 经常需要连接的列，如多表联合查询时的列。 数据量较大的表，通过索引可以加快数据检索速度。 以下是在数据库中使用索引的几个好处：\n快速数据检索：通过使用索引，数据库引擎可以更快地找到符合查询条件的数据行。相比于全表扫描，索引可以通过跳过不符合条件的行，快速定位到需要的数据。 提高查询性能：使用索引可以减少查询语句的执行时间。当数据库系统执行查询时，它会首先查找索引，然后根据索引找到相应的数据行。 优化排序和分组操作：当进行排序或分组操作时，索引可以帮助数据库避免对整个表进行排序或分组。通过使用索引，数据库可以直接使用索引中已排序的数据，减少排序和分组的开销，提高性能。 尽管索引提供了许多优点，但也需要权衡考虑以下方面：\n索引会占用额外的存储空间，特别是在处理大量数据时。 **索引的维护需要一定的时间和资源。**当数据发生变更时，索引也需要进行更新。 拓展：\n在平时使用的时候有没有遇到过索引失效的情况呢? 说说什么情况会让索引失效\n索引文件具有 B-Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引。\n计算、函数、类型转换(自动或手动)导致索引失效\n不等于(!= 或者\u0026lt;\u0026gt;)索引失效\nlike以通配符%开头索引失效\nOR前后的条件列没有进行索引，那么索引会失效\n。。。\n你是怎么知道这个查询是否使用到了索引的？\n使用 EXPLAIN + 查询语句，可以看到该查询的执行计划，会表明使用了哪些索引。 知道索引覆盖和索引下推吗？什么是回表？\n索引覆盖指要查询的列在索引中都存在，直接可以返回数据 索引下推指当数据库系统接收到一个查询请求时，它会将查询条件中可用于索引的部分下推到存储引擎层，由存储引擎完成索引的遍历和查询匹配，这样可以提高数据库查询的效率 回表查询（面试重点）：当对非聚集索引进行查询的时候，首先根据二级索引找到值对应的主键id，此时返回的就是一个id，而不是该条数据的全部信息，然后还需要到聚集索引中根据主键id拿到所有的数据，这个操作叫做回表查询。\n如何解决回表查询：使用联合索引，不会再次触发回表查询。\nMySQL 索引用的什么数据结构了解吗？X MySQL 的默认存储引擎是 InnoDB，它采用的是 B+树结构的索引。\nB+tree：只有叶子节点才会存储数据，非叶子节点只存储键值。叶子节点之间使用双向指针连接，最底层的叶子节点形成了一个双向有序链表。 在 MySQL 中，B+ 树的实现主要是通过 InnoDB 存储引擎来实现的。InnoDB 存储引擎中的索引主要有聚簇索引和辅助索引两种类型，聚簇索引是根据主键创建的索引，而辅助索引是根据非主键列创建的索引。对于辅助索引，MySQL 中会同时创建一个对应的聚簇索引，这样可以提高查询效率。\nB+tree数据结构的特点：\n平衡多路查找树：B+树是一种平衡树，它可以保持树的高度相对较小，使得在大规模数据中进行查找、插入、删除等操作都具有高效性能。 非叶子节点存储键值：B+树的非叶子节点存储键值，而不是实际的数据，这降低了树的高度，进一步提高了检索效率。 叶子节点链表：所有的叶子节点通过指针连接成一个有序链表，使得范围查询和范围扫描非常高效，可以在O(log n + k)时间内完成，其中n是数据总量，k是范围内的数据量。 有序性：B+树的所有节点都保持有序，包括叶子节点和非叶子节点。这使得在进行范围查询时，可以直接顺着链表进行遍历。 聚簇索引支持：B+树的特点使得它非常适合作为聚簇索引的数据结构。数据库的聚簇索引决定了数据在磁盘上的物理存储顺序，而B+树的有序性正好满足了这一要求。 适应磁盘IO：B+树的节点通常可以存储多个键值，这意味着在读写时可以一次性读取或写入多个数据项，减少了磁盘IO次数，提高了数据操作效率。 适应动态插入和删除：B+树的平衡性质使得在插入和删除数据时可以进行节点的分裂和合并操作，保持树的平衡状态，不会出现退化为链表的情况。 如何创建和使用索引？有哪些常见的索引类型？ 索引是一种高效查询数据的的数据结构，类似于书的目录，通过创建特定的数据结构，将表中的一个或多个列的值与其在表中的物理位置相关联。加速数据的查找和访问，从而提高数据库的查询性能。\n在MySQL中，可以使用以下方式创建和使用索引：\n创建索引：\n使用CREATE INDEX语句创建单列索引，例如：CREATE INDEX index_name ON table_name (column_name); 使用CREATE INDEX语句创建多列索引，例如：CREATE INDEX index_name ON table_name (column1, column2); 使用CREATE UNIQUE INDEX语句创建唯一索引，确保索引列的值唯一，例如：CREATE UNIQUE INDEX index_name ON table_name (column_name); 使用索引：\n查询优化：MySQL查询优化器会根据查询语句和表的索引信息选择最优的索引来执行查询 使用索引：可以使用USE INDEX建议使用索引或FORCE INDEX来强制使用特定的索引执行查询。 禁用索引：使用IGNORE INDEX提示可以在查询中禁用特定的索引 索引分类\n按「数据结构」分类：B+tree索引、Hash索引、Full-text索引。 按「物理存储」分类：聚簇索引（主键索引）、二级索引（辅助索引）。 按「字段特性」分类：主键索引、唯一索引、普通索引、前缀索引。 按「字段个数」分类：单列索引、联合索引。 按照数据结构分为：\nB+tree索引 Full-text索引 哈希索引 MySQL常见的存储引擎分别所支持的索引类型：\n按照索引的存储结构可分为：\n聚集索引（聚簇索引） 非聚簇索引（二级索引） 主键索引与二级索引的区别：\n主键索引的B+tree的叶子结点中存放的是实际数据，所有完整的用户记录都存放在主键索引B+tree的叶子结点里 二级索引的B+tree的叶子结点中存放的是主键值，而不是实际数据，可以有多个二级索引（非聚簇索引） 回表：查询使用了二级索引，只能查询到索引字段和主键的值，需要通过主键索引再次查询数据库，获取完整数据\n回表的原因是在非聚簇索引上只存储了索引字段和对应的主键值，并未存储完整的行数据。为了获取完整的行数据，需要再次通过聚簇索引来定位到数据页，并获取相应的行记录。 按照字段特性分为：\n主键索引: InnoDB 主键是默认的索引，满足唯一性约束、非空约束，一个表只能有一个主键。 唯一索引: 数据列不允许重复，允许为 NULL 值，一个表允许多个列创建唯一索引。 普通索引: 基本的索引类型，没有唯一性的限制，允许为 NULL 值。 前缀索引：对字符类型字段的前几个字符建立的索引，而不是在整个字段上建立的索引，前缀索引可以建立在字段类型为 char、 varchar、binary、varbinary 的列上。 按「字段个数」分类\n单列索引，建立在单列上的索引 联合索引，建立在多列上的索引 使用联合索引时，存在最左匹配原则，也就是按照最左优先的方式进行索引的匹配。在使用联合索引进行查询的时候，如果不遵循「最左匹配原则」，联合索引会失效。\n自增主键和非自增主键的区别和使用场景，如何将非自增变为自增主键？ （提到了自增主键效率高一点，非自增主键可能会暴露一些敏感信息）\n为什么自增主键效率高一点（底层b+树，数据有序，双向链表，范围查询快，批量插入不用页分裂合并） 场景题：既然自增主键效率高一点如何把非自增主键变为自增主键（这边说了内部用自增主键，然后前台返回数据的时候脱敏处理） 如果我们使用自增主键，那么每次插入的新数据就会按顺序添加到当前索引节点的位置，不需要移动已有的数据，当页面写满，就会自动开辟一个新页面。因为每次插入一条新记录，都是追加操作，不需要重新移动数据，因此这种插入数据的方法效率非常高。 如果我们使用非自增主键，由于每次插入主键的索引值都是随机的，因此每次插入新的数据时，就可能会插入到现有数据页中间的某个位置，这将不得不移动其它数据来满足新数据的插入，甚至需要从一个页面复制数据到另外一个页面，我们通常将这种情况称为页分裂。页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率。 自增主键和非自增主键区别和使用场景：\n自增主键：自增主键是指在插入数据时，数据库会自动为主键列分配一个唯一的自增值。自增主键通常适用于那些不需要涉及敏感信息的表，例如用户 ID、订单号等。自增主键具有高效性能，因为底层的 B+ 树数据结构以及有序的关系，使得范围查询和批量插入都更加高效。 非自增主键：非自增主键是需要由用户显示地指定的主键值。非自增主键适用于那些需要涉及敏感信息的表，例如用户的身份证号、手机号等。由于非自增主键的值可能有特定的业务含义，因此需要灵活处理。 如何将非自增主键变为自增主键： 如果已经创建了一个非自增主键的表，并且需要将其变为自增主键，可以通过以下步骤进行操作：\n创建一个新的带有自增主键的辅助表，例如 temp_table，它可以只包含一个自增主键列和其他需要的列。 从原始表中选择所有的数据并插入到 temp_table 表中，此时自增主键会自动分配。 根据需要重命名或删除原始表，并将 temp_table 重命名为原始表的名称。 在重新命名后的表中，可以增加其他需要的列或者进行其他的修改操作。 注意，如果表之间有关联关系，需要额外处理外键约束，在执行上述操作之前请确保已考虑到相关的约束问题。 在将非自增主键变为自增主键后，前端返回数据时可以进行脱敏处理，避免暴露敏感信息。例如，可以使用哈希函数、加密算法等对敏感信息进行处理，以保护用户的隐私。\n索引的使用场景？ 索引最大的好处是提高查询速度，但是索引也是有缺点的，比如：\n需要占用额外的物理空间 创建索引和维护索引要耗费时间 会降低表的增删改的效率，因为每次增删改索引，B+ 树为了维护索引有序性，都需要进行动态维护。 什么时候适用索引？\n字段有唯一性限制的，比如商品编码； 经常用于 WHERE 查询条件的字段，这样能够提高整个表的查询速度，如果查询条件不是一个字段，可以建立联合索引。 经常用于 GROUP BY 和 ORDER BY 的字段，这样在查询的时候就不需要再去做一次排序了，因为我们都已经知道了建立索引之后在 B+Tree 中的记录都是排序好的。 什么时候不需要创建索引？\nWHERE 条件，GROUP BY，ORDER BY 里用不到的字段，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的，因为索引是会占用物理空间的。 字段中存在大量重复数据，不需要创建索引，比如性别字段，只有男女，如果数据库表中，男女的记录分布均匀，那么无论搜索哪个值都可能得到一半的数据。在这些情况下，还不如不要索引，因为 MySQL 还有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比很高的时候，它一般会忽略索引，进行全表扫描。 表数据太少的时候，不需要创建索引； **经常更新的字段不用创建索引，**比如不要对电商项目的用户余额建立索引，因为索引字段频繁修改，由于要维护 B+Tree的有序性，那么就需要频繁的重建索引，这个过程是会影响数据库性能的 数据库表中的数据量较小时，可能不需要为每个字段都创建索引。 索引是不是建的越多越好呢？\n不是的，索引并不是建得越多越好，需要权衡索引的数量和性能需求。过多的索引可能会带来以下问题：\n额外的存储空间：**每个索引都需要占用一定的存储空间，如果过多地创建索引，会增加数据库的存储需求。**这在大规模数据集和有限存储空间的环境下可能成为问题。 写操作的开销：当数据发生插入、更新或删除操作时，数据库不仅需要更新数据行，还需要更新相关的索引。过多的索引会增加写操作的开销，导致写入性能下降。 索引维护成本：**维护索引需要消耗额外的计算资源和时间。**当数据发生变化时，数据库需要保持索引的一致性，对索引进行更新和重建。过多的索引会增加维护的开销。 查询性能下降：**当查询涉及多个索引时，数据库需要选择并使用合适的索引。**过多的索引会增加查询优化器的复杂性，并可能导致选择不合适的索引或产生冲突，从而降低查询性能。 因此，在创建索引时，需要谨慎评估和选择适当的字段进行索引，避免过多的冗余索引。应根据具体的查询需求、数据量、写入操作频率和存储资源等因素综合考虑，以获得最佳的性能平衡。定期检查和优化索引，清理无效或不再使用的索引，也是保持数据库性能的重要工作。\n覆盖索引和联合索引是什么？索引的最左匹配原则？ 覆盖索引是指一个包含了所有查询需要的列的索引，查询时可以直接从索引中取到需要的数据，而不需要再回到表中查找，从而可以提高查询效率。\n联合索引是指使用多个列组合起来作为一个索引，可以同时查询多个列，以提高查询效率。联合索引可以包含多个列，但是查询时只能使用前缀列进行查询，即只有在查询中使用了联合索引的前几个列，才能利用联合索引进行查询。如果查询中没有使用前缀列，那么联合索引就不能发挥作用，需要使用单独的索引或全表扫描。\n-- 假设有联合索引(A,B,C) SELECT A,B,C FROM table -- 使用覆盖索引，查询列均为索引列，无需回表 最左前缀匹配原则是指如果一个联合索引包含了多个列，那么在查询时只能使用前面的列进行匹配。例如，一个联合索引包含了 A、B、C 三列，那么查询时只能使用 A、AB 或 ABC 进行匹配，而不能只使用 B 或 C 进行匹配。这是因为如果查询时使用的列不是最左前缀列，那么 MySQL 就无法使用索引进行查询，会导致全表扫描，从而降低查询效率。\n-- 假设有联合索引(A,B,C) SELECT A,B,C FROM table WHERE A=1 AND B=1 -- 满足最左前缀匹配原则，可以使用到联合索引 SELECT A,B,C FROM table WHERE B=1 AND C=1 -- 不满足最左前缀匹配原则，未使用到联合索引 最左前缀法则（Leftmost Prefix Rule）是指查询要从联合索引的最左列开始，并且不跳过索引中的列。如果跳过某一列，索引将部分失效（后面的字段索引失效）\n索引下推 索引下推（Index Pushdown）是数据库查询优化的一种技术，它通过将部分查询条件的计算下推到存储引擎层级，以减少数据传输和处理的开销，提高查询性能。\n传统的查询处理流程是：首先从存储引擎中读取数据页到内存中，然后在内存中进行查询条件的计算和过滤。这种方式要经历两次数据传输，即从磁盘到内存，再从内存到查询处理引擎，而且需要在内存中进行大量的计算操作。\n而索引下推则是在存储引擎层级就能进行一部分查询计算和过滤，从而避免了不必要的数据传输和计算。\n具体来说，索引下推是指**当数据库系统接收到一个查询请求时，它会将查询条件中可用于索引的部分下推到存储引擎层，由存储引擎完成索引的遍历和查询匹配，**从而减少需要传输到查询处理引擎的数据量。\n索引下推的优势在于：\n减少数据传输：只将满足查询条件的数据传输到查询处理引擎，减少了不必要的数据传输和磁盘I/O开销。 减少计算开销：在存储引擎层级完成一部分查询条件的计算和过滤，减少了内存中的计算开销。 提高查询性能：通过减少数据传输和计算开销，提高了查询的响应速度和吞吐量。 需要注意的是，索引下推并不适用于所有类型的查询和所有存储引擎。具体是否支持索引下推，以及支持的程度，取决于数据库系统和存储引擎的实现。\n以MySQL为例，从MySQL 5.6开始，InnoDB存储引擎开始支持索引下推的优化技术，可以通过配置参数innodb_use_index_extensions来启用或禁用索引下推优化。而在MySQL 8.0版本中，引入了更强大的索引下推优化，进一步提升了查询性能。\n总的来说，索引下推是一种有效的查询优化技术，通过减少数据传输和计算开销来提高查询性能，值得在需要优化查询的场景中考虑使用。\n下面是一些常见的索引下推的例子：\n索引下推列选择：当查询条件包含索引列时，存储引擎可以在索引层级直接完成比较操作并返回匹配的行，而无需将所有数据传输到查询处理引擎。这样可以减少数据传输和计算开销。例如，对于以下查询： SELECT * FROM table WHERE index_col = 123 AND other_col = \u0026#39;abc\u0026#39;; 如果index_col是一个索引列，存储引擎可以先在索引层级过滤掉不满足条件的行，只将满足条件的数据传输到查询处理引擎进行进一步处理。\n索引下推列过滤：当查询条件包含索引列和非索引列的比较操作时，存储引擎可以在索引层级直接使用索引完成过滤，而不必将所有数据传输到查询处理引擎进行过滤操作。例如，对于以下查询： SELECT * FROM table WHERE index_col = 123 AND non_index_col LIKE \u0026#39;%abc%\u0026#39;; 存储引擎可以先通过索引过滤掉不符合index_col条件的行，然后对剩余的数据应用LIKE操作，减少了数据传输和计算开销。\n索引下推索引合并：当查询条件包含多个索引列的比较操作时，存储引擎可以将这些索引条件下推合并到一个单独的索引扫描操作中，减少多次索引扫描和数据传输开销。例如，对于以下查询： SELECT * FROM table WHERE index_col1 \u0026gt; 100 AND index_col2 \u0026lt; 200; 存储引擎可以将index_col1和index_col2两个索引的扫描合并到一个扫描过程中，减少了数据传输和计算开销。\n这些只是索引下推的一些常见例子，实际上，索引下推的优化技术还可以应用于更复杂的查询条件和多表关联查询，以提高查询性能和效率。不同的数据库和存储引擎可能会有不同的实现方式和支持程度，具体的效果可能会因数据库系统和表结构而异。因此，在具体应用中，需要根据查询场景和数据库特性进行适当的调整和优化。\n唯一索引和主键索引有什么区别？ 唯一索引（Unique Index）和主键索引（Primary Key Index）是两种常见的索引类型，它们在功能和用途上有一些区别。\n唯一索引（Unique Index）：\n唯一索引要求索引列的值必须是唯一的，即不允许有重复的值。 可以在一个表中创建多个唯一索引，每个唯一索引可以涵盖一个或多个列。 唯一索引允许空值（NULL），但只允许一个空值。 唯一索引可以用于确保数据的唯一性约束，防止重复数据的插入。 主键索引（Primary Key Index）：\n主键索引是一种特殊的唯一索引，用于标识表中的每一行数据的唯一性。 每个表只能有一个主键索引，通常由一个或多个列组成，用于唯一标识表中的每一行。 主键索引不允许空值（NULL），每一行都必须有一个非空的主键值。 主键索引可以用作其他表的外键关联。 区别：\n唯一性约束：唯一索引要求索引列的值唯一，而主键索引不仅要求唯一性，还要求非空性。 表中数量：每个表可以有多个唯一索引，但只能有一个主键索引。 空值允许性：唯一索引允许空值，但只允许一个空值；主键索引不允许空值，每一行都必须有一个非空的主键值。 用途：唯一索引可以用于确保数据的唯一性约束，防止重复数据的插入；主键索引用于唯一标识表中的每一行数据，并可作为其他表的外键关联。 InnoDB主键索引的B+tree高度高度为多高呢？ MySQL中主键索引的B+tree结构如下：\n假设：一行数据大小为1k，一页中可以存储16行这样的数据。InnoDB的指针占用6个字节的空间，主键为bigint类型，占用字节数为8。叶子结点存放行数据，n指当前这个结点存储的key数量\n高度为2的B+tree：（n为记录数） n * 8 + (n + 1) * 6 = 16*1024 , 算出n约为 1170\n也就是说，如果树的高度为2，则可以存储 18000 多条记录。\n高度为3的B+tree： 1171 * 1171 * 16 = 21939856\n也就是说，如果树的高度为3，则可以存储 2200w 左右的记录\n为什么要用 B+ 树索引，而不用其他索引？ 可以从几个维度去看这个问题，查询是否够快，效率是否稳定，存储数据多少，以及查找磁盘次数。\n为什么不用普通二叉树？ 普通二叉树存在退化的情况，如果它退化成链表，相当于全表扫描。查询效率会很低。\n为什么不用平衡二叉树呢？ 读取数据的时候，是从磁盘读到内存。如果使用平衡二叉树这种数据结构作为索引，那每查找一次数据就需要从磁盘中读取一个节点，也就是一个磁盘块，但是平衡二叉树可是每个节点只存储一个键值和数据的，如果是 B+ 树，可以存储更多的节点数据，树的高度也会降低，因此读取磁盘的次数就降下来啦，查询效率就快\n为什么用B+树索引而不使用hash索引呢？ 哈希索引的key是经过hash运算得出的，即跟实际数据的值没有关系，因此哈希索引不适用于范围查询和排序操作容易导致全表扫描，因为可能存在不同的key经过hash运算后值相同。索引列上的值相同的话，易造成hash冲突，效率低下。\n相比较，B+tree索引支持范围查询和排序操作，适用场景更广泛\n为什么用 B+ 树而不用 B 树呢？ 对于B-tree，无论是叶子节点还是非叶子节点，都会保存数据，这样导致一页中存储的键值减少，指针跟着减少，要同样保存大量数据，只能增加树的高度，导致性能降低；\n而B+tree所有数据都会出现在叶子结点上，叶子结点只存放数据，非叶子结点起索引数据作用，使得B+树节点可以容纳更多的键，减少了磁盘上的节点数量，从而减少了IO访问的次数。B+tree叶子结点会形成一个单向有序链表，进行顺序遍历时非常高效，而且范围查询高效\nHash 索引和 B+ 树索引区别是什么？ B+ 树可以进行范围查询，Hash 索引不能。 B+ 树支持联合索引的最左侧原则，Hash 索引不支持。 B+ 树支持 order by 排序，Hash 索引不支持。 Hash 索引在等值查询上比 B+ 树效率更高。 B+ 树使用 like 进行模糊查询的时候，like 后面（比如 % 开头）的话可以起到优化的作用，Hash 索引根本无法进行模糊查询 Hash索引和B+树索引是两种常见的索引结构，它们有以下几个区别：\n存储结构：Hash索引使用哈希表的数据结构，其中键值对在内存中直接通过哈希函数进行存储和访问；而B+树索引使用B+树的数据结构，通过有序的节点链表组织数据。\n查找方式：Hash索引通过哈希函数将键映射到对应的桶中，然后在桶内进行查找，因此查找速度非常快，具有常数时间复杂度。而B+树索引通过从根节点到叶子节点的路径搜索，具有对数时间复杂度。\n支持范围查询和排序：B+树索引支持范围查询和排序操作，因为B+树具有有序的节点链表结构，而Hash索引只适用于等值查找，不支持范围查询和排序。\n内存使用：Hash索引需要将全部索引数据加载到内存中，因为哈希表的关键是直接在内存中进行操作，如果索引数据太大无法完全放入内存，性能会受到影响。而B+树索引可以根据需要，只加载部分索引数据到内存中进行操作。\n支持高并发写入：Hash索引对于插入和更新操作非常高效，因为只需要通过哈希函数即可快速定位到特定位置进行写入。而B+树索引对于高并发写入的场景，由于需要维护树的平衡性，可能需要进行频繁的节点分裂和合并操作，会导致一定的性能开销。\n综上所述，Hash索引适用于等值查找，并且对于内存能够容纳的数据集大小比较适用。而B+树索引适用于范围查询、排序和拥有大数据集的场景，并且能够支持高并发写入。在实际应用中，根据具体的查询需求、数据规模和系统特点选择合适的索引结构。\n索引失效场景有哪些？ 索引列运算，在索引列上进行运算操作，索引会失效（索引保存的是索引字段的原始值，而不是经过函数计算后的值，自然就没办法走索引了） 查询时字符串没加引号 or操作符两侧字段非都有索引导致两侧索引都失效 模糊匹配 尾部模糊匹配（like \u0026lsquo;177%\u0026rsquo;）索引不会失效，但头部模糊匹配（like \u0026lsquo;%x\u0026rsquo;）和 like %xx%这两种方式，索引将会失效 数据分布影响，如果MySQL评估使用索引比全表更慢，则不使用索引，即索引失效 联合索引在使用时违反了最左前缀法则 为什么联合索引不遵循最左匹配原则就会失效？\n原因是，在联合索引的情况下，数据是按照索引第一列排序，第一列数据相同时才会按照第二列排序。\n也就是说，如果我们想使用联合索引中尽可能多的列，查询条件中的各个列必须是联合索引中从最左边开始连续的列。如果我们仅仅按照第二列搜索，肯定无法走索引\n查询时字符串没加引号为什么就会索引失效？\n首先我们要知道 MySQL 的数据类型转换规则是什么？就是看 MySQL 是会将字符串转成数字处理，还是将数字转换成字符串处理。\n可以通过 select “10” \u0026gt; 9 的结果来知道MySQL 的数据类型转换规则是什么：\n如果规则是 MySQL 会将自动「字符串」转换成「数字」，就相当于 select 10 \u0026gt; 9，这个就是数字比较，所以结果应该是 1； 如果规则是 MySQL 会将自动「数字」转换成「字符串」，就相当于 select \u0026ldquo;10\u0026rdquo; \u0026gt; \u0026ldquo;9\u0026rdquo;，这个是字符串比较，字符串比较大小是逐位从高位到低位逐个比较（按ascii码） ，那么\u0026quot;10\u0026quot;字符串相当于 “1”和“0”字符的组合，所以先是拿 “1” 字符和 “9” 字符比较，因为 “1” 字符比 “9” 字符小，所以结果应该是 0。 在 MySQL 中，执行的结果如下图：\n上面的结果为 1，说明 MySQL 在遇到字符串和数字比较的时候，会自动把字符串转为数字，然后再进行比较。\n索引失效中使用了非通配符左前缀like%在前和不在前索引失效和不失效的情况 在 MySQL 查询中如果使用 like \u0026lsquo;%xxx\u0026rsquo;，则查询时 MySQL 不会使用索引，而是进行全表扫描，索引一定会失效\n但是对于 like \u0026lsquo;xxx%\u0026rsquo; 操作来说，我们需要注意以下几个方面：\n\\1. 索引失效原因：\n如果对于指定的列使用 like \u0026lsquo;xxx%\u0026rsquo; 操作，在该列上创建的索引就会失效，原因是该操作涉及通配符 %，因此 MySQL 无法确定要查找的值的具体位置，只能一行一行的查询，即使用全表扫描的方式。\n\\2. 索引可以生效：\n使用 like \u0026lsquo;xxx%\u0026rsquo; 进行查询时，如果查询条件中指定了该列的前缀 xx，那么该列上的索引就可以生效。例如，查询条件为 like \u0026lsquo;xxx%\u0026rsquo;，而索引列选择的是该列的前缀 xx，此时 MySQL 查询器会按照索引检索，不会使用全表扫描。\n\\3. 左前缀：\n使用 like \u0026lsquo;xxx%\u0026rsquo; 进行查询时，如果查询条件中指定了该列的左前缀 xxx，那么该列上的索引也可以生效。例如，查询条件为 like \u0026lsquo;xxx%yyy\u0026rsquo;，此时 MySQL 查询器会按照 xxx 这一部分去检索索引，如果该列上有索引，那么查询时就可以使用索引加速，而不会进行全表扫描。\n\\4. 通配符在前：\n对于 like \u0026lsquo;%xxx\u0026rsquo; 来说，由于通配符 % 在最前面，因此 MySQL 无法使用索引加速查询，而是进行全表扫描，导致索引失效。\n综上所述，对于 like \u0026lsquo;xxx%\u0026rsquo; 来说，如果查询条件中指定的是该列的前缀，或者是该列的左前缀，或者是不带通配符，则索引可以生效，否则会导致索引失效。另外，如果是 like \u0026lsquo;%xxx\u0026rsquo;，则索引一定会失效。\nSQL优化 \u0026amp;\u0026amp; 索引优化 MySQL超大分页如何处理？ 使用覆盖索引\n在数据量很大时，Limit分页查询需要对数据进行排序，效率较低\n解决方案：覆盖索引+子查询\nselect * from tb_sku limit 9000000, 10; //使用覆盖索引 select * from tb_sku s (select id from tb_sku order by id limit 90000000,10) a where s.id = a.id; 如何优化 SQL 查询性能？ 慢SQL的优化，主要从两个方面考虑，SQL语句本身的优化，以及数据库设计的优化。\n避免不必要的列 这个是老生常谈，但还是经常会出的情况，SQL查询的时候，应该只查询需要的列，而不要包含额外的列，像select * 这种写法应该尽量避免。\n分页优化 在数据量比较大，分页比较深的情况下，需要考虑分页的优化。\n例如：\nselect * from table where type = 2 and level = 9 order by id asc limit 190289,10; 优化方案：\n延迟关联\n先通过where条件提取出主键，在将该表与原数据表关联，通过主键id提取数据行，而不是通过原来的二级索引提取数据行\n例如：\nselect a.* from table a, (select id from table where type = 2 and level = 9 order by id asc limit 190289,10 ) b where a.id = b.id 书签方式\n书签方式就是找到limit第一个参数对应的主键值，根据这个主键值再去过滤并limit\n例如：\nselect * from table where id \u0026gt; (select * from table where type = 2 and level = 9 order by id asc limit 190 索引优化 合理地设计和使用索引，是优化慢SQL的利器。\n利用覆盖索引\nInnoDB使用非主键索引查询数据时会回表，但是如果索引的叶节点中已经包含要查询的字段，那它没有必要再回表查询了，这就叫覆盖索引\n例如对于如下查询：\nselect name from test where city=\u0026#39;上海\u0026#39; 我们将被查询的字段建立到联合索引中，这样查询结果就可以直接从索引中获取\nalter table test add index idx_city_name (city, name); 低版本避免使用or查询\n在 MySQL 5.0 之前的版本要尽量避免使用 or 查询，可以使用 union 或者子查询来替代，因为早期的 MySQL 版本使用 or 查询可能会导致索引失效，高版本引入了索引合并，解决了这个问题。\n避免使用 != 或者 \u0026lt;\u0026gt; 操作符\nSQL中，不等于操作符会导致查询引擎放弃查询索引，引起全表扫描，即使比较的字段上有索引\n解决方法：通过把不等于操作符改成or，可以使用索引，避免全表扫描\n例如，把column\u0026lt;\u0026gt;’aaa’，改成column\u0026gt;’aaa’ or column\u0026lt;’aaa’，就可以使用索引了\n适当使用前缀索引\n适当地使用前缀所云，可以降低索引的空间占用，提高索引的查询效率。\n比如，邮箱的后缀都是固定的“@xxx.com”，那么类似这种后面几位为固定值的字段就非常适合定义为前缀索引\nalter table test add index index2(email(6)); PS:需要注意的是，前缀索引也存在缺点，MySQL无法利用前缀索引做order by和group by 操作，也无法作为覆盖索引\n避免列上函数运算\n要避免在列字段上进行算术运算或其他表达式运算，否则可能会导致存储引擎无法正确使用索引，从而影响了查询的效率\nselect * from test where id + 1 = 50; select * from test where month(updateTime) = 7; 正确使用联合索引\n使用联合索引的时候，注意最左匹配原则。\nJOIN优化 优化子查询\n尽量使用 Join 语句来替代子查询，因为子查询是嵌套查询，而嵌套查询会新创建一张临时表，而临时表的创建与销毁会占用一定的系统资源以及花费一定的时间，同时对于返回结果集比较大的子查询，其对查询性能的影响更大\n小表驱动大表\n关联查询的时候要拿小表去驱动大表，因为关联的时候，MySQL内部会遍历驱动表，再去连接被驱动表。\n比如left join，左表就是驱动表，A表小于B表，建立连接的次数就少，查询速度就被加快了。\nselect name from A left join B ; 适当增加冗余字段\n增加冗余字段可以减少大量的连表查询，因为多张表的连表查询性能很低，所有可以适当的增加冗余字段，以减少多张表的关联查询，这是以空间换时间的优化策略\n避免使用JOIN关联太多的表\n《阿里巴巴Java开发手册》规定不要join超过三张表，第一join太多降低查询的速度，第二join的buffer会占用更多的内存。\n如果不可避免要join多张表，可以考虑使用数据异构的方式异构到ES中查询。\n排序优化 利用索引扫描做排序\nMySQL有两种方式生成有序结果：其一是对结果集进行排序的操作，其二是按照索引顺序扫描得出的结果自然是有序的\n但是如果索引不能覆盖查询所需列，就不得不每扫描一条记录回表查询一次，这个读操作是随机IO，通常会比顺序全表扫描还慢\n因此，在设计索引时，尽可能使用同一个索引既满足排序又用于查找行\n例如：\n--建立索引（date,staff_id,customer_id） select staff_id, customer_id from test where date = \u0026#39;2010-01-01\u0026#39; order by staff_id,customer_id; 只有当索引的列顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向都一样时，才能够使用索引来对结果做排序\nUNION优化 条件下推\nMySQL处理union的策略是先创建临时表，然后将各个查询结果填充到临时表中最后再来做查询，很多优化策略在union查询中都会失效，因为它无法利用索引\n最好手工将where、limit等子句下推到union的各个子查询中，以便优化器可以充分利用这些条件进行优化\n此外，除非确实需要服务器去重，一定要使用union all，如果不加all关键字，MySQL会给临时表加上distinct选项，这会导致对整个临时表做唯一性检查，代价很高。\n索引优化详细讲讲 常见优化索引的方法：\n**前缀索引优化：**使用前缀索引是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。在一些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小。\n**覆盖索引优化：**覆盖索引是指 SQL 中 query 的所有字段，在索引 B+Tree 的叶子节点上都能找得到的那些索引，从二级索引中查询得到记录，而不需要通过聚簇索引查询获得，可以避免回表的操作。\n主键索引最好是自增的：\n如果我们使用自增主键，那么每次插入的新数据就会按顺序添加到当前索引节点的位置，不需要移动已有的数据，当页面写满，就会自动开辟一个新页面。因为每次插入一条新记录，都是追加操作，不需要重新移动数据，因此这种插入数据的方法效率非常高。 如果我们使用非自增主键，由于每次插入主键的索引值都是随机的，因此每次插入新的数据时，就可能会插入到现有数据页中间的某个位置，这将不得不移动其它数据来满足新数据的插入，甚至需要从一个页面复制数据到另外一个页面，我们通常将这种情况称为页分裂。页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率。 防止索引失效：\n当我们使用左或者左右模糊匹配的时候，也就是like %xx或者like %xx%这两种方式都会造成索引失效； 当我们在查询条件中对索引列做了计算、函数、类型转换操作，这些情况下都会造成索引失效； 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。 在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。 查询字符串时没加引号 有什么优化索引的方法? 这里说一下几种常见优化索引的方法：\n前缀索引优化； 覆盖索引优化； 主键索引最好是自增的； 防止索引失效 以下是一些优化索引的方法：\n**选择适当的索引列是优化索引的关键。**根据查询频率和过滤条件的选择，选择那些经常用于查询和过滤的字段作为索引列。 **对于聚集索引，选择合适的列作为索引列，通常选择主键或常用的查询字段。**聚集索引决定了数据在磁盘上的物理排序方式，优化聚集索引可以减少磁盘I/O操作和数据块的访问。 **对于常用的多个字段组合进行查询的情况，创建组合索引。**但要注意组合索引的字段排列顺序 ，把散列性高(区分度高)的值放在前面 避免在同一列上创建冗余的索引。 过长的字段，使用前缀索引。 如何定位慢查询？ 介绍产生问题的场景（一个接口测试，压测的结果大概5秒钟）\n使用开源工具（没用过就直接回答第三条）\n调试工具：Arthas 运维工具：Prometheus、Skywalking 开启MySQL自带慢查询日志： 慢查询日志记录了所有执行时间超过指定参数（long_query_time，单位：秒，默认10秒）的所有SQL语句的日志。\n开启慢查询日志：在MySQL的配置文件/etc/my.cnf中配置信息：\n# 开启MySQL慢日志查询开关 slow_query_log=1 # 设置慢日志的时间为2秒，即SQL执行时间超过2秒就会记录在慢查询日志中 long_query_time=2 配置完成后，重新启动MySQL服务器进行测试\n通过查询慢日志文件/var/lib/mysql/local-slow.log中记录的信息（实时显示）\n如何对该慢查询SQL进行分析呢？=\u0026gt;explain执行计划\n慢查询怎么优化 优化慢查询是提高数据库性能和响应时间的重要任务。下面是一些常用的慢查询优化方法：\n使用索引：**确保数据库表中的频繁查询字段和关联字段都有适当的索引。**索引可以加快查询速度并减少数据扫描的开销。但需要注意，过多的索引可能会增加写操作的开销，因此需要在索引设计上进行权衡。\n优化查询语句：**分析并优化查询语句，避免不必要的查询和数据处理操作。**可以使用 EXPLAIN 语句来查看查询执行计划，并识别需要优化的地方，如全表扫描、使用临时表等。注意避免使用负向查询、模糊查询等影响性能的操作。\n避免跨表关联查询：跨表关联查询会增加查询的复杂度和开销。可以通过冗余字段或者使用NoSQL数据库等方式来减少关联查询的次数。\n分页查询优化：对于需要分页查询的场景，避免对整个数据集进行查询和排序，可以使用 LIMIT 和 OFFSET 语句来限制返回的结果集，并在需要时创建合适的索引。\n优化数据库配置：合理配置数据库参数，如调整缓存大小、调整连接池大小、优化日志设置等。确保数据库能够充分利用硬件资源，并适应实际负载。\n缓存数据：对于一些频繁查询但数据变动较少的场景，可以通过缓存工具（如Redis、Memcached等）缓存查询结果，减少数据库访问。\n水平拆分或垂直拆分：如果数据库表过大，可以考虑拆分成多个小表，提高查询效率。可以根据业务场景进行水平拆分（按照数据行进行拆分）或垂直拆分（按照列进行拆分）。\n使用合适的硬件和系统资源：确保数据库服务器具备足够的内存、CPU和磁盘空间，以及适合的操作系统和文件系统设置。\n以上仅是一些常用的慢查询优化方法，实际优化过程需要根据具体情况进行分析和调整。可以通过监控工具、数据库日志和性能分析工具等进行性能分析，找出慢查询的根本原因，并采取相应的优化措施。同时，持续监测和优化数据库性能是保持系统高效运行的重要工作。\n什么是MySQL 执行计划？如何获取执行计划并对其进行分析？ 说明：无需记住所有的字段的含义，建议举例分析如何定位慢查询\nMySQL 执行计划是指 MySQL 查询优化器生成的一份详细的查询执行计划，它展示了 MySQL 在执行查询时所采取的具体执行计划，包括表的访问顺序、数据读取方式、使用的索引、使用的排序方式等等。通过分析执行计划，可以帮助我们找出查询性能瓶颈所在，进而进行优化，提高查询效率。\n要获取执行计划，可以在执行查询 SQL 语句时在前面添加 explain 关键字，例如：\nexplain select * from table where id = 1; 这样，MySQL 会输出该查询语句的执行计划。\n执行计划中的各个字段含义如下：\nid：每个 Select 子句或者是一个操作符或者是一个查询语句。 select_type：查询类型，表示查询的类型（简单查询、联合查询、子查询等等）。 type：访问类型，表示 MySQL 在表中找到所需行的方式。（常见的有NULL、system、const、ref、range、index、all） possible_keys：表示查询可能使用到的索引。 key：实际使用到的索引。 key_len：使用的索引长度。索引占用大小 rows：根据表统计信息及索引选用情况，大致估算出找到所需的记录所需要读取的行数。 filtered：返回结果的行数占总行数的比例。越大越好 Extra：包含 MySQL 解决查询的详细信息。 Extra字段含义：\nexplain分析执行计划时，需要注意以下几个方面：\n扫描行数：rows 字段，表示查询所需扫描的行数，如果该值过大，说明查询效率不高，需要优化。 使用索引：key 字段，表示查询所使用的索引，如果没有使用索引或者使用的不是最优索引，需要考虑优化。 优化空间：type字段，可以分析出是否还有进一步的优化空间，是否存在全索引扫描或者全表扫描 排序：Extra 字段，分析会否出现了回表的情况，如果出现了可以尝试添加索引或者修改返回字段优化 通过分析执行计划，可以确定查询优化的方向和方法，提高查询效率。\n锁 什么是悲观锁和乐观锁，它们在并发控制中有什么区别？ 悲观锁（Pessimistic Locking）和乐观锁（Optimistic Locking）是并发控制中常用的两种策略，用于解决并发访问数据时的冲突问题。\n悲观锁：\n悲观锁的核心思想是假设会出现并发冲突，因此在访问共享资源之前会先获取锁，确保自己独占资源。 当一个事务获取了悲观锁后，其他事务必须等待该事务释放锁才能访问资源，从而保证了资源的独占性。 常见的悲观锁实现方式有MySQL的行级锁和表级锁、Java的synchronized关键字、ReentrantLock等。 悲观锁的缺点是会降低并发性能，因为它要求事务在访问数据之前先获得锁，并且可能会导致锁竞争和死锁问题 乐观锁：\n乐观锁的核心思想是认为并发冲突的概率较低，因此在访问共享资源时并不会立即加锁，而是在提交事务前才会检测是否有冲突。 当一个事务要修改某个数据时，它不会主动获取独占锁，而是在事务提交前检查数据是否被其他事务修改过。 乐观锁的实现通常使用版本号（Versioning）或时间戳（Timestamp）机制。每个数据都有一个与之关联的版本号或时间戳，事务在修改数据时检查版本号或时间戳是否匹配。 如果版本号或时间戳匹配，则说明没有冲突，事务可以继续进行操作；如果不匹配，则说明有冲突发生，事务需要进行回滚或重试操作。 乐观锁的优点是在大多数情况下不会阻塞其他事务，因此并发性能较高。但如果发生冲突，可能会导致事务失败并需要进行重试。 常见的乐观锁实现方式有数据库的MVCC（多版本并发控制）、Java的CAS（Compare and Swap）等。 悲观锁和乐观锁的应用场景\n悲观锁适用于并发冲突概率较高的场景，适合长时间占有资源或需要较长事务的情况。 而乐观锁适用于并发冲突概率较低的场景，适合短时间占有资源或有较短事务的情况。 总结： 悲观锁和乐观锁在并发控制中的区别主要在于对并发冲突的处理方式。悲观锁默认假设会有冲突发生，因此在访问数据之前获取独占锁，保证数据的一致性和并发安全性。乐观锁则默认假设不会有冲突发生，不主动获取独占锁，而是在提交前检查数据是否发生冲突。悲观锁会导致较低的并发性能和可能的锁竞争问题，而乐观锁在大多数情况下具有较高的并发性能，但可能需要处理冲突并进行重试操作。选择何种锁策略取决于应用程序的需求和对并发性能和数据一致性的权衡。\nMySQL中有几种锁？ 按照锁的粒度来分，分为：\n全局锁：锁定数据库中的所有表。 表级锁：每次操作锁住整张表。 行级锁：每次操作锁住对应的行数据 全局锁 全局锁就是对整个数据库实例加锁，加锁后整个实例就处于只读状态，后续的DML的写语句，DDL语句，已经更新操作的事务提交语句都将被阻塞。\n其典型的使用场景是做全库的逻辑备份，对所有的表进行锁定，从而获取一致性视图，保证数据的完整性。\n-- 加全局锁 flush tables with read lock; -- 数据备份（要在非加锁的窗口执行，不要登录MySQL数据库) mysqldump -uroot -p1234 db01 \u0026gt; D:/db01.sql mysqldump -h192.168.235.128 -uroot -p1234 db01 \u0026gt; D:/db01.sql -- 释放锁 unlock tables; 表级锁 表级锁，每次操作锁住整张表。锁定粒度大，发生锁冲突的概率最高，并发度最低。应用在MyISAM、InnoDB、BDB等存储引擎中\n表级锁主要分为：\n表锁\n元数据锁（meta data lock，MDL）\n意向锁\n表锁\n-- 加表读（写）锁 lock tables 表名 read/write -- 释放锁 unlock tables; / 客户端断开连接 表锁分为共享锁（读锁read）和独占锁（写锁write）\n当前客户端加读锁，该客户端可以读，其他客户端只能读不能写 当前客户端加写锁，该客户端可以读和写，其他客户端不能读和写 元数据锁（meta data lock, 简称MDL）\nMDL加锁过程是系统自动控制，无需显式使用，在访问一张表的时候会自动加上。MDL锁主要作用是维护表元数据的数据一致性，在表上有活动事务的时候，不可以对元数据进行写入操作。为了避免DML与DDL冲突，保证读写的正确性。\n这里的元数据就是一张表的表结构。 也就是说，某一张表涉及到未提交的事务时，是不能够修改这张表的表结构的。\n在MySQL5.5中引入了MDL，当对一张表进行增删改查的时候，加MDL读锁(共享)；当对表结构进行变更操作的时候，加MDL写锁(排他)。\n元数据锁可以防止你在查询数据时表结构被别人更改\n开启事务后，进行增删改查操作会自动加上元数据锁，其他客户端不能修改表结构\n意向锁\n为了避免DML在执行时，加的行锁与表锁的冲突，在InnoDB中引入了意向锁，使得表锁不用检查每行数据是否加锁，使用意向锁来减少表锁的检查。\n在使用 InnoDB 引擎的表里对某些记录加上「共享锁」之前，需要先在表级别加上一个「意向共享锁」；意向共享锁（IS）与表级锁读锁兼容，与表级锁写锁互斥\n在使用 InnoDB 引擎的表里对某些纪录加上「独占锁」之前，需要先在表级别加上一个「意向独占锁」；意向排它锁（IX）与表级锁读锁和写锁都互斥\n也就是，当执行插入、更新、删除操作，需要先对表加上「意向独占锁」，然后对该记录加独占锁。\n而普通的 select 是不会加行级锁的，普通的 select 语句是利用 MVCC 实现一致性读，是无锁的。\n不过，select 也是可以对记录加共享锁和独占锁的，具体方式如下：\n//先在表上加上意向共享锁，然后对读取的记录加共享锁 select ... lock in share mode; //先表上加上意向独占锁，然后对读取的记录加独占锁 select ... for update; 意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁（lock tables \u0026hellip; read）和独占表锁（lock tables \u0026hellip; write）发生冲突。\n表锁和行锁是满足读读共享、读写互斥、写写互斥的。\n如果没有「意向锁」，那么加「独占表锁」时，就需要遍历表里所有记录，查看是否有记录存在独占锁，这样效率会很慢。\n那么有了「意向锁」，由于在对记录加独占锁前，先会加上表级别的意向独占锁，那么在加「独占表锁」时，直接查该表是否有意向独占锁，如果有就意味着表里已经有记录被加了独占锁，这样就不用去遍历表里的记录。\n所以，意向锁的目的是为了快速判断表里是否有记录被加锁\n行级锁 行级锁，每次操作锁住对应的行数据，锁定粒度最小，发生锁冲突的概率最低，并发度最高。应用InnoDB存储引擎中\nInnoDB的数据是基于索引组织的，行锁是通过对索引上的索引项加锁来实现的，而不是对记录加的锁。对于行级\n锁，主要分为以下三类：\n行锁（Record Lock）：锁定单个行记录的锁，防止其他事务对此行进行update和delete。在RC、RR隔离级别下都支持（RR隔离级别是指\u0026quot;可重复读\u0026quot;（Repeatable Read）级别） 间隙锁（Gap Lock）：锁定索引记录间隙（不含该记录），确保索引记录间隙不变，防止其他事务在这个间隙进行insert，产生幻读。在RR隔离级别下都支持 临键锁（Next-Key Lock）：行锁和间隙锁组合，同时锁住数据，并锁住数据前面的间隙Gap。在RR隔离级别下支持 InnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁。\n行锁 //对读取的记录加共享锁 select ... lock in share mode; //对读取的记录加独占锁 select ... for update; InnoDB实现了以下两种类型的行锁：\n共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排它锁 排他锁（X）：允许获取排他锁的事务更新数据，阻止其他事务获得相同数据集的共享锁和排他锁 Record Lock 称为记录锁，锁住的是一条记录。而且记录锁是有 S 锁和 X 锁之分的：\n当一个事务对一条记录加了 S 型记录锁后，其他事务也可以继续对该记录加 S 型记录锁（S 型与 S 锁兼容），但是不可以对该记录加 X 型记录锁（S 型与 X 锁不兼容）;\n当一个事务对一条记录加了 X 型记录锁后，其他事务既不可以对该记录加 S 型记录锁（S 型与 X 锁不兼容），也不可以对该记录加 X 型记录锁（X 型与 X 锁不兼容）。\n间隙锁\n间隙锁（Gap Lock）是在一个范围之间应用的，阻止其他事务在范围内插入新数据。这样可以避免幻读的发生。\n假设，表中有一个范围 id 为（3，5）间隙锁，那么其他事务就无法插入 id = 4 这条记录了，这样就有效的防止幻读现象的发生。\n间隙锁唯一目的是防止其他事务插入间隙。间隙锁可以共存，一个事务采用的间隙锁不会阻止另一个事务在同一间隙上采用间隙锁。\n临键锁 Next-Key Lock 称为临键锁，是 Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。\n假设，表中有一个范围 id 为（3，5] 的 next-key lock，那么其他事务即不能插入 id = 4 记录，也不能修改 id = 5 这条记录。\n所以，next-key lock 即能保护该记录，又能阻止其他事务将新纪录插入到被保护记录前面的间隙中。\nnext-key lock 是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的。\n意向锁是什么？作用？表级锁还是行级锁？ 意向锁是一个表级锁\n意向锁的出现是为了支持 InnoDB 的多粒度锁，它解决的是DML语句中表锁和行锁共存的问题。\n当我们需要给一个表加表锁的时候，我们需要根据去判断表中有没有数据行被锁定，以确定是否能加成功。\n假如没有意向锁，那么我们就得遍历表中所有数据行来判断有没有行锁；效率低\n有了意向锁这个表级锁之后，则我们直接判断一次就知道表中是否有数据行被锁定了。\n有了意向锁之后，要执行的事务 A 在申请行锁（写锁）之前，数据库会自动先给事务 A 申请表的意向排他锁。当事务 B 去申请表的互斥锁时就会失败，因为表上有意向排他锁之后事务 B 申请表的互斥锁时会被阻塞。\n意向锁是什么？\n在使用 InnoDB 引擎的表里时对某些记录加上「共享锁」之前，需要先在表级别加上一个「意向共享锁」 在使用 InnoDB 引擎的表里时对某些记录加上「独占锁」之前，需要先在表级别加上一个「意向独占锁」 也就是，当执行插入、更新、删除操作，需要先对表加上「意向独占锁」，然后对该记录加独占锁。\n意向锁的作用\n意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁（lock tables \u0026hellip; read）和独占表锁（lock tables \u0026hellip; write）发生冲突。\n表锁和行锁是满足读读共享、读写互斥、写写互斥的。\n作用：为了快速判断表里是否有记录被加锁\n如果没有「意向锁」，那么加「独占表锁」时，就需要遍历表里所有记录，查看是否有记录存在独占锁，这样效率会很慢。 那么有了「意向锁」，由于在对记录加独占锁前，先会加上表级别的意向独占锁，那么在加「独占表锁」时，直接查该表是否有意向独占锁，如果有就意味着表里已经有记录被加了独占锁，这样就不用去遍历表里的记录。 普通的 select 是不会加行级锁的，普通的 select 语句是利用 MVCC 实现一致性读，是无锁的。\n不过，select 也是可以对记录加共享锁和独占锁的，具体方式如下：\n//先在表上加上意向共享锁，然后对读取的记录加共享锁 select ... lock in share mode; //先表上加上意向独占锁，然后对读取的记录加独占锁 select ... for update; 介绍一下MySQL的锁机制。 MySQL的锁机制主要包括两种类型的锁：共享锁（Shared Lock）和排他锁（Exclusive Lock）。这些锁用于控制对数据库中数据的并发访问，确保数据的一致性和完整性。\n共享锁（Shared Lock）： 共享锁允许多个会话同时持有相同的锁并读取数据，共享锁之间不会互相阻塞。 共享锁适用于读操作，可以防止其他会话对数据进行写操作，但不阻止其他会话的读操作。 共享锁使用语句：SELECT ... LOCK IN SHARE MODE; 或 SELECT ... FOR SHARE; 排他锁（Exclusive Lock）： 排他锁只允许一个会话独占地持有锁并修改数据，排他锁与其他锁（共享锁或排他锁）之间互斥。 排他锁适用于写操作，当一个会话持有排他锁时，其他会话无法读取或写入相同的数据。 排他锁使用语句：SELECT ... FOR UPDATE; 或 UPDATE ...; 或 DELETE ...; MySQL还使用了其他类型的锁来控制并发访问：\n记录锁（Record Lock）：也称为行级锁，用于在某个会话中锁定数据的特定记录，以防止其他会话修改该记录。 间隙锁（Gap Lock）：用于在某个会话中锁定索引范围之间的间隙，以防止其他会话在范围内插入新记录。 临键锁（Next-Key Lock）：结合了记录锁和间隙锁，用于同时锁定索引记录和索引范围之间的间隙，以防止其他会话插入新记录或修改已存在的记录。 MySQL还提供了事务和事务隔离级别来管理并发访问和锁的使用。通过合理地使用锁和事务，可以保证数据的一致性、完整性和并发访问的正确性。\n什么是共享锁和排他锁？它们的区别是什么？ 共享锁（Shared Lock）和排他锁（Exclusive Lock）是MySQL中的两种基本锁类型，它们的主要区别如下：\n共享锁（Shared Lock）：\n共享锁允许多个会话同时持有相同的锁并读取数据，共享锁之间不会互相阻塞。 共享锁适用于读操作，它允许多个并发的读操作同时进行，提高了并发性能。 共享锁不会阻止其他会话获取共享锁，因为共享锁之间是兼容的。共享锁与排它锁之间互斥。 排他锁（Exclusive Lock）：\n排他锁只允许一个会话独占地持有锁并修改数据，排他锁与其他锁（共享锁或排他锁）之间互斥。 排他锁适用于写操作，当一个会话持有排他锁时，其他会话无法读取或写入相同的数据。 排他锁可以阻止其他会话获取共享锁和排他锁，因为它与其他锁是互斥的。 总结：\n共享锁用于并发读操作，允许多个会话同时持有相同的锁，并发性能较高。 排他锁用于独占写操作，只允许一个会话持有锁，确保数据的独占性和一致性。 共享锁和排他锁之间是互斥的，一个会话持有排他锁时，其他会话无法获取共享锁或排他锁。 共享锁之间是兼容的，多个会话可以同时持有共享锁并进行读操作。 通过在读写操作中合理地使用共享锁和排他锁，可以确保数据的一致性和并发访问的正确性。在并发环境中，共享锁和排他锁的正确使用可以提高数据库系统的性能和数据的完整性。\n请介绍一下行锁和表锁，它们的特点和适用场景是什么？ 行锁（Row Lock）和表锁（Table Lock）是MySQL中常用的锁级别，它们具有不同的特点和适用场景：\n行锁（Row Lock）：\n行锁是对数据行进行加锁，仅针对锁定的行进行并发控制，允许其他事务并发地访问其他行数据。 行锁的特点是粒度小，只锁定需要修改的行，减少了锁的冲突，提高并发性能。 行锁适用于多事务并发修改同一表的场景，特别是在高并发环境下，可以最大程度地减少锁的冲突。 表锁（Table Lock）：\n表锁是对整个表进行加锁，锁定整个表，其他事务无法同时修改表中的任何行。 表锁的特点是粒度大，锁定整个表，可能导致其他事务无法并发地修改表中的其他行数据。 表锁适用于只有少数事务并发访问同一表且事务之间修改的数据行没有冲突的场景，可以避免复杂的锁管理操作。 总结：\n行锁适用于高并发、多事务同时操作同一表且数据行冲突较多的场景，可以提高并发性能。 表锁适用于少数事务并发访问同一表且事务之间修改的数据行没有冲突的场景，操作简单但可能导致并发性能下降。 在MySQL中，默认使用行级锁来实现并发控制，通过合理的设计和配置，可以在保证数据的一致性和完整性的同时，提高数据库系统的并发性能。需要根据具体的应用场景和并发访问情况选择适当的锁级别和锁策略。\n什么是死锁？如何避免和解决死锁问题？ 死锁（Deadlock）是指两个或多个事务互相等待对方释放资源，导致所有事务无法继续执行的情况。当发生死锁时，没有任何事务可以继续执行，只能通过干预来解除死锁。\n死锁的常见情况可以描述为以下四个条件的同时发生：\n互斥条件（Mutual Exclusion）：资源只能被一个事务独占，其他事务无法同时访问。 请求与保持条件（Hold and Wait）：事务在持有一部分资源的同时，又请求其他事务持有的资源。 不剥夺条件（No Preemption）：资源只能由持有者主动释放，而不能被其他事务强制性剥夺。 循环等待条件（Circular Wait）：存在一个事务的资源请求链，形成一个循环等待的关系。 为了避免和解决死锁问题，可以采取以下几种方法：\n设置合理的事务隔离级别：合理选择事务隔离级别，例如读已提交（Read Committed）或可重复读（Repeatable Read），避免较高隔离级别下的幻读和不可重复读问题。 优化数据库设计和查询语句：减少事务持有锁的时间，避免长事务和长时间的查询操作，尽量缩短事务执行时间。 使用合理的锁粒度：在事务中尽量只锁定需要修改的行，减小锁的范围，以降低死锁的概率。 避免事务中多次加锁的顺序不一致：多个事务在操作相同的资源时，尽量按照相同的顺序加锁，避免不一致的加锁顺序导致死锁的发生。 设置合理的超时时间和重试机制：当发现死锁时，可以设置合理的超时时间，并尝试回滚事务并重新执行，避免事务一直阻塞。 监控和检测死锁：通过监控和检测机制来发现死锁的发生，及时进行干预和处理。 使用死锁检测和解除工具：数据库管理系统提供了死锁检测和解除的工具，可以帮助识别和解除死锁情况。 以上方法可以帮助避免和解决死锁问题，提高数据库系统的并发性能和可靠性。在设计和开发过程中，需要充分考虑并发访问和锁的使用，以减少死锁的发生。\n什么是数据库的并发控制？请讨论一下并发控制的方法和技术。 数据库的并发控制是指在多个并发执行的事务访问和修改数据库时，通过一定的方法和技术来确保数据的一致性、完整性和并发性能。并发控制旨在解决并发访问可能引发的数据不一致或冲突的问题。\n以下是一些常见的数据库并发控制方法和技术：\n锁机制： 乐观锁：在读取数据时不加锁，而是在更新数据时进行冲突检测。 悲观锁：在读取数据时先加锁，以防止其他事务对数据进行修改。 事务隔离级别： 读未提交（Read Uncommitted）：允许脏读，事务可以读取未提交的数据。 读已提交（Read Committed）：保证一个事务读取的数据是其他事务已提交的数据。 可重复读（Repeatable Read）：保证一个事务多次读取同一数据时的结果一致。 串行化（Serializable）：最高隔离级别，保证事务串行执行，避免并发冲突。 多版本并发控制（MVCC）： 使用版本号或时间戳来标记事务读取的数据版本，从而实现并发读取而不会产生冲突。 读操作可以读取已提交的数据版本，写操作会创建新的数据版本，并更新相应的版本号。 两阶段锁协议（Two-Phase Locking，2PL）： 2PL是基于锁的并发控制方法，包括加锁阶段和解锁阶段。 在加锁阶段，事务会获取所有需要的锁，并且不会释放任何锁。 在解锁阶段，事务会释放所有的锁，使其他事务可以访问相应的资源。 死锁检测和解除： 通过监控事务之间的等待关系，检测并发执行中可能发生的死锁情况。 采用死锁检测算法，例如图论算法（如有向图检测算法）来检测死锁，然后解除死锁。 乐观并发控制（Optimistic Concurrency Control，OCC）： 基于乐观锁的并发控制方法，假设并发冲突较少，避免显式加锁。 在提交事务之前，检查是否有其他事务对数据进行了修改，若没有则提交，否则进行回滚或重试。 并发控制方法和技术的选择取决于应用的具体需求和性能要求。合理的并发控制可以提高数据库系统的并发性能、数据一致性和完整性。在设计和开发数据库应用时，需要考虑并发访问情况，选择适合的并发控制策略。\n请解释一下数据库中的间隙锁（Gap Lock）和Next-Key Lock的概念和作用。 间隙锁（Gap Lock）和Next-Key Lock是MySQL中用于并发控制的锁类型，用于保护索引范围之间的间隙和记录之间的关系。它们的作用是为了防止并发事务在索引范围内插入新记录或修改已存在的记录。\n间隙锁（Gap Lock）： 间隙锁是一种锁定索引范围之间的间隙的锁，不包括实际的记录。它会锁定一个索引范围，以防止其他事务在范围内插入新的记录。 间隙锁的作用是为了防止幻读（Phantom Read）问题，即在同一范围内的多个事务插入新的记录，导致其他事务的查询结果出现不一致。 间隙锁在读已提交（Read Committed）隔离级别下起作用，会阻止其他事务在锁定的范围内插入新的记录。 Next-Key Lock： Next-Key Lock是间隙锁的扩展，它不仅锁定索引范围之间的间隙，还会锁定实际的记录，保护了记录之间的关系。 Next-Key Lock是行级锁（Record Lock）和间隙锁（Gap Lock）的组合，可以防止幻读和修改已存在的记录。 Next-Key Lock在可重复读（Repeatable Read）隔离级别下起作用，它会锁定索引范围内的间隙和实际的记录，阻止其他事务在范围内插入新的记录或修改已存在的记录。 间隙锁和Next-Key Lock在并发事务中起到重要的作用，保护了数据的一致性和完整性。通过锁定索引范围之间的间隙和实际的记录，它们防止了并发事务在同一范围内插入或修改数据，避免了幻读和不一致的查询结果。需要注意的是，间隙锁和Next-Key Lock可能会导致并发性能的下降，因为它们在一定程度上限制了其他事务的操作。因此，在设计和开发数据库应用时，需要根据具体的业务需求和并发访问情况来考虑使用间隙锁和Next-Key Lock的合理性。\n日志 MySQL 日志有了解过吗？binlog、redolog、undolog 分别有什么作用、有什么区别？ 来自：编程导航官方\nMySQL 是一款流行的关系型数据库，其日志是其关键功能之一。MySQL 包括三种类型的日志，分别是 binlog、redolog 和 undolog，它们分别有不同的作用和特点。\nbinlog（Binary log）是 MySQL 中的二进制日志文件，用于记录 MySQL 服务器上的所有更新和修改操作。它可以记录所有的 DDL（Data Definition Language）和 DML（Data Modification Language）操作，包括对表结构的更改、数据的插入、修改、删除等等。binlog是在事务提交后生成的，因此可以用于恢复数据库。 redo log（Redo log）用于恢复数据，保证数据的持久性。当 MySQL 发生修改时，redolog 会将这些操作记录下来，并写入磁盘。这样，当 MySQL 发生宕机或崩溃时，通过重放 redolog 就可以恢复数据。 undo log（Undo log）用于回滚操作。记录的是事务操作的逆操作，比如执行了insert，那么在undolog中就会记录一条delete，用于事务的回滚。当 MySQL 发生事务回滚时，undolog 会记录这些操作并将其写入磁盘。这样，当 MySQL 需要回滚时，**通过重放 undolog 就可以回滚事务。**保证了事务的原子性和一致性 区别：\nbinlog 和 redolog 都是 MySQL 中的二进制日志，但是它们的作用和实现方式有所不同。binlog 是 MySQL 记录所有的操作，而 redolog 则是用于保证数据的一致性和持久性。此外，binlog 是逻辑日志，redolog 是物理日志。binlog 记录的是SQL语句，而 redolog 记录的是数据页的修改，所以 binlog 可以跨平台使用，而 redolog 不能。undolog 和 redolog 的区别是，undolog 是用于回滚操作的，而 redolog 是用于恢复数据的。\n运维 请介绍数据库的主从复制和读写分离。 数据库的主从复制和读写分离是常见的数据库架构技术，用于提高数据库的性能、可用性和可伸缩性。它们的主要原理和功能如下：\n·主从复制（Master-Slave Replication） 主从复制是一种数据库复制技术，其中一个数据库服务器（主服务器）作为数据源，将其更新操作同步到一个或多个备份服务器（从服务器）。主服务器负责处理写操作（INSERT、UPDATE、DELETE），而从服务器负责复制主服务器的数据，并处理读操作（SELECT）。\n主从复制的主要目的是提高数据库的可用性和数据备份。通过将读操作分散到从服务器上，主服务器可以专注于处理写操作，从而提高系统的并发性和性能。此外，从服务器可以作为灾难恢复的备份，当主服务器发生故障时，可以快速切换到从服务器以保持系统的可用性。\n读写分离（Read-Write Splitting） 读写分离是一种数据库架构模式，将数据库的读操作和写操作分离到不同的服务器上。其中一个数据库服务器（主服务器）负责处理写操作，而多个数据库服务器（从服务器）负责处理读操作。\n读写分离的主要目的是提高数据库的读操作性能。通过将读操作分摊到多个从服务器上，可以减轻主服务器的负载压力，提高整体的读写吞吐量。此外，读写分离还可以根据业务需求，对不同的从服务器进行负载均衡和故障恢复的调整。\n要实现主从复制和读写分离，通常会使用数据库系统的复制机制和代理工具，例如MySQL数据库可以使用MySQL复制和MySQL Proxy来实现。具体的配置和部署取决于数据库系统的不同，通常需要设置主服务器和从服务器之间的连接、复制规则、数据同步机制以及客户端的连接路由规则。\n需要注意的是，主从复制和读写分离虽然提供了性能和可用性的好处，但也引入了一定的复杂性和数据一致性的问题。在使用这些技术时，需要考虑数据同步的延迟、故障恢复的时间、并发写操作的冲突处理等因素，并根据实际情况进行合理的配置和调优。\n请解释MySQL的主从复制和读写分离，以及它们的作用和优势。 MySQL的主从复制（Master-Slave Replication）和读写分离（Read-Write Splitting）是数据库架构中常用的技术手段，用于提高数据库的性能、可用性和扩展性。\n主从复制：\n主从复制是指将一个MySQL数据库服务器（主服务器）的数据复制到其他一个或多个MySQL数据库服务器（从服务器）的过程。 主服务器负责处理写操作（INSERT、UPDATE、DELETE），并将写操作的日志（二进制日志）复制到从服务器。 从服务器接收主服务器的写操作日志，并将这些操作在自身上执行，从而保持与主服务器数据的同步。 主从复制的作用和优势：\n提高读取性能：通过将读操作分发到从服务器，减轻了主服务器的负载，提高了数据库的读取性能。读操作可以在多个从服务器上并行执行，提高了系统的并发性能。\n数据备份和灾难恢复：从服务器可以用作主服务器的备份，当主服务器发生故障时，可以快速切换到从服务器来保证系统的可用性和数据的完整性。\n分担主服务器压力：通过将读操作分发到从服务器，减少了主服务器的读负载，使主服务器可以专注于处理写操作，提高了主服务器的性能和响应能力。\n读写分离：\n读写分离是在主从复制的基础上进一步扩展，将读操作和写操作分离到不同的数据库服务器上。 主服务器继续负责处理写操作，从服务器负责处理读操作。 应用程序通过访问从服务器来执行读操作，从而分担了主服务器的读负载。 读写分离的作用和优势：\n提高读取性能：通过将读操作分发到从服务器，减轻了主服务器的负载，提高了数据库的读取性能和并发能力。\n分离读写压力：将读操作和写操作分离到不同的服务器上，使得读操作和写操作不再相互影响，提高了系统的稳定性和可靠性。\n数据一致性：读写分离可以确保读操作不会影响到主服务器的数据一致性。读操作只在从服务器上执行，不会对主服务器上的数据造成影响。\n通过主从复制和读写分离的组合应用，可以实现高性能、高可用性和可扩展性的数据库架构。主服务器负责处理写操作，保证数据的一致性和完整性，从服务器负责处理读操作，提高读取性能和并发能力。同时，通过多个从服务器的部署，可以提高系统的可用性和容灾能力。\n请简述如何备份和恢复数据库。 备份和恢复数据库是确保数据安全和持久性的重要任务。以下是备份和恢复数据库的一般步骤：\n数据库备份：\n选择合适的备份方法：根据数据库类型和需求，选择合适的备份方法，如物理备份或逻辑备份。 设定备份策略：确定备份的频率和保留周期，根据业务需求和数据变化的频率制定备份计划。 执行数据库备份：按照备份策略和选择的备份方法，执行数据库备份操作。 存储备份数据：将备份数据保存在可靠的存储介质上，如本地磁盘、网络存储或云存储等。 数据库恢复：\n检查备份完整性：确保备份文件完整可用，验证备份数据的完整性和一致性。 停止数据库服务：在恢复之前，停止数据库服务，确保数据库处于可恢复状态。 执行数据库恢复：根据备份类型和需求，执行相应的数据库恢复操作，包括完全恢复、差异恢复或增量恢复。 测试恢复结果：恢复后，验证数据库的完整性和可用性，确保数据正确恢复。 定期验证和测试：\n定期验证备份数据：定期检查备份数据的完整性和可用性，确保备份文件正常可用。 定期测试恢复流程：定期进行恢复测试，验证恢复流程的可行性和效果。 注意事项：\n备份和恢复过程中要确保数据的一致性和完整性，避免备份和恢复过程中的数据损失。 备份应存储在不同的位置，以防止备份数据丢失或存储介质发生故障。 恢复前要停止数据库服务，以防止在恢复过程中的数据冲突或不一致性。 数据库备份和恢复操作需要在恰当的时间窗口内进行，避免对业务运行造成过多的影响。 备份和恢复数据库是一项关键任务，应根据具体数据库管理系统的要求和最佳实践进行操作。建议参考数据库的官方文档和相关的备份和恢复指南来进行详细操作。\n备份和恢复数据库是非常重要的数据库管理任务，以下是一般的备份和恢复数据库的步骤：\n备份数据库： 选择备份的方式：可以使用物理备份（例如使用数据库管理系统提供的备份工具）或逻辑备份（通过导出数据库为 SQL 脚本）。 确定备份策略：选择完整备份、增量备份或差异备份等方式，根据需求决定频率和保留时间。 执行备份操作：根据选择的备份方式和策略，执行相应的备份操作，将数据库的数据和日志文件备份到指定的位置。 恢复数据库： 准备恢复环境：确保数据库管理系统已安装，并准备好用于恢复的备份文件。 关闭数据库：在开始恢复之前，关闭数据库以确保数据的一致性。 执行恢复操作：根据备份的类型和方式，执行相应的恢复操作。对于物理备份，可以使用数据库管理系统的恢复工具，对备份文件依次进行恢复。对于逻辑备份，可以运行导入 SQL 脚本的命令来恢复数据。 配置数据库：根据需要，配置数据库参数、权限和其他相关设置。 检查和测试：恢复完成后，对数据库进行必要的检查和测试，确保数据的完整性和可用性。 mysqldump命令可以用来备份数据库或者在不同数据库之间进行数据迁移，备份内容包括创建表、插入表的SQL语句\n语法 ： mysqldump [options] db_name [tables] mysqldump [options] --database/-B db1 [db2 db3...] mysqldump [options] --all-databases/-A 连接选项 ： -u, --user=name 指定用户名 -p, --password[=name] 指定密码 -h, --host=name 指定服务器ip或域名 -P, --port=# 指定连接端口 输出选项： --add-drop-database 在每个数据库创建语句前加上 drop database 语句 --add-drop-table 在每个表创建语句前加上 drop table 语句 , 默认开启 ; 不开启 (--skip-add-drop-table) -n, --no-create-db 不包含数据库的创建语句 -t, --no-create-info 不包含数据表的创建语句 -d --no-data 不包含数据 -T, --tab=name 自动生成两个文件：一个.sql文件，创建表结构的语句；一个.txt文件，数据文件 mysqlimport是客户端数据导入工具，用来导入mysqldump -T 导出的txt文件\n语法 ： mysqlimport [options] db_name textfile1 [textfile2...] 示例 ： -- 导入/tmp/city.txt文件 mysqlimport -uroot -p2143 test /tmp/city.txt 如何备份和恢复 MySQL 数据库？有哪些备份和恢复策略？ 备份和恢复MySQL数据库是确保数据安全和持久性的重要任务。以下是一些备份和恢复MySQL数据库的常见策略和方法：\n备份策略：\n完全备份（Full Backup）：将整个数据库备份到一个文件中。这是最基本的备份策略，可以用于全面恢复数据库。 增量备份（Incremental Backup）：仅备份自上次备份以来发生更改的部分数据。它依赖于完全备份和增量备份的组合，可以减少备份时间和存储空间。 备份方法：\n物理备份：直接备份数据库的物理文件，包括数据文件、日志文件和配置文件等。常用的物理备份工具包括mysqldump、mysqlbackup和第三方工具如Percona XtraBackup。 逻辑备份：通过导出数据库的逻辑结构和数据，生成SQL语句或数据文件进行备份。常用的逻辑备份工具包括mysqldump和mysqlpump。 备份存储：\n本地存储：将备份文件保存在本地磁盘上，提供快速访问和恢复。需要确保备份文件的安全性和可靠性。 远程存储：将备份文件上传到远程服务器、云存储或网络文件共享。提供数据的远程存储和灾备保护。 恢复策略：\n完全恢复（Full Restore）：使用完全备份文件进行恢复，将整个数据库恢复到备份时的状态。 增量恢复（Incremental Restore）：结合完全备份和增量备份文件，逐步恢复数据库的增量变化。 恢复方法：\n物理恢复：将物理备份文件还原到数据库服务器上，覆盖原有的数据文件和日志文件。这种方法可以快速恢复大型数据库。 逻辑恢复：通过执行逻辑备份的SQL语句或导入数据文件，将备份的数据重新加载到数据库中。 需要注意的是，备份和恢复数据库时应该：\n定期备份：根据业务需求和数据变化频率制定备份计划，确保及时备份关键数据。 验证备份完整性：定期验证备份文件的完整性和可用性，确保备份数据没有损坏。 存储安全性：备份文件应存储在安全的地方，防止数据泄露和丢失。 此外，还可以考虑其他高级备份和恢复策略，如主从复制、热备份、冷备份、数据复制和灾备方案等，以提高数据的可用性和可恢复性。根据实际情况和需求，选择合适的备份和恢复策略来保护MySQL数据库。\n请介绍几种常见的 SQL 注入攻击方式，以及如何防止 SQL 注入。 **SQL注入是一种常见的网络应用程序安全漏洞，攻击者通过构造恶意的SQL查询语句，利用应用程序的漏洞，来执行非法的数据库操作。**以下是几种常见的SQL注入攻击方式以及如何防止它们：\n基于字符串的注入：攻击者通过在用户输入的字符串中插入恶意的SQL代码来实现注入攻击。例如，在一个登录表单的用户名字段中输入\u0026quot;admin\u0026rsquo; OR \u0026lsquo;1\u0026rsquo;=\u0026lsquo;1\u0026rsquo;\u0026quot;，如果应用程序没有正确的处理和过滤用户输入，攻击者可能会成功绕过认证。\n防御措施：使用参数化查询（预处理语句）或者绑定变量来处理用户输入，而不是直接将用户输入的字符串拼接到SQL查询语句中。这样可以防止恶意的SQL代码被执行。\n基于数字的注入：类似于字符串注入，攻击者通过在用户输入的数字值中插入恶意的SQL代码来实现注入攻击。例如，在一个商品搜索的价格范围字段中输入\u0026quot;0 OR 1=1\u0026quot;，如果应用程序没有正确的处理和验证输入，攻击者可能会绕过价格过滤条件。\n防御措施：对于数字输入，应该使用类型检查和范围验证来确保输入的有效性。另外，可以将用户输入进行转义处理，确保特殊字符不会被解释为SQL代码。\n基于布尔的注入：攻击者通过利用应用程序在查询中使用布尔逻辑运算符（如AND、OR）的方式来实现注入攻击。例如，在一个用户搜索的查询中输入\u0026quot;admin\u0026rsquo; AND \u0026lsquo;1\u0026rsquo;=\u0026lsquo;1\u0026rsquo;\u0026quot;，如果应用程序没有正确的处理和构造查询，攻击者可能会绕过权限验证。\n防御措施：应该对用户输入进行适当的验证和过滤，并确保构造查询时使用正确的布尔逻辑运算符。另外，限制用户输入的长度和类型，可以降低注入攻击的风险。\n盲注入：盲注入是一种攻击方式，攻击者无法直接获取查询结果，但通过构造特定的SQL语句，利用应用程序的响应或者其他外部信号来推断出查询结果。攻击者可以使用时间延迟或者错误消息等方式进行盲注入攻击。\n防御措施：避免向用户返回明确的错误消息，尽量限制应用程序的响应时间。对于涉及敏感信息的查询，可以使用白名单或者其他安全机制进行限制。\n综合来说，以下是一些通用的防御措施来预防SQL注入攻击：\n使用参数化查询或者绑定变量，而不是直接拼接用户输入到SQL查询语句中。 对用户输入进行验证和过滤，限制输入的长度和类型。 对用户输入进行转义处理，确保特殊字符不会被解释为SQL代码。 最小权限原则，确保应用程序连接数据库的账户具有最小必要的权限。 对应用程序进行安全审计和漏洞扫描，及时修复发现的安全漏洞。 定期更新和维护数据库和应用程序，以修复已知的安全漏洞。 总之，通过合理的编码和严格的输入验证，可以有效地减少SQL注入攻击的风险。\n其他 如何处理数据库的并发冲突？ 在处理数据库的并发冲突时，可以采取以下一些常见的策略和技术：\n乐观并发控制（Optimistic Concurrency Control）：乐观并发控制假设冲突较少发生，因此不会对数据进行加锁。它通过在更新数据时检查旧的版本号或时间戳，来验证数据是否被其他事务修改过。如果检测到冲突，可以选择中止当前事务或重新尝试更新操作。\n悲观并发控制（Pessimistic Concurrency Control）：悲观并发控制假设冲突会频繁发生，因此会在操作数据时使用锁来阻止其他事务对其进行修改。悲观并发控制可以使用行级锁或表级锁，以确保数据的一致性。这可能会导致一些性能上的开销，因为其他事务需要等待锁的释放。\n数据版本控制（Data Versioning）：数据版本控制是一种用于处理并发冲突的技术，通过为每个数据项添加一个版本号或时间戳来跟踪其修改历史。当有多个事务试图修改相同的数据时，可以通过比较版本号或时间戳来判断最新的有效修改。这种方式下，每个事务都可以并发地执行，只有在提交时才需要检查冲突。\n数据分片（Data Sharding）：数据分片是将数据分散存储在多个数据库节点上的技术。通过将数据分散存储，可以减少并发操作发生的概率。每个节点只处理一部分数据，从而降低了并发冲突的可能性。\n死锁检测与解决：死锁是指两个或多个事务相互等待对方释放资源的情况。数据库管理系统通常提供死锁检测机制，可以检测到死锁的存在并采取相应的解决措施，例如终止其中一个事务或回滚一部分事务操作。\n无论采用哪种策略，处理数据库的并发冲突都需要综合考虑业务需求、性能开销和数据一致性等方面。选择合适的并发控制方法取决于具体的应用场景和系统要求。\n请解释数据库的批量插入和批量更新，与逐条插入和更新相比，它们的优势在哪里？ 数据库的批量插入和批量更新是一种在单个操作中处理多个数据记录的方法，与逐条插入和更新相比，它们具有以下优势：\n减少通信开销：逐条插入和更新会导致多次与数据库的通信交互，每次通信都会有一定的开销，包括网络延迟和数据包传输。而批量操作将多个记录合并为一个操作，减少了通信的次数，从而显著降低了通信开销。\n提升性能：批量操作可以在单个事务中处理多个记录，减少了事务的开销，包括锁竞争和日志记录。相比逐条操作，批量操作在相同的时间内能够处理更多的数据记录，从而提高了数据库的整体性能。\n简化代码逻辑：逐条插入和更新需要在应用程序中编写循环来处理每条记录，增加了代码的复杂性和维护的成本。而批量操作可以使用数据库的特定语法和功能，将多个记录一次性提交，简化了代码逻辑，提高了开发效率。\n保持数据一致性：批量操作在单个事务中处理多个记录，要么全部成功，要么全部失败。这有助于维护数据的一致性。如果逐条操作中的某个记录失败，可能会导致部分数据插入或更新，造成数据的不一致性。\n减少数据库负载：批量操作可以有效地减少数据库的负载压力。通过减少通信开销和事务开销，数据库系统可以更高效地处理大量的数据记录，减少了系统资源的占用。\n需要注意的是，批量插入和批量更新并非适用于所有情况。它们更适合处理大批量的数据记录，例如从文件导入数据或进行大规模数据更新。对于小规模的数据操作，逐条插入和更新可能更简单和直观。因此，在使用批量操作之前，应根据具体情况评估其对应用程序的性能和开发的影响。\n如何优化数据库的表结构设计？ 优化数据库的表结构设计是提高数据库性能和效率的重要步骤。以下是一些常见的优化策略和建议：\n规范化数据库结构：使用规范化的设计原则，将数据分解为更小的逻辑单元，避免数据冗余和不一致。合理使用主键、外键和索引，确保数据的完整性和一致性。\n考虑数据的访问模式：了解应用程序对数据的访问模式，根据读写操作的频率和模式来优化表结构。对于频繁读取的数据，考虑添加合适的索引。对于写入频繁的数据，可以根据情况考虑避免或减少索引的使用。\n避免过度规范化：虽然规范化是好的，但过度规范化可能导致复杂的连接操作和性能下降。在设计过程中，需权衡规范化和性能之间的权衡，根据实际需求决定是否进行适度的反规范化。\n合理选择字段类型：根据数据的特性和存储需求，选择合适的字段类型。避免使用过大或过小的数据类型，以节省存储空间和提高查询性能。\n使用合适的索引：根据查询需求和访问模式，选择合适的索引策略。注意索引的选择性和覆盖度，避免创建过多或过少的索引。\n分区和分表：对于大型数据表，考虑使用分区或分表技术，将数据分割成更小的部分。这有助于提高查询性能和维护的效率。\n预估数据量和增长：在设计表结构时，要预估数据量的大小和增长趋势。根据预估的数据量合理设计表结构，避免未来的性能问题。\n考虑缓存和缓存策略：使用适当的缓存技术，如缓存查询结果或热门数据，以减轻数据库的负载压力。\n定期维护和优化：定期进行数据库的维护和优化工作，包括索引重建、碎片整理、统计信息更新等，以保持数据库的性能和效率。\n监控和调优：监控数据库的性能指标，如查询响应时间、数据库连接数、锁竞争等，及时进行性能调优和优化。\n最重要的是，优化数据库的表结构设计需要综合考虑业务需求、数据特性和应用程序的访问模式。因此，设计阶段需要与开发团队和数据库管理员紧密合作，并不断评估和调整表结构，以达到最佳的性能和效率。\n请解释数据库的连接池，为什么使用连接池？ 数据库连接池（Database Connection Pool）是一种管理数据库连接的技术，它通过预先创建一组数据库连接并对其进行管理，以便在需要时重复使用这些连接。连接池中的连接可以被多个线程共享，从而提高数据库访问的性能和效率。\n使用数据库连接池的主要原因如下：\n提高性能和效率：数据库连接的创建和销毁是一项开销较大的操作。连接池通过预先创建一定数量的连接，并在需要时重复使用这些连接，避免了频繁的连接创建和销毁操作，从而提高了数据库访问的性能和效率。\n资源管理和控制：数据库连接是一种有限的资源，每个数据库系统都有最大连接数的限制。使用连接池可以对连接进行有效管理和控制，确保不超过系统的最大连接数，并防止连接泄漏和滥用。\n连接复用：连接池允许多个线程共享连接，避免了每个线程都创建自己的连接，减少了连接的竞争和冲突。这样可以更好地利用数据库连接资源，并提供更好的并发性能。\n连接的验证和维护：连接池可以对连接进行验证，确保连接的可用性和有效性。它还可以执行连接的定期检查、维护和优化操作，如连接的健康检查、超时处理和连接的自动重连等。\n避免连接超时和性能下降：在使用连接池的情况下，连接可以在连接池中被保持活动状态，避免了长时间空闲导致的连接超时和性能下降。连接池可以在连接空闲一段时间后自动释放，从而避免了因长时间保持连接而浪费资源。\n总而言之，使用数据库连接池可以提高数据库访问的性能、效率和资源利用率。它可以管理和控制连接资源，避免连接的频繁创建和销毁，提供连接的复用和有效管理，从而提供更好的并发性能和可靠性。\n请解释数据库的分页查询，以及如何在MySQL中进行高效的分页查询。 数据库的分页查询是指将查询结果按照指定的页数和每页记录数进行分割，只返回指定页数的数据，用于展示大量数据时的分页展示。\n在MySQL中进行高效的分页查询可以使用LIMIT子句来限制返回的记录数量和偏移量。LIMIT子句的语法如下：\nSELECT * FROM table_name LIMIT offset, count; 其中，offset表示偏移量，即从结果集的第几行开始返回数据，count表示要返回的记录数量。\n例如，如果要查询第2页每页显示10条记录的数据，可以使用以下语句：\nSELECT * FROM table_name LIMIT 10 OFFSET 10; 这将返回从结果集的第11行开始的10条记录。\n为了提高分页查询的效率，可以考虑以下几个方面：\n使用合适的索引：通过在WHERE子句中使用合适的索引列，可以减少数据的扫描范围，提高查询效率。\n避免大偏移量：在分页查询中，如果偏移量非常大，会导致数据库扫描大量的数据并且跳过很多记录，降低查询效率。建议通过其他方式限制数据的范围，如使用WHERE子句指定合适的条件。\n缓存查询结果：如果分页查询的结果数据量较大，可以考虑在应用程序中缓存查询结果，避免每次都执行分页查询。\n使用优化的查询语句：根据具体的需求和业务场景，优化查询语句的性能，如合理使用JOIN、避免全表扫描等。\n需要注意的是，分页查询的效率受到数据量和查询条件的影响。在处理大量数据时，可以考虑使用滚动式分页查询或使用游标来优化查询性能，避免一次性返回大量数据。此外，应根据具体的数据库系统和业务需求进行性能测试和优化，以提高分页查询的效率和响应速度。\n请解释数据库的视图（View），以及它的作用和优势。 数据库的视图（View）是一个虚拟的表，它是基于一个或多个数据库表的查询结果构建而成的。视图是一种逻辑结构，不包含实际的数据，而是根据定义的查询条件和表关系动态生成的结果集。\n视图的作用和优势如下：\n数据安全性和权限控制：视图可以隐藏底层表的某些列或数据，只暴露给用户或应用程序所需的数据。通过视图，可以对不同用户或用户组分配不同的访问权限，提高数据的安全性和隐私保护。\n简化复杂查询：通过创建视图，可以将复杂的查询逻辑抽象成简单的视图查询语句，简化了复杂查询的编写和维护。视图可以隐藏底层表的复杂关系和逻辑，使查询更加直观和易于理解。\n数据逻辑独立性：视图将物理存储和逻辑结构分离，使应用程序可以与视图进行交互，而无需关心底层表的结构和关系变化。当底层表结构发生变化时，只需修改视图的定义，而不会影响应用程序的查询逻辑。\n数据聚合和格式转换：通过视图可以对数据进行聚合、计算和格式转换，生成更有意义和易于使用的数据集。视图可以用于生成报表、汇总数据、格式化日期等操作，提供更灵活和定制化的数据展示。\n提高性能和优化查询：数据库优化的一个重要手段是使用视图来预先计算和存储复杂查询的结果。视图可以对查询进行优化，提高查询性能，并避免重复的计算操作。\n需要注意的是，视图本身并不存储数据，而是根据底层表的数据进行实时查询生成的结果集。视图的查询性能受到底层表的性能影响。因此，在设计视图时需要考虑查询的复杂性、底层表的索引和性能，并适时进行索引优化和查询优化。\n总结而言，视图是一个灵活且强大的数据库工具，它提供了数据的安全性、简化查询、逻辑独立性和性能优化等多种优势，使数据库系统更易于使用和管理。\n请解释数据库的触发器（Trigger），以及它的作用和使用场景。 数据库的触发器（Trigger）是一种特殊的数据库对象，它与表相关联，可以在特定的数据库操作（如插入、更新、删除）前或后自动触发执行定义好的代码逻辑。触发器通常用于实现数据完整性约束、日志记录、数据验证和业务逻辑的自动执行等功能。\n触发器的作用和使用场景如下：\n数据完整性约束：触发器可以用于强制实施数据库的完整性约束，例如在插入或更新数据时进行验证，确保数据满足特定的条件和规则。触发器可以执行复杂的验证逻辑，例如检查外键关系、检查唯一性约束等。\n数据变更日志记录：触发器可以用于记录数据的变更历史，可以将变更操作写入日志表或审计表中，以便跟踪和审计数据的修改。触发器可以捕获数据变更前后的值，并将相关信息记录下来。\n业务逻辑的自动执行：触发器可以自动执行业务逻辑，如计算、更新其他表、发送通知等。例如，在插入新订单时，触发器可以自动更新库存表中的库存数量，并发送通知给相关人员。\n数据复制和同步：触发器可以用于在数据库的主从复制中实现数据同步，当主数据库发生数据变更时，触发器可以自动将变更操作同步到从数据库中。\n数据转换和处理：触发器可以对插入、更新、删除的数据进行转换和处理，例如对特定字段进行加密、解密或格式化。\n触发器通常以SQL语句或存储过程的形式定义，并与特定的表相关联。它们在数据库操作前或后触发执行，并在特定的上下文中访问和处理数据。需要注意的是，触发器的使用应慎重，因为触发器的执行可能会对数据库的性能产生一定的影响，过多或复杂的触发器可能会导致性能下降。\n在设计和使用触发器时，需要考虑业务需求、数据完整性约束和性能影响，合理选择触发器的时机和处理逻辑，以确保触发器的正确性和效率。\n","permalink":"https://lidengxm.github.io/posts/java/mysql%E5%85%AB%E8%82%A1/","summary":"写在前面 1、一条 SQL 是如何执行的？也就是说，从MySQL 客户端执行了一条 SQL 语句，MySQL 服务端会进行哪些处理。 2、索引相关：索引是如何实现的？MySQL 的索引采用的哪种数据结构？哈希索引和 B+ 树索引的区别是什么？ 3、事务相关：事务的四大特性是什么？什么是幻读、脏读、不可重复读？、M","title":"MySQL八股"},{"content":"1. 介绍 scan命令的作用和keys *的作用类似，主要用于查找redis中的键，但是在正式的生产环境中一般不会直接使用keys *这个命令，因为他会返回所有的键，如果键的数量很多会导致查询时间很长，进而导致服务器阻塞，所以需要scan来进行更细致的查找\nscan总共有这几种命令：scan、sscan、hscan、zscan，分别用于迭代数据库中的：数据库中所有键、集合键、哈希键、有序集合键，命令具体结构如下：\nscan cursor [MATCH pattern] [COUNT count] [TYPE type] sscan key cursor [MATCH pattern] [COUNT count] hscan key cursor [MATCH pattern] [COUNT count] zscan key cursor [MATCH pattern] [COUNT count] 2. scan scan cursor [MATCH pattern] [COUNT count] [TYPE type]，cursor表示游标，指查询开始的位置，count默认为10，查询完后会返回下一个开始的游标，当返回0的时候表示所有键查询完了\n127.0.0.1:6379[2]\u0026gt; scan 0 1) \u0026#34;3\u0026#34; 2) 1) \u0026#34;mystring\u0026#34; 2) \u0026#34;myzadd\u0026#34; 3) \u0026#34;myhset\u0026#34; 4) \u0026#34;mylist\u0026#34; 5) \u0026#34;myset2\u0026#34; 6) \u0026#34;myset1\u0026#34; 7) \u0026#34;mystring1\u0026#34; 8) \u0026#34;mystring3\u0026#34; 9) \u0026#34;mystring4\u0026#34; 10) \u0026#34;myset\u0026#34; 127.0.0.1:6379[2]\u0026gt; scan 3 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;myzadd1\u0026#34; 2) \u0026#34;mystring2\u0026#34; 3) \u0026#34;mylist2\u0026#34; 4) \u0026#34;myhset1\u0026#34; 5) \u0026#34;mylist1\u0026#34; MATCH可以采用模糊匹配找出自己想要查找的键，这里的逻辑是先查出20个，再匹配，而不是先匹配再查询，这里加上count 20是因为默认查出的10个数中可能不能包含所有的相关项，所以把范围扩大到查20个，我这里测试的键总共有15个\n127.0.0.1:6379[2]\u0026gt; scan 0 match mylist* count 20 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;mylist\u0026#34; 2) \u0026#34;mylist2\u0026#34; 3) \u0026#34;mylist1\u0026#34; TYPE可以根据具体的结构类型来匹配该类型的键\n127.0.0.1:6379[2]\u0026gt; scan 0 count 20 type list 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;mylist\u0026#34; 2) \u0026#34;mylist2\u0026#34; 3) \u0026#34;mylist1\u0026#34; 3. sscan sscan key cursor [MATCH pattern] [COUNT count]，sscan的第一个参数总是集合类型的key\n127.0.0.1:6379[2]\u0026gt; sadd myset1 a b c d (integer) 4 127.0.0.1:6379[2]\u0026gt; smembers myset1 1) \u0026#34;d\u0026#34; 2) \u0026#34;a\u0026#34; 3) \u0026#34;c\u0026#34; 4) \u0026#34;b\u0026#34; 127.0.0.1:6379[2]\u0026gt; sscan myset1 0 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;d\u0026#34; 2) \u0026#34;c\u0026#34; 3) \u0026#34;b\u0026#34; 4) \u0026#34;a\u0026#34; 127.0.0.1:6379[2]\u0026gt; sscan myset1 0 match a 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;a\u0026#34; 4. hscan hscan key cursor [MATCH pattern] [COUNT count]，sscan的第一个参数总是哈希类型的key\n127.0.0.1:6379[2]\u0026gt; hset myhset1 kk1 vv1 kk2 vv2 kk3 vv3 (integer) 3 127.0.0.1:6379[2]\u0026gt; hgetall myhset1 1) \u0026#34;kk1\u0026#34; 2) \u0026#34;vv1\u0026#34; 3) \u0026#34;kk2\u0026#34; 4) \u0026#34;vv2\u0026#34; 5) \u0026#34;kk3\u0026#34; 6) \u0026#34;vv3\u0026#34; 127.0.0.1:6379[2]\u0026gt; hscan myhset1 0 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;kk1\u0026#34; 2) \u0026#34;vv1\u0026#34; 3) \u0026#34;kk2\u0026#34; 4) \u0026#34;vv2\u0026#34; 5) \u0026#34;kk3\u0026#34; 6) \u0026#34;vv3\u0026#34; 5. zscan zscan key cursor [MATCH pattern] [COUNT count]，sscan的第一个参数总是有序集合类型的key\n127.0.0.1:6379[2]\u0026gt; zadd myzadd1 1 zz1 2 zz2 3 zz3 (integer) 3 127.0.0.1:6379[2]\u0026gt; zrange myzadd1 0 -1 withscores 1) \u0026#34;zz1\u0026#34; 2) \u0026#34;1\u0026#34; 3) \u0026#34;zz2\u0026#34; 4) \u0026#34;2\u0026#34; 5) \u0026#34;zz3\u0026#34; 6) \u0026#34;3\u0026#34; 127.0.0.1:6379[2]\u0026gt; zscan myzadd1 0 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;zz1\u0026#34; 2) \u0026#34;1\u0026#34; 3) \u0026#34;zz2\u0026#34; 4) \u0026#34;2\u0026#34; 5) \u0026#34;zz3\u0026#34; 6) \u0026#34;3\u0026#34; ","permalink":"https://lidengxm.github.io/posts/tech/tech1/","summary":"1. 介绍 scan命令的作用和keys *的作用类似，主要用于查找redis中的键，但是在正式的生产环境中一般不会直接使用keys *这个命令，因为他会返回所有的键，如果键的数量很多会导致查询时间很长，进而导致服务器阻塞，所以需要scan来进行更细致的查找 scan总共有这几种命令：sca","title":"Redis scan命令学习"},{"content":"sj djs\nshha dssdh tsaagdga jsjaj hhshd ds sash3 df ","permalink":"https://lidengxm.github.io/posts/tech/test/","summary":"sj djs shha dssdh tsaagdga jsjaj hhshd ds sash3 df","title":"Redis scan命令学习"},{"content":"","permalink":"https://lidengxm.github.io/posts/java/blog/","summary":"","title":"Blog"},{"content":"","permalink":"https://lidengxm.github.io/posts/life/life/","summary":"","title":"Life"},{"content":"","permalink":"https://lidengxm.github.io/posts/read/read/","summary":"","title":"Read"},{"content":"\u0026lt;div\u0026gt; 科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 \u0026lt;/div\u0026gt; ","permalink":"https://lidengxm.github.io/posts/tech/tech/","summary":"\u0026lt;div\u0026gt; 科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代码 科技代","title":"Tech"},{"content":"新建\n","permalink":"https://lidengxm.github.io/links/","summary":"新建","title":"🤝友链"},{"content":"关于我 大四正在准备秋招 非科班转码一枚 想改变生活 我喜欢的名言 生活不是一潭死水，有信念才有希望！ 关于博客 框架 hugo 主题 PaperMod ","permalink":"https://lidengxm.github.io/about/","summary":"关于我 大四正在准备秋招 非科班转码一枚 想改变生活 我喜欢的名言 生活不是一潭死水，有信念才有希望！ 关于博客 框架 hugo 主题 PaperMod","title":"🙋🏻‍♂️关于我"}]